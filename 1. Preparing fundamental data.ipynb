{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading quarterly compustat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading quarterly fundamental data from 1967-2020\n",
    "# CUSIP number (one of the most important columns to join fundamental & price data) was introduced in 1967 only\n",
    "\n",
    "quarterly_comp_data = pd.read_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\Quarterly compustat data.csv')\n",
    "quarterly_comp_data = quarterly_comp_data.dropna(subset = ['cusip']) \n",
    "# Dropping rows where cusip is NULL. Can't join them anyway\n",
    "# I din't take gross and operating margin because those values are not comparable across industries.\n",
    "\n",
    "# Considering only IND industrial format. Below line not executed as quarterly has only IND anyway\n",
    "# quarterly_comp_data = quarterly_comp_data[quarterly_comp_data['indfmt'] == 'INDL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting for currency exchange rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USD    1643731\n",
       "CAD     185586\n",
       "Name: curcdq, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarterly_comp_data['curcdq'].value_counts()\n",
    "# Noticed there are lot of stocks with base currency as CAD. Need to adjust their financials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th># Missing values in currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1967</td>\n",
       "      <td>1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1968</td>\n",
       "      <td>1505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1969</td>\n",
       "      <td>1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1970</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1971</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1972</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1973</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1974</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1975</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1976</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1977</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1978</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1979</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1980</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1981</td>\n",
       "      <td>8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1982</td>\n",
       "      <td>5882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1983</td>\n",
       "      <td>3037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1984</td>\n",
       "      <td>2847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1985</td>\n",
       "      <td>4416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1986</td>\n",
       "      <td>4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1987</td>\n",
       "      <td>4546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1988</td>\n",
       "      <td>4805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1989</td>\n",
       "      <td>5278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1990</td>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1991</td>\n",
       "      <td>7191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1992</td>\n",
       "      <td>8527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1993</td>\n",
       "      <td>8424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1994</td>\n",
       "      <td>7042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1995</td>\n",
       "      <td>9559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1996</td>\n",
       "      <td>8179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1997</td>\n",
       "      <td>6371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1998</td>\n",
       "      <td>7845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1999</td>\n",
       "      <td>6979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2000</td>\n",
       "      <td>5020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2001</td>\n",
       "      <td>4619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2002</td>\n",
       "      <td>5265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2003</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2004</td>\n",
       "      <td>6051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2005</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2006</td>\n",
       "      <td>6680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2007</td>\n",
       "      <td>7673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2008</td>\n",
       "      <td>8502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009</td>\n",
       "      <td>9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>9953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2011</td>\n",
       "      <td>10817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012</td>\n",
       "      <td>12089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013</td>\n",
       "      <td>12068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014</td>\n",
       "      <td>11666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015</td>\n",
       "      <td>11979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>12842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>13543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>15240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>14673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>14279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  # Missing values in currency\n",
       "42  1967                          1088\n",
       "40  1968                          1505\n",
       "41  1969                          1475\n",
       "44  1970                           330\n",
       "45  1971                           261\n",
       "47  1972                           253\n",
       "46  1973                           260\n",
       "48  1974                           250\n",
       "50  1975                           162\n",
       "53  1976                            96\n",
       "52  1977                            97\n",
       "51  1978                           103\n",
       "49  1979                           213\n",
       "43  1980                           344\n",
       "13  1981                          8944\n",
       "29  1982                          5882\n",
       "38  1983                          3037\n",
       "39  1984                          2847\n",
       "37  1985                          4416\n",
       "33  1986                          4875\n",
       "36  1987                          4546\n",
       "34  1988                          4805\n",
       "30  1989                          5278\n",
       "27  1990                          6044\n",
       "20  1991                          7191\n",
       "14  1992                          8527\n",
       "16  1993                          8424\n",
       "21  1994                          7042\n",
       "11  1995                          9559\n",
       "17  1996                          8179\n",
       "25  1997                          6371\n",
       "18  1998                          7845\n",
       "22  1999                          6979\n",
       "32  2000                          5020\n",
       "35  2001                          4619\n",
       "31  2002                          5265\n",
       "28  2003                          5988\n",
       "26  2004                          6051\n",
       "24  2005                          6508\n",
       "23  2006                          6680\n",
       "19  2007                          7673\n",
       "15  2008                          8502\n",
       "12  2009                          9098\n",
       "10  2010                          9953\n",
       "9   2011                         10817\n",
       "5   2012                         12089\n",
       "6   2013                         12068\n",
       "8   2014                         11666\n",
       "7   2015                         11979\n",
       "4   2016                         12842\n",
       "3   2017                         13543\n",
       "0   2018                         15240\n",
       "1   2019                         14673\n",
       "2   2020                         14279"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currency_exchange_rate_missing_byyear = pd.DataFrame(pd.to_datetime(quarterly_comp_data[quarterly_comp_data['currtrq'].isnull()]\n",
    "                                                                    ['datadate']).dt.year.value_counts()).reset_index()\n",
    "currency_exchange_rate_missing_byyear.columns = ['Year', '# Missing values in currency']\n",
    "currency_exchange_rate_missing_byyear = currency_exchange_rate_missing_byyear.sort_values(by = ['Year'], ascending = True)\n",
    "currency_exchange_rate_missing_byyear\n",
    "# Noticed that apart from 1667-1970, the currency exchange rate variable has been properly filled only from 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging USD-CAD from external data: https://fred.stlouisfed.org/series/EXCAUS\n",
    "# need external data because the USD-CAD translation variable provided by compustat (curuscnq) is poorly filled\n",
    "usd_cad_currency_translation = pd.read_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\usd_cad_exchange_rate.csv')\n",
    "usd_cad_currency_translation.columns = ['date','CAD_exchange_rate']\n",
    "usd_cad_currency_translation['date'] = pd.to_datetime(usd_cad_currency_translation['date']) \n",
    "usd_cad_currency_translation['Year'] = usd_cad_currency_translation['date'].dt.year\n",
    "usd_cad_currency_translation['Month'] = usd_cad_currency_translation['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_comp_data['datadate'] = pd.to_datetime(quarterly_comp_data['datadate']) \n",
    "quarterly_comp_data['Year'] = quarterly_comp_data['datadate'].dt.year\n",
    "quarterly_comp_data['Month'] = quarterly_comp_data['datadate'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1829317, 88)\n",
      "(1829317, 89)\n"
     ]
    }
   ],
   "source": [
    "print(quarterly_comp_data.shape)\n",
    "quarterly_comp_data = pd.merge(quarterly_comp_data,\n",
    "                               usd_cad_currency_translation[['Month','Year','CAD_exchange_rate']],\n",
    "                               left_on = ['Month','Year'],\n",
    "                               right_on = ['Month','Year'],\n",
    "                               how = 'left')\n",
    "print(quarterly_comp_data.shape)\n",
    "\n",
    "# For canadian dollars, the currtrq has to be adjusted. \n",
    "# Because if curcdq = CAD & currtrq, that means the company is declaring results in CA\n",
    "# need to keep data format in USD across entire data\n",
    "quarterly_comp_data['currtrq'] = np.where(quarterly_comp_data['curcdq'] == 'CAD',quarterly_comp_data['currtrq']/quarterly_comp_data['CAD_exchange_rate'],quarterly_comp_data['currtrq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gvkey', 'datadate', 'fyearq', 'fqtr', 'fyr', 'indfmt', 'consol',\n",
       "       'popsrc', 'datafmt', 'tic', 'cusip', 'conm', 'ajexq', 'curcdq',\n",
       "       'currtrq', 'curuscnq', 'datacqtr', 'datafqtr', 'rp', 'updq', 'pdateq',\n",
       "       'rdq', 'actq', 'aqpq', 'atq', 'ceqq', 'cheq', 'chq', 'ciq', 'csh12q',\n",
       "       'cshfd12', 'cshfdq', 'cshiq', 'cshoq', 'cshprq', 'cstkq', 'dlttq',\n",
       "       'epsf12', 'epsfi12', 'epsfiq', 'epsfxq', 'epspi12', 'epspiq', 'epspxq',\n",
       "       'epsx12', 'gdwlamq', 'gdwlq', 'ibcomq', 'ibq', 'intanoq', 'intanq',\n",
       "       'lseq', 'ltq', 'mibq', 'niq', 'obkq', 'piq', 'ppentq', 'rectq', 'req',\n",
       "       'revtq', 'saleq', 'seqq', 'spce12', 'spced12', 'spcedq', 'spceeps12',\n",
       "       'spceepsq', 'spceq', 'teqq', 'ugiq', 'xiq', 'xoprq', 'exchg', 'costat',\n",
       "       'mkvaltq', 'busdesc', 'ggroup', 'gind', 'gsector', 'gsubind', 'naics',\n",
       "       'sic', 'spcindcd', 'spcseccd', 'spcsrc', 'Year', 'Month',\n",
       "       'CAD_exchange_rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarterly_comp_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns in quarterly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping certain columns there are other better columns that are very similar\n",
    "\n",
    "#dropping consol because entire column is \"c\"\n",
    "#dropping poprse because entire column is \"d\"\n",
    "#dropping datafmt because entire column is \"std\"\n",
    "#dropping curcdq because currtrq has already been adjusted for CAD\n",
    "#dropping curuscnq because it's poorly filled\n",
    "#dropping aqpq because I did not find any non null data before 2000's.also,half of non null data is 0.\n",
    "#dropping both cshiq and cshoq because of too many null values(moreover I already have cshprq)\n",
    "#dropping cstkq because I already have ceqq(cstkq is inbuilt added to ceqq)\n",
    "#dropping lseq because I already have total liabilities(If I need lseq I can subtract equity from liability+equity)\n",
    "#dropping mibq because I already have total equity\n",
    "#dropping ibq because I already have niq\n",
    "#dropping obkq because too less information\n",
    "#dropping piq because I already have net income\n",
    "#dropping req because I already have ceqq\n",
    "#dropping saleq because I already have revenue\n",
    "#dropping ceqq because I already have seqq\n",
    "#dropping ciq because it is some weird balance sheet variable(don't confuse this variable with niq)\n",
    "#dropping ugiq because it is only available for utility codes\n",
    "#dropping ggroup because I have more granulated data in gsubind\n",
    "#dropping gsector because I have more granulated data in gsubind\n",
    "#dropping gind because I have more granulated data in gsubind\n",
    "#dropping spcseccd because I have very less data\n",
    "#dropping pdateq because I discovered that preliminary date and result date are very close.\n",
    "quarterly_comp_data = quarterly_comp_data.drop(['consol','popsrc','datafmt','curcdq','curuscnq','aqpq','cshiq','cshoq','cstkq',\n",
    "                                                'lseq','mibq','ibq','obkq','piq','req','saleq','ceqq','ciq','ugiq','ggroup',\n",
    "                                                'gsector','gind','spcseccd','pdateq'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gvkey', 'datadate', 'fyearq', 'fqtr', 'fyr', 'indfmt', 'tic', 'cusip',\n",
       "       'conm', 'ajexq', 'currtrq', 'datacqtr', 'datafqtr', 'rp', 'updq', 'rdq',\n",
       "       'actq', 'atq', 'cheq', 'chq', 'csh12q', 'cshfd12', 'cshfdq', 'cshprq',\n",
       "       'dlttq', 'epsf12', 'epsfi12', 'epsfiq', 'epsfxq', 'epspi12', 'epspiq',\n",
       "       'epspxq', 'epsx12', 'gdwlamq', 'gdwlq', 'ibcomq', 'intanoq', 'intanq',\n",
       "       'ltq', 'niq', 'ppentq', 'rectq', 'revtq', 'seqq', 'spce12', 'spced12',\n",
       "       'spcedq', 'spceeps12', 'spceepsq', 'spceq', 'teqq', 'xiq', 'xoprq',\n",
       "       'exchg', 'costat', 'mkvaltq', 'busdesc', 'gsubind', 'naics', 'sic',\n",
       "       'spcindcd', 'spcsrc', 'Year', 'Month', 'CAD_exchange_rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarterly_comp_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns based on their business description\n",
    "quarterly_comp_data.rename(columns = {'datadate': 'quarter_end_date','fyearq': 'fiscal_year','fqtr': 'fiscal_quarter',\n",
    "                                     'fyr': 'fiscal_year_end_month', 'indfmt': 'industrial_format','tic': 'ticker',\n",
    "                                     'conm': 'company_name','ajexq': 'split_adjusting_factor',\n",
    "                                     'currtrq': 'currency_exchange_rate','datacqtr': 'reporting_year_quarter',\n",
    "                                     'datafqtr': 'fiscal_year_quarter','rp': 'reporting_frequency','updq': 'financial_normal',\n",
    "                                     'pdateq':'preliminary_date','rdq': 'result_reported_date','actq':'total_current_asset',\n",
    "                                     'atq':'asset_total','seqq': 'share_holder_equity','teqq':'equity_total',\n",
    "                                     'cheq':'cash_st_investment','chq': 'cash',\n",
    "                                     'cshprq':'num_shares_eps','dlttq': 'total_LT_debt',\n",
    "                                     'epsf12': 'eps_d_excl_extraordinary_12M','epsfi12': 'eps_d_12M','epsfiq': 'eps_d',\n",
    "                                     'epsfxq': 'eps_d_excl_extraordinary','epspi12': 'eps_12M','epspiq': 'eps',\n",
    "                                     'epspxq':'eps_excl_extraordinary', 'epsx12': 'eps_excl_extraordinary_12M', \n",
    "                                     'gdwlamq':'goodwill_amortization', 'gdwlq':'goodwill', 'ibcomq': 'income_excl_extraordinary',\n",
    "                                     'intanq': 'intangible_asset_total','ltq':'liability_total', 'niq': 'income', 'ppentq': 'ppe_total',\n",
    "                                     'rectq': 'receivable_total','revtq':'revenue', 'spce12':'core_earnings_12M', 'spced12':'core_eps_d_12M',\n",
    "                                     'spcedq':'core_eps_d','spceeps12':'core_eps_12M','spceepsq':'core_eps', \n",
    "                                     'spceq': 'core_earnings','xiq':'extraordinary_income','xoprq':'operating_expense',\n",
    "                                     'exchg': 'stock_exchange','costat':'active_or_inactive_status','mkvaltq':'Mcap', \n",
    "                                     'busdesc': 'business_description','gsubind': 'gics_sub_industry','naics': 'NAICS',\n",
    "                                     'sic': 'sic_code', 'spcindcd':'S&P_industry_sector_code','spcsrc':'S&P_grade',\n",
    "                                     'csh12q':'num_shares_eps_12','cshfd12': 'num_d_shares_eps_12','cshfdq': 'num_d_shares_eps'},\n",
    "                          inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81520,)\n",
      "(54920, 65)\n"
     ]
    }
   ],
   "source": [
    "# Noticed that there are lot of rows where financial information is present but exchange rate is missing\n",
    "print(quarterly_comp_data[((quarterly_comp_data['currency_exchange_rate'].isnull()) & \n",
    "                     (~quarterly_comp_data['revenue'].isnull()))]['stock_exchange'].shape)\n",
    "\n",
    "# Noticed that among the rows where currency exchange rate is null, almost 70% before 2000 \n",
    "# Chinese ADR's weren't a thing back then\n",
    "print(quarterly_comp_data[((quarterly_comp_data['currency_exchange_rate'].isnull()) & \n",
    "                     (~quarterly_comp_data['revenue'].isnull()) & (quarterly_comp_data['fiscal_year'] <2000))].shape)\n",
    "\n",
    "# Not executing below 4 rows\n",
    "# So, for stocks on big american stock exchanges, let's assume results are in USD, impute currency exchange rates as 1\n",
    "#most_prob_usd_exch = [11,15,16,17,18,12]\n",
    "#quarterly_comp_data['currency_exchange_rate'] = np.where(((quarterly_comp_data['currency_exchange_rate'].isnull()) & \n",
    "#                                                          (quarterly_comp_data['stock_exchange'].isin(most_prob_usd_exch))), \n",
    "#                                                          1, quarterly_comp_data['currency_exchange_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quarter_end_date</th>\n",
       "      <th>split_adjusting_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39809</th>\n",
       "      <td>1979-12-31</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39810</th>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39811</th>\n",
       "      <td>1980-06-30</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39812</th>\n",
       "      <td>1980-09-30</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39813</th>\n",
       "      <td>1980-12-31</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39969</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39970</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39971</th>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39972</th>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39973</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      quarter_end_date  split_adjusting_factor\n",
       "39809       1979-12-31                   224.0\n",
       "39810       1980-03-31                   224.0\n",
       "39811       1980-06-30                   224.0\n",
       "39812       1980-09-30                   224.0\n",
       "39813       1980-12-31                   224.0\n",
       "...                ...                     ...\n",
       "39969       2019-12-31                     4.0\n",
       "39970       2020-03-31                     4.0\n",
       "39971       2020-06-30                     4.0\n",
       "39972       2020-09-30                     1.0\n",
       "39973       2020-12-31                     1.0\n",
       "\n",
       "[165 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If apple gave 4 to 1 split in aug 2020, then the most recent quaters (from 2020Q3) will have split adjusting factor as 1\n",
    "# The rows between 2020Q2 and all the way back to prior split will have split adjusting factor as 4\n",
    "quarterly_comp_data['split_adjusting_factor'].describe()\n",
    "quarterly_comp_data[quarterly_comp_data['company_name'] == 'APPLE INC'][['quarter_end_date','split_adjusting_factor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>quarter_end_date</th>\n",
       "      <th>num_shares_eps_12_split_adj</th>\n",
       "      <th>num_shares_eps_12</th>\n",
       "      <th>num_shares_eps</th>\n",
       "      <th>num_shares_eps_split_adj</th>\n",
       "      <th>split_adjusting_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1769289</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2008-03-31</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769290</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2008-06-30</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769291</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2008-09-30</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769292</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769293</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2009-03-31</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769294</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2009-06-30</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769295</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769296</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769297</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769298</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>465.5450</td>\n",
       "      <td>93.1090</td>\n",
       "      <td>93.109</td>\n",
       "      <td>465.545</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769299</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>464.4990</td>\n",
       "      <td>92.8998</td>\n",
       "      <td>92.271</td>\n",
       "      <td>461.355</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769300</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>465.9100</td>\n",
       "      <td>93.1820</td>\n",
       "      <td>94.240</td>\n",
       "      <td>471.200</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769301</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>468.5090</td>\n",
       "      <td>93.7018</td>\n",
       "      <td>95.187</td>\n",
       "      <td>475.935</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769302</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>474.3340</td>\n",
       "      <td>94.8668</td>\n",
       "      <td>97.757</td>\n",
       "      <td>488.785</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769303</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>489.2000</td>\n",
       "      <td>97.8400</td>\n",
       "      <td>104.077</td>\n",
       "      <td>520.385</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769304</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>501.9450</td>\n",
       "      <td>100.3890</td>\n",
       "      <td>104.392</td>\n",
       "      <td>521.960</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769305</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>513.7625</td>\n",
       "      <td>102.7525</td>\n",
       "      <td>104.784</td>\n",
       "      <td>523.920</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769306</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>523.1190</td>\n",
       "      <td>104.6238</td>\n",
       "      <td>105.242</td>\n",
       "      <td>526.210</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769307</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2012-09-30</td>\n",
       "      <td>524.9750</td>\n",
       "      <td>104.9950</td>\n",
       "      <td>105.556</td>\n",
       "      <td>527.780</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769308</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>536.7450</td>\n",
       "      <td>107.3490</td>\n",
       "      <td>113.763</td>\n",
       "      <td>568.815</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769309</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>549.0915</td>\n",
       "      <td>109.8183</td>\n",
       "      <td>114.712</td>\n",
       "      <td>573.560</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769310</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>565.3040</td>\n",
       "      <td>113.0608</td>\n",
       "      <td>118.194</td>\n",
       "      <td>590.970</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769311</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>585.7615</td>\n",
       "      <td>117.1523</td>\n",
       "      <td>121.862</td>\n",
       "      <td>609.310</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769312</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>597.1050</td>\n",
       "      <td>119.4210</td>\n",
       "      <td>122.802</td>\n",
       "      <td>614.010</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769313</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>607.9140</td>\n",
       "      <td>121.5828</td>\n",
       "      <td>123.473</td>\n",
       "      <td>617.365</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769314</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>615.4900</td>\n",
       "      <td>123.0980</td>\n",
       "      <td>124.250</td>\n",
       "      <td>621.250</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769315</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>619.3165</td>\n",
       "      <td>123.8633</td>\n",
       "      <td>124.911</td>\n",
       "      <td>624.555</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769316</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>622.6950</td>\n",
       "      <td>124.5390</td>\n",
       "      <td>125.497</td>\n",
       "      <td>627.485</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769317</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>625.7565</td>\n",
       "      <td>125.1513</td>\n",
       "      <td>125.947</td>\n",
       "      <td>629.735</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769318</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2015-06-30</td>\n",
       "      <td>628.8100</td>\n",
       "      <td>125.7620</td>\n",
       "      <td>126.689</td>\n",
       "      <td>633.445</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769319</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2015-09-30</td>\n",
       "      <td>633.9650</td>\n",
       "      <td>126.7930</td>\n",
       "      <td>129.006</td>\n",
       "      <td>645.030</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769320</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>641.0100</td>\n",
       "      <td>128.2020</td>\n",
       "      <td>131.100</td>\n",
       "      <td>655.500</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769321</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>649.3390</td>\n",
       "      <td>129.8678</td>\n",
       "      <td>132.676</td>\n",
       "      <td>663.380</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769322</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>665.9575</td>\n",
       "      <td>133.1915</td>\n",
       "      <td>139.983</td>\n",
       "      <td>699.915</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769323</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>691.0540</td>\n",
       "      <td>138.2108</td>\n",
       "      <td>148.991</td>\n",
       "      <td>744.955</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769324</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>721.0600</td>\n",
       "      <td>144.2120</td>\n",
       "      <td>155.024</td>\n",
       "      <td>775.120</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769325</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>757.6590</td>\n",
       "      <td>151.5318</td>\n",
       "      <td>162.129</td>\n",
       "      <td>810.645</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769326</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>789.2165</td>\n",
       "      <td>157.8433</td>\n",
       "      <td>165.212</td>\n",
       "      <td>826.060</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769327</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>812.1440</td>\n",
       "      <td>162.4288</td>\n",
       "      <td>167.294</td>\n",
       "      <td>836.470</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769328</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>828.7900</td>\n",
       "      <td>165.7580</td>\n",
       "      <td>168.314</td>\n",
       "      <td>841.570</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769329</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>837.4575</td>\n",
       "      <td>167.4915</td>\n",
       "      <td>169.146</td>\n",
       "      <td>845.730</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769330</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>843.4450</td>\n",
       "      <td>168.6890</td>\n",
       "      <td>169.997</td>\n",
       "      <td>849.985</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769331</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>847.9640</td>\n",
       "      <td>169.5928</td>\n",
       "      <td>170.893</td>\n",
       "      <td>854.465</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769332</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>852.6250</td>\n",
       "      <td>170.5250</td>\n",
       "      <td>172.026</td>\n",
       "      <td>860.130</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769333</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>857.3815</td>\n",
       "      <td>171.4763</td>\n",
       "      <td>172.989</td>\n",
       "      <td>864.945</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769334</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>865.7265</td>\n",
       "      <td>173.1453</td>\n",
       "      <td>176.654</td>\n",
       "      <td>883.270</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769335</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>875.0325</td>\n",
       "      <td>175.0065</td>\n",
       "      <td>179.000</td>\n",
       "      <td>895.000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769336</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>885.0000</td>\n",
       "      <td>177.0000</td>\n",
       "      <td>180.000</td>\n",
       "      <td>900.000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769337</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>898.3175</td>\n",
       "      <td>179.6635</td>\n",
       "      <td>183.000</td>\n",
       "      <td>915.000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769338</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>908.7500</td>\n",
       "      <td>181.7500</td>\n",
       "      <td>186.000</td>\n",
       "      <td>930.000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769339</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>920.2500</td>\n",
       "      <td>920.2500</td>\n",
       "      <td>937.000</td>\n",
       "      <td>937.000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769340</th>\n",
       "      <td>TESLA INC</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>933.0000</td>\n",
       "      <td>933.0000</td>\n",
       "      <td>951.000</td>\n",
       "      <td>951.000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        company_name quarter_end_date  num_shares_eps_12_split_adj  \\\n",
       "1769289    TESLA INC       2008-03-31                     465.5450   \n",
       "1769290    TESLA INC       2008-06-30                     465.5450   \n",
       "1769291    TESLA INC       2008-09-30                     465.5450   \n",
       "1769292    TESLA INC       2008-12-31                     465.5450   \n",
       "1769293    TESLA INC       2009-03-31                     465.5450   \n",
       "1769294    TESLA INC       2009-06-30                     465.5450   \n",
       "1769295    TESLA INC       2009-09-30                     465.5450   \n",
       "1769296    TESLA INC       2009-12-31                     465.5450   \n",
       "1769297    TESLA INC       2010-03-31                     465.5450   \n",
       "1769298    TESLA INC       2010-06-30                     465.5450   \n",
       "1769299    TESLA INC       2010-09-30                     464.4990   \n",
       "1769300    TESLA INC       2010-12-31                     465.9100   \n",
       "1769301    TESLA INC       2011-03-31                     468.5090   \n",
       "1769302    TESLA INC       2011-06-30                     474.3340   \n",
       "1769303    TESLA INC       2011-09-30                     489.2000   \n",
       "1769304    TESLA INC       2011-12-31                     501.9450   \n",
       "1769305    TESLA INC       2012-03-31                     513.7625   \n",
       "1769306    TESLA INC       2012-06-30                     523.1190   \n",
       "1769307    TESLA INC       2012-09-30                     524.9750   \n",
       "1769308    TESLA INC       2012-12-31                     536.7450   \n",
       "1769309    TESLA INC       2013-03-31                     549.0915   \n",
       "1769310    TESLA INC       2013-06-30                     565.3040   \n",
       "1769311    TESLA INC       2013-09-30                     585.7615   \n",
       "1769312    TESLA INC       2013-12-31                     597.1050   \n",
       "1769313    TESLA INC       2014-03-31                     607.9140   \n",
       "1769314    TESLA INC       2014-06-30                     615.4900   \n",
       "1769315    TESLA INC       2014-09-30                     619.3165   \n",
       "1769316    TESLA INC       2014-12-31                     622.6950   \n",
       "1769317    TESLA INC       2015-03-31                     625.7565   \n",
       "1769318    TESLA INC       2015-06-30                     628.8100   \n",
       "1769319    TESLA INC       2015-09-30                     633.9650   \n",
       "1769320    TESLA INC       2015-12-31                     641.0100   \n",
       "1769321    TESLA INC       2016-03-31                     649.3390   \n",
       "1769322    TESLA INC       2016-06-30                     665.9575   \n",
       "1769323    TESLA INC       2016-09-30                     691.0540   \n",
       "1769324    TESLA INC       2016-12-31                     721.0600   \n",
       "1769325    TESLA INC       2017-03-31                     757.6590   \n",
       "1769326    TESLA INC       2017-06-30                     789.2165   \n",
       "1769327    TESLA INC       2017-09-30                     812.1440   \n",
       "1769328    TESLA INC       2017-12-31                     828.7900   \n",
       "1769329    TESLA INC       2018-03-31                     837.4575   \n",
       "1769330    TESLA INC       2018-06-30                     843.4450   \n",
       "1769331    TESLA INC       2018-09-30                     847.9640   \n",
       "1769332    TESLA INC       2018-12-31                     852.6250   \n",
       "1769333    TESLA INC       2019-03-31                     857.3815   \n",
       "1769334    TESLA INC       2019-06-30                     865.7265   \n",
       "1769335    TESLA INC       2019-09-30                     875.0325   \n",
       "1769336    TESLA INC       2019-12-31                     885.0000   \n",
       "1769337    TESLA INC       2020-03-31                     898.3175   \n",
       "1769338    TESLA INC       2020-06-30                     908.7500   \n",
       "1769339    TESLA INC       2020-09-30                     920.2500   \n",
       "1769340    TESLA INC       2020-12-31                     933.0000   \n",
       "\n",
       "         num_shares_eps_12  num_shares_eps  num_shares_eps_split_adj  \\\n",
       "1769289            93.1090          93.109                   465.545   \n",
       "1769290            93.1090          93.109                   465.545   \n",
       "1769291            93.1090          93.109                   465.545   \n",
       "1769292            93.1090          93.109                   465.545   \n",
       "1769293            93.1090          93.109                   465.545   \n",
       "1769294            93.1090          93.109                   465.545   \n",
       "1769295            93.1090          93.109                   465.545   \n",
       "1769296            93.1090          93.109                   465.545   \n",
       "1769297            93.1090          93.109                   465.545   \n",
       "1769298            93.1090          93.109                   465.545   \n",
       "1769299            92.8998          92.271                   461.355   \n",
       "1769300            93.1820          94.240                   471.200   \n",
       "1769301            93.7018          95.187                   475.935   \n",
       "1769302            94.8668          97.757                   488.785   \n",
       "1769303            97.8400         104.077                   520.385   \n",
       "1769304           100.3890         104.392                   521.960   \n",
       "1769305           102.7525         104.784                   523.920   \n",
       "1769306           104.6238         105.242                   526.210   \n",
       "1769307           104.9950         105.556                   527.780   \n",
       "1769308           107.3490         113.763                   568.815   \n",
       "1769309           109.8183         114.712                   573.560   \n",
       "1769310           113.0608         118.194                   590.970   \n",
       "1769311           117.1523         121.862                   609.310   \n",
       "1769312           119.4210         122.802                   614.010   \n",
       "1769313           121.5828         123.473                   617.365   \n",
       "1769314           123.0980         124.250                   621.250   \n",
       "1769315           123.8633         124.911                   624.555   \n",
       "1769316           124.5390         125.497                   627.485   \n",
       "1769317           125.1513         125.947                   629.735   \n",
       "1769318           125.7620         126.689                   633.445   \n",
       "1769319           126.7930         129.006                   645.030   \n",
       "1769320           128.2020         131.100                   655.500   \n",
       "1769321           129.8678         132.676                   663.380   \n",
       "1769322           133.1915         139.983                   699.915   \n",
       "1769323           138.2108         148.991                   744.955   \n",
       "1769324           144.2120         155.024                   775.120   \n",
       "1769325           151.5318         162.129                   810.645   \n",
       "1769326           157.8433         165.212                   826.060   \n",
       "1769327           162.4288         167.294                   836.470   \n",
       "1769328           165.7580         168.314                   841.570   \n",
       "1769329           167.4915         169.146                   845.730   \n",
       "1769330           168.6890         169.997                   849.985   \n",
       "1769331           169.5928         170.893                   854.465   \n",
       "1769332           170.5250         172.026                   860.130   \n",
       "1769333           171.4763         172.989                   864.945   \n",
       "1769334           173.1453         176.654                   883.270   \n",
       "1769335           175.0065         179.000                   895.000   \n",
       "1769336           177.0000         180.000                   900.000   \n",
       "1769337           179.6635         183.000                   915.000   \n",
       "1769338           181.7500         186.000                   930.000   \n",
       "1769339           920.2500         937.000                   937.000   \n",
       "1769340           933.0000         951.000                   951.000   \n",
       "\n",
       "         split_adjusting_factor  \n",
       "1769289                     5.0  \n",
       "1769290                     5.0  \n",
       "1769291                     5.0  \n",
       "1769292                     5.0  \n",
       "1769293                     5.0  \n",
       "1769294                     5.0  \n",
       "1769295                     5.0  \n",
       "1769296                     5.0  \n",
       "1769297                     5.0  \n",
       "1769298                     5.0  \n",
       "1769299                     5.0  \n",
       "1769300                     5.0  \n",
       "1769301                     5.0  \n",
       "1769302                     5.0  \n",
       "1769303                     5.0  \n",
       "1769304                     5.0  \n",
       "1769305                     5.0  \n",
       "1769306                     5.0  \n",
       "1769307                     5.0  \n",
       "1769308                     5.0  \n",
       "1769309                     5.0  \n",
       "1769310                     5.0  \n",
       "1769311                     5.0  \n",
       "1769312                     5.0  \n",
       "1769313                     5.0  \n",
       "1769314                     5.0  \n",
       "1769315                     5.0  \n",
       "1769316                     5.0  \n",
       "1769317                     5.0  \n",
       "1769318                     5.0  \n",
       "1769319                     5.0  \n",
       "1769320                     5.0  \n",
       "1769321                     5.0  \n",
       "1769322                     5.0  \n",
       "1769323                     5.0  \n",
       "1769324                     5.0  \n",
       "1769325                     5.0  \n",
       "1769326                     5.0  \n",
       "1769327                     5.0  \n",
       "1769328                     5.0  \n",
       "1769329                     5.0  \n",
       "1769330                     5.0  \n",
       "1769331                     5.0  \n",
       "1769332                     5.0  \n",
       "1769333                     5.0  \n",
       "1769334                     5.0  \n",
       "1769335                     5.0  \n",
       "1769336                     5.0  \n",
       "1769337                     5.0  \n",
       "1769338                     5.0  \n",
       "1769339                     1.0  \n",
       "1769340                     1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating num of shares adjusted for split history\n",
    "quarterly_comp_data['num_shares_eps_12_split_adj'] = quarterly_comp_data['num_shares_eps_12'] * quarterly_comp_data['split_adjusting_factor']\n",
    "quarterly_comp_data['num_shares_eps_split_adj'] = quarterly_comp_data['num_shares_eps'] * quarterly_comp_data['split_adjusting_factor']\n",
    "\n",
    "# Checking calculations for tesla\n",
    "quarterly_comp_data[quarterly_comp_data['ticker'] == 'TSLA'][['company_name','quarter_end_date','num_shares_eps_12_split_adj',\n",
    "                                                              'num_shares_eps_12','num_shares_eps',\n",
    "                                                              'num_shares_eps_split_adj','split_adjusting_factor']]\n",
    "#.to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\junk.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noticed that there are few nulls in very imp columns\n",
    "quarterly_comp_data[['gvkey','fiscal_year','fiscal_quarter']].isnull().sum()\n",
    "# Let us drop any rows with nulls in those 3 key columns\n",
    "quarterly_comp_data = quarterly_comp_data.dropna(subset = ['gvkey','fiscal_year','fiscal_quarter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing level of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14745920061.0    2\n",
       "2596020181.0     2\n",
       "2797520002.0     2\n",
       "2730920071.0     2\n",
       "10880420112.0    2\n",
       "                ..\n",
       "15681620064.0    1\n",
       "349119924.0      1\n",
       "6485320093.0     1\n",
       "14019120204.0    1\n",
       "13253920062.0    1\n",
       "Name: possible_level_data, Length: 1828155, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarterly_comp_data['possible_level_data'] = quarterly_comp_data['gvkey'].astype(str) + quarterly_comp_data['fiscal_year'].astype(str) + quarterly_comp_data['fiscal_quarter'].astype(str)\n",
    "quarterly_comp_data['possible_level_data'].value_counts()\n",
    "# Realized that few companies have 2 Q1 (or Q2, Q3, Q4) rows in the same year\n",
    "# This is because companies can change quarter end date during the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1827341\n",
      "2        814\n",
      "Name: # Unique possible level data, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "gvkey_possible_level_mapping = pd.DataFrame(quarterly_comp_data['possible_level_data'].value_counts()).reset_index()\n",
    "gvkey_possible_level_mapping.columns = ['gvkey_fiscal_quarter_year', '# Unique possible level data']\n",
    "gvkey_possible_level_mapping.sort_values(by = ['# Unique possible level data'], ascending = False)\n",
    "print(gvkey_possible_level_mapping['# Unique possible level data'].value_counts())\n",
    "# Noticed that there are few gvkey_fiscal_quarter which repeat multiplee times in data\n",
    "# Let us store these problematic gvkey_fiscal_quarter (which appear more than once)\n",
    "problematic_gvkey_fiscal_qtr_yr = gvkey_possible_level_mapping[gvkey_possible_level_mapping['# Unique possible level data'] > 1]['gvkey_fiscal_quarter_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip_6digit_year_qtr</th>\n",
       "      <th>Num Unique possible level data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46137V20182.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46137V20184.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46137V20164.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46137V20183.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46137V20174.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cusip_6digit_year_qtr  Num Unique possible level data\n",
       "0         46137V20182.0                              78\n",
       "2         46137V20184.0                              78\n",
       "3         46137V20164.0                              78\n",
       "4         46137V20183.0                              78\n",
       "5         46137V20174.0                              78"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# While gvkey may be an ideal ID to use in level of data, it's important to figure out level of data with CUSIP too\n",
    "# This is because CRSP-compustat join needs to happens on CUSIP\n",
    "# So, compustat data 'HAS' to be unique at CUSIP, date level before joining\n",
    "\n",
    "quarterly_comp_data['cusip_6digit'] = quarterly_comp_data['cusip'].astype(str).str[0:6]\n",
    "quarterly_comp_data['cusip6_fiscal_year_fiscal_quarter'] = quarterly_comp_data['cusip_6digit'].astype(str) + quarterly_comp_data['fiscal_year'].astype(str) + quarterly_comp_data['fiscal_quarter'].astype(str)\n",
    "possible_level_cusip_mapping = pd.DataFrame(quarterly_comp_data['cusip6_fiscal_year_fiscal_quarter'].value_counts()).reset_index()\n",
    "possible_level_cusip_mapping.columns = ['cusip_6digit_year_qtr', 'Num Unique possible level data']\n",
    "possible_level_cusip_mapping['Num Unique possible level data'].value_counts()\n",
    "possible_level_cusip_mapping.sort_values(by = ['Num Unique possible level data'], ascending = False)[0:5]\n",
    "# Noticed that a combination of CUSIP (6 digit), qtr & yr appears 78 times. Needs further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1450343\n",
      "0     302554\n",
      "2       1680\n",
      "3         52\n",
      "5          7\n",
      "4          5\n",
      "Name: Num_unique_revenue, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip6_fiscal_year_fiscal_quarter</th>\n",
       "      <th>Num_unique_revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>664582</th>\n",
       "      <td>37291719994.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664585</th>\n",
       "      <td>37291720003.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664579</th>\n",
       "      <td>37291719991.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664580</th>\n",
       "      <td>37291719992.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664581</th>\n",
       "      <td>37291719993.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cusip6_fiscal_year_fiscal_quarter  Num_unique_revenue\n",
       "664582                     37291719994.0                   5\n",
       "664585                     37291720003.0                   5\n",
       "664579                     37291719991.0                   5\n",
       "664580                     37291719992.0                   5\n",
       "664581                     37291719993.0                   5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's possible that the CUSIP (6digit), qtr, yr combination that appear so many times are because of company restate results\n",
    "# Excluding the effect of restatements and then studying how often does a combination of CUSIP, qtr, yr have different revenue's\n",
    "# If a particular CUSIP has 2 different revenue's in the same qtr, I have a big problem\n",
    "\n",
    "Num_unique_revenue_cusip_qtr_yr_combination = pd.DataFrame(quarterly_comp_data[~quarterly_comp_data['possible_level_data'].isin(problematic_gvkey_fiscal_qtr_yr)]\n",
    "                                                           .groupby('cusip6_fiscal_year_fiscal_quarter')\n",
    "                                                           ['revenue'].nunique()).reset_index()\n",
    "Num_unique_revenue_cusip_qtr_yr_combination.columns = ['cusip6_fiscal_year_fiscal_quarter', 'Num_unique_revenue']\n",
    "print(Num_unique_revenue_cusip_qtr_yr_combination['Num_unique_revenue'].value_counts())\n",
    "# Realized I have a big problem\n",
    "\n",
    "Num_unique_revenue_cusip_qtr_yr_combination = Num_unique_revenue_cusip_qtr_yr_combination.sort_values(by=['Num_unique_revenue'],ascending = False)\n",
    "Num_unique_revenue_cusip_qtr_yr_combination[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>cusip_6digit</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>company_name</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>650066</th>\n",
       "      <td>372917104</td>\n",
       "      <td>372917</td>\n",
       "      <td>12233</td>\n",
       "      <td>GENZYME CORP</td>\n",
       "      <td>1999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>172.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449054</th>\n",
       "      <td>372917500</td>\n",
       "      <td>372917</td>\n",
       "      <td>117298</td>\n",
       "      <td>GENZYME MOLECULAR ONCOLOGY</td>\n",
       "      <td>1999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453678</th>\n",
       "      <td>372917401</td>\n",
       "      <td>372917</td>\n",
       "      <td>118653</td>\n",
       "      <td>GENZYME TISSUE REPAIR</td>\n",
       "      <td>1999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464459</th>\n",
       "      <td>372917609</td>\n",
       "      <td>372917</td>\n",
       "      <td>121742</td>\n",
       "      <td>GENZYME SURGICAL PRODUCTS</td>\n",
       "      <td>1999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536708</th>\n",
       "      <td>372917708</td>\n",
       "      <td>372917</td>\n",
       "      <td>143176</td>\n",
       "      <td>GENZYME BIOSURGERY</td>\n",
       "      <td>1999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36.594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             cusip cusip_6digit   gvkey                company_name  \\\n",
       "650066   372917104       372917   12233                GENZYME CORP   \n",
       "1449054  372917500       372917  117298  GENZYME MOLECULAR ONCOLOGY   \n",
       "1453678  372917401       372917  118653       GENZYME TISSUE REPAIR   \n",
       "1464459  372917609       372917  121742   GENZYME SURGICAL PRODUCTS   \n",
       "1536708  372917708       372917  143176          GENZYME BIOSURGERY   \n",
       "\n",
       "         fiscal_year  fiscal_quarter  revenue  \n",
       "650066          1999             4.0  172.726  \n",
       "1449054         1999             4.0    1.164  \n",
       "1453678         1999             4.0    6.032  \n",
       "1464459         1999             4.0   30.562  \n",
       "1536708         1999             4.0   36.594  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realized that there are few companies like Genzyme, liberty media which have acquired companies in the past\n",
    "quarterly_comp_data[quarterly_comp_data['cusip6_fiscal_year_fiscal_quarter'] == '37291719994.0'][['cusip','cusip_6digit','gvkey',\n",
    "                                                                                                  'company_name','fiscal_year',\n",
    "                                                                                                  'fiscal_quarter','revenue']]\n",
    "# In this case, Genzyme acquired genzyme surgical products, they both got same CUSIP\n",
    "# Not every acquisition leads to scenario like this. Salesforce acquired Tableau but those rows don't have this problem\n",
    "# There is something special about Genzyme, liberty media & few others where CUSIP 6 digit is not fully enough to uniquely identify a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realized that there are 150 companies that have this problem (CUSIP 6 digit not being enough to uniquely identify a company)\n",
    "problematic_cusip = Num_unique_revenue_cusip_qtr_yr_combination[Num_unique_revenue_cusip_qtr_yr_combination['Num_unique_revenue'] > 1]['cusip6_fiscal_year_fiscal_quarter'].unique()\n",
    "len(quarterly_comp_data[quarterly_comp_data['cusip6_fiscal_year_fiscal_quarter'].isin(problematic_cusip)]['company_name'].unique())\n",
    "# So, CUSIP (6 digit) is not a terrible ID to use. But there are few companies where using CUSIP (6 digit) can be dangerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1454128\n",
      "0     373213\n",
      "Name: Num_unique_revenue, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# What if I use the full CUSIP (9 digits) rather than 6digit CUSIP?\n",
    "quarterly_comp_data['possible_level_data_cusip'] = quarterly_comp_data['cusip'].astype(str) + quarterly_comp_data['fiscal_year'].astype(str) + quarterly_comp_data['fiscal_quarter'].astype(str)\n",
    "cusip_possible_level_mapping = pd.DataFrame(quarterly_comp_data['possible_level_data_cusip'].value_counts()).reset_index()\n",
    "cusip_possible_level_mapping.columns = ['cusip_year_qtr', 'Num Unique possible level data']\n",
    "Num_unique_revenue_cusip_qtr_yr = pd.DataFrame(quarterly_comp_data[~quarterly_comp_data['possible_level_data'].isin(problematic_gvkey_fiscal_qtr_yr)]\n",
    "                                               .groupby('possible_level_data_cusip')['revenue'].nunique()).reset_index()\n",
    "Num_unique_revenue_cusip_qtr_yr.columns = ['cusip_fiscal_year_fiscal_quarter', 'Num_unique_revenue']\n",
    "print(Num_unique_revenue_cusip_qtr_yr['Num_unique_revenue'].value_counts())\n",
    "# The big problem goes away\n",
    "# Apart from the earlier identified few problematic gvkey rows, data is unique at CUSIP (9 digit), qtr & year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decided to take the first quarter rows that appears in every year (if there are multiple rows with the same gvkey, year & quarter)\n",
    "quarterly_comp_data = quarterly_comp_data.sort_values(by=['gvkey','fiscal_year','fiscal_quarter','reporting_year_quarter','result_reported_date'],ascending = True)\n",
    "quarterly_comp_data = quarterly_comp_data.drop_duplicates(subset=['gvkey','fiscal_year','fiscal_quarter'], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-6d622526d0d4>:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  quarterly_comp_data['company_mod'] = quarterly_comp_data['company_name'].str.replace('[^a-zA-Z]', '')\n"
     ]
    }
   ],
   "source": [
    "# Running a quick test to see if same company can have 2 gvkeys(how good is compustat mapping)\n",
    "# For example, Let's see if one company name can have 2 different gvkeys\n",
    "# But before doing that, I need to prepare company name text column\n",
    "\n",
    "# Taking only alphabets in companies name\n",
    "quarterly_comp_data['company_mod'] = quarterly_comp_data['company_name'].str.replace('[^a-zA-Z]', '')\n",
    "# Removing all white spaces (even between company name)\n",
    "quarterly_comp_data['company_mod'] = quarterly_comp_data['company_mod'].str.replace(' ', '')\n",
    "# Converting all characters to upper\n",
    "quarterly_comp_data['company_mod'] = quarterly_comp_data['company_mod'].str.upper()\n",
    "\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_name'].astype(str).str[-4:] == '-OLD', quarterly_comp_data['company_mod'].astype(str).str[:-4], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_name'].astype(str).str[-3:] == 'NEW', quarterly_comp_data['company_mod'].astype(str).str[:-3], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_name'].astype(str).str[-4:] == 'CL A', quarterly_comp_data['company_mod'].astype(str).str[:-4], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_name'].astype(str).str[-4:] == 'CL B', quarterly_comp_data['company_mod'].astype(str).str[:-4], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_name'].astype(str).str[-4:] == 'CL C', quarterly_comp_data['company_mod'].astype(str).str[:-4], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_name'].astype(str).str[-3:] == ' FD', quarterly_comp_data['company_mod'].astype(str).str[:-3], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_name'].astype(str).str[-3:] == ' TR', quarterly_comp_data['company_mod'].astype(str).str[:-3], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_name'].astype(str).str[-3:] == ' CO', quarterly_comp_data['company_mod'].astype(str).str[:-3], quarterly_comp_data['company_mod'])\n",
    "\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-3:] == 'LTD', quarterly_comp_data['company_mod'].astype(str).str[:-3], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-3:] == 'INC', quarterly_comp_data['company_mod'].astype(str).str[:-3], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-3:] == 'ETF', quarterly_comp_data['company_mod'].astype(str).str[:-3], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-3:] == 'PLC', quarterly_comp_data['company_mod'].astype(str).str[:-3], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-4:] == 'CORP', quarterly_comp_data['company_mod'].astype(str).str[:-4], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-4:] == 'FUND', quarterly_comp_data['company_mod'].astype(str).str[:-4], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-5:] == 'COLTD', quarterly_comp_data['company_mod'].astype(str).str[:-5], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-5:] == 'TRUST', quarterly_comp_data['company_mod'].astype(str).str[:-5], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-6:] == 'BERHAD', quarterly_comp_data['company_mod'].astype(str).str[:-6], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-7:] == 'LIMITED', quarterly_comp_data['company_mod'].astype(str).str[:-7], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-7:] == 'COMPANY', quarterly_comp_data['company_mod'].astype(str).str[:-7], quarterly_comp_data['company_mod'])\n",
    "quarterly_comp_data['company_mod'] = np.where(quarterly_comp_data['company_mod'].astype(str).str[-11:] == 'CORPORATION', quarterly_comp_data['company_mod'].astype(str).str[:-11], quarterly_comp_data['company_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     38672\n",
      "2       232\n",
      "3        23\n",
      "4         7\n",
      "6         2\n",
      "9         2\n",
      "10        2\n",
      "5         1\n",
      "7         1\n",
      "8         1\n",
      "Name: # Unique gvkey, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>1093</td>\n",
       "      <td>ACCURAY CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692761</th>\n",
       "      <td>176670</td>\n",
       "      <td>ACCURAY INC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gvkey  company_name\n",
       "5114       1093  ACCURAY CORP\n",
       "1692761  176670   ACCURAY INC"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_mod_gvkey_uniques = pd.DataFrame(quarterly_comp_data.groupby('company_mod')['gvkey'].nunique()).reset_index()\n",
    "company_mod_gvkey_uniques.columns = ['company_mod', '# Unique gvkey']\n",
    "print(company_mod_gvkey_uniques['# Unique gvkey'].value_counts())\n",
    "\n",
    "# Clearly, there's some cases where same company name can have different gvkeys\n",
    "company_mod_gvkey_uniques[company_mod_gvkey_uniques['# Unique gvkey'] == 2]\n",
    "quarterly_comp_data[quarterly_comp_data['company_mod'] == 'ACCURAY'][['gvkey','company_name']].drop_duplicates()\n",
    "# But this problem happens for <1% companies\n",
    "# So, I am just going to ignore this problem and assume that gvkey is the ultimate unique company identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intial grouping of columns in quarterly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_group_col = ['ticker','cusip','gvkey','company_name']\n",
    "date_group_col = ['quarter_end_date','fiscal_year','fiscal_quarter','fiscal_year_end_month','reporting_year_quarter', \n",
    "                  'fiscal_year_quarter','reporting_frequency','preliminary_date','result_reported_date']\n",
    "general_group_col = ['split_adjusting_factor','currency_exchange_rate','financial_normal','stock_exchange',\n",
    "                     'active_or_inactive_status','Mcap','business_description','gics_sub_industry','NAICS','sic_code',\n",
    "                     'S&P_industry_sector_code','S&P_grade']\n",
    "balance_sheet_group = ['total_current_asset','asset_total','share_holder_equity','equity_total','cash_st_investment','cash','total_LT_debt',\n",
    "                       'goodwill_amortization','goodwill','liability_total','ppe_total','receivable_total']\n",
    "income_statement_group = ['num_shares_eps_12','num_d_shares_eps_12','num_d_shares_eps','num_shares_eps',\n",
    "                          'eps_d_excl_extraordinary_12','eps_d_12M','core_eps_d_12M','eps_d','core_eps_d',\n",
    "                          'eps_d_excl_extraordinary','eps_12M','core_eps_12M','eps','core_eps','eps_excl','eps_excl_12M',\n",
    "                          'income_excl_extraordinary', 'income','revenue','core_earnings_12M','core_earnings', \n",
    "                          'extraordinary_income','operating_expense']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core_eps_d_12M       1565759\n",
      "core_eps_d           1537508\n",
      "core_eps_12M         1565705\n",
      "core_eps             1537474\n",
      "core_earnings_12M    1553099\n",
      "core_earnings        1523653\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2001    33814\n",
       "2002    32422\n",
       "2003    30804\n",
       "2004    29055\n",
       "2005    27097\n",
       "2006    24986\n",
       "2007    22212\n",
       "2008    20598\n",
       "2009    19180\n",
       "2010    17397\n",
       "2011    15864\n",
       "2012    13911\n",
       "2013    11666\n",
       "2014     5461\n",
       "2015       35\n",
       "Name: fiscal_year, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many null values are there in core eps columns\n",
    "print(quarterly_comp_data[['core_eps_d_12M','core_eps_d','core_eps_12M','core_eps','core_earnings_12M','core_earnings']].isnull().sum())\n",
    "# Note that the 12M EPS varaibles has a lot of nulls in Compustat because 12M is not calculated by Compustat (using quarterly EPS)\n",
    "# 12M EPS is directly taken from companies filing (available only if company has provided)\n",
    "\n",
    "# Realized that core earnings columns are available only after 2000\n",
    "quarterly_comp_data[~quarterly_comp_data['core_earnings'].isnull()]['fiscal_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    290647.000000\n",
      "mean          0.286072\n",
      "std         379.496612\n",
      "min      -49718.000000\n",
      "25%          -0.040000\n",
      "50%           0.050000\n",
      "75%           0.320000\n",
      "max      186470.000000\n",
      "Name: core_eps_d, dtype: float64\n",
      "count    1.462307e+06\n",
      "mean    -3.527396e+00\n",
      "std      1.288073e+04\n",
      "min     -4.241253e+06\n",
      "25%     -3.000000e-02\n",
      "50%      7.000000e-02\n",
      "75%      3.700000e-01\n",
      "max      1.343972e+07\n",
      "Name: eps_d, dtype: float64\n",
      "count    1.461775e+06\n",
      "mean    -4.147751e-02\n",
      "std      1.255810e+04\n",
      "min     -4.241253e+06\n",
      "25%     -3.000000e-02\n",
      "50%      7.000000e-02\n",
      "75%      3.700000e-01\n",
      "max      1.343972e+07\n",
      "Name: eps_d_excl_extraordinary, dtype: float64\n",
      "(153979, 72)\n",
      "(124867, 72)\n",
      "count    272385.000000\n",
      "mean         -5.497998\n",
      "std         182.904087\n",
      "min      -18900.000000\n",
      "25%          -6.060606\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max       48440.000000\n",
      "Name: core_eps_minus_eps_d_excl_extraodrinary, dtype: float64\n",
      "count    151071.000000\n",
      "mean         -9.913035\n",
      "std         245.509033\n",
      "min      -18900.000000\n",
      "25%         -20.454545\n",
      "50%          -4.395604\n",
      "75%           6.250000\n",
      "max       48440.000000\n",
      "Name: core_eps_minus_eps_d_excl_extraodrinary, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Selected three eps related columns - core_eps_d,eps_d and eps_d_excl_extraordinary\n",
    "# Let us understand the relationship between these 3 eps columns\n",
    "# I understood by below steps that it's better to consider core_eps_d and eps_d_excl_extr because eps_d has no direct comparison values\n",
    "# Let us analyze the rows where core_eps & eps_excl_extraordinary are not matching. \n",
    "# Let's create a percentage difference column and study the mean 25%, 50% and 75% \n",
    "\n",
    "print(quarterly_comp_data['core_eps_d'].describe())\n",
    "print(quarterly_comp_data['eps_d'].describe())\n",
    "print(quarterly_comp_data['eps_d_excl_extraordinary'].describe())\n",
    "\n",
    "print(quarterly_comp_data[(~(quarterly_comp_data['core_eps_d'].isnull()) &\n",
    "                    ~(quarterly_comp_data['eps_d_excl_extraordinary'].isnull()) &\n",
    "                    (quarterly_comp_data['core_eps_d']!=quarterly_comp_data['eps_d_excl_extraordinary']))].shape)\n",
    "\n",
    "print(quarterly_comp_data[(~(quarterly_comp_data['core_eps_d'].isnull()) &\n",
    "                    ~(quarterly_comp_data['eps_d'].isnull()) &\n",
    "                    (quarterly_comp_data['core_eps_d']==quarterly_comp_data['eps_d']))].shape)\n",
    "\n",
    "#If denominator is zero, replacing the percentage calculation with none\n",
    "quarterly_comp_data['core_eps_minus_eps_d_excl_extraodrinary'] = ((quarterly_comp_data['core_eps_d'] - quarterly_comp_data['eps_d_excl_extraordinary'])/(quarterly_comp_data['eps_d_excl_extraordinary']))*100\n",
    "quarterly_comp_data['core_eps_minus_eps_d_excl_extraodrinary'][np.isinf(quarterly_comp_data['core_eps_minus_eps_d_excl_extraodrinary'])] = None\n",
    "# Need to take care of infinity values\n",
    "\n",
    "# Checking percentage difference column distribution in overall data\n",
    "print(quarterly_comp_data['core_eps_minus_eps_d_excl_extraodrinary'].describe())\n",
    "\n",
    "# Checking percentage difference distribution only for the rows where values don't match\n",
    "print(quarterly_comp_data[quarterly_comp_data['core_eps_d'] != quarterly_comp_data['eps_d_excl_extraordinary']]['core_eps_minus_eps_d_excl_extraodrinary'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    440786.000000\n",
       "mean         -1.976767\n",
       "std         278.093466\n",
       "min     -118833.168317\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           0.000000\n",
       "max       80775.000000\n",
       "Name: seqq_teqq_percentage_difference, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the difference between seqq and teqq\n",
    "quarterly_comp_data['seqq_teqq_percentage_difference'] = ((quarterly_comp_data['share_holder_equity'] - quarterly_comp_data['equity_total'])/quarterly_comp_data['equity_total'])*100\n",
    "quarterly_comp_data['seqq_teqq_percentage_difference'].describe()\n",
    "# Both columns are very close in general\n",
    "# quarterly_comp_data[((quarterly_comp_data['share_holder_equity'].isnull())&(~quarterly_comp_data['equity_total'].isnull()))].shape\n",
    "# I noticed that when share holder equity is null, total equity is also mostly null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As it's decided to continue with core eps_d values and eps_d_excl_extr values I observed that it will be more approximate if I combine both the columns and create a new column\n",
    "#So, I created new columns for all core_eps_d values combined with eps_s_excl_values using fillna()\n",
    "quarterly_comp_data['eps_d_core_excl_extr'] =  quarterly_comp_data['core_eps_d'].fillna(quarterly_comp_data['eps_d_excl_extraordinary'])\n",
    "quarterly_comp_data['eps_d_core_excl_extr_12M'] = quarterly_comp_data['core_eps_d_12M'].fillna(quarterly_comp_data['eps_d_excl_extraordinary_12M'])\n",
    "quarterly_comp_data['eps_core_excl_extr_12M'] = quarterly_comp_data['core_eps_12M'].fillna(quarterly_comp_data['eps_excl_extraordinary_12M'])\n",
    "quarterly_comp_data['eps_core_excl_extr'] = quarterly_comp_data['core_eps'].fillna(quarterly_comp_data['eps_excl_extraordinary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested relationship between preliminary date and result reported date\n",
    "# Concluded preliminary date to be excluded because both preliminary date and result date are mostly same\n",
    "\n",
    "# quarterly_comp_data['preliminary_date'].isnull().sum()\n",
    "# quarterly_comp_data['preliminary_date'] = pd.to_datetime(quarterly_comp_data['preliminary_date'])\n",
    "# quarterly_comp_data['result_reported_date'] = pd.to_datetime(quarterly_comp_data['result_reported_date'])\n",
    "# quarterly_comp_data['preliminary_actual_result_gap'] = quarterly_comp_data['preliminary_date'] - quarterly_comp_data['result_reported_date']\n",
    "# quarterly_comp_data['preliminary_actual_result_gap'].value_counts()\n",
    "# quarterly_comp_data['preliminary_actual_result_gap'] = quarterly_comp_data['preliminary_actual_result_gap'].dt.days\n",
    "# quarterly_comp_data[quarterly_comp_data['preliminary_actual_result_gap'] < -5].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating derived variables in fundamental quarterly compustat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested relationship between preliminary date and result reported date, concluded preliminary date to be excluded because both preliminary date and result date are mostly same\n",
    "# quarterly_comp_data['preliminary_date'].isnull().sum()\n",
    "# quarterly_comp_data['preliminary_date'] = pd.to_datetime(quarterly_comp_data['preliminary_date'])\n",
    "# quarterly_comp_data['result_reported_date'] = pd.to_datetime(quarterly_comp_data['result_reported_date'])\n",
    "# quarterly_comp_data['preliminary_actual_result_gap'] = quarterly_comp_data['preliminary_date'] - quarterly_comp_data['result_reported_date']\n",
    "# quarterly_comp_data['preliminary_actual_result_gap'].value_counts()\n",
    "# quarterly_comp_data['preliminary_actual_result_gap'] = quarterly_comp_data['preliminary_actual_result_gap'].dt.days\n",
    "# quarterly_comp_data[quarterly_comp_data['preliminary_actual_result_gap'] < -5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    466205\n",
       "2.0    461336\n",
       "3.0    455655\n",
       "4.0    444959\n",
       "Name: fiscal_quarter, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If a companies FY ends on March 31st, March result fiscal quarter would be 4\n",
    "quarterly_comp_data['fiscal_quarter'].value_counts()\n",
    "# Let's take only fiscal_quarter only from financial info because rest of the date columns can be created from the date columns in price data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rollins, Inc., through its subsidiaries, provides pest and termite control services to residential and commercial customers.                                                                                                                                                                               218\n",
       "The Kraft Heinz Company, together with its subsidiaries, manufactures and markets food and beverage products in the United States, Canada, the United Kingdom, and internationally.                                                                                                                        218\n",
       "Seaboard Corporation operates as an agribusiness and transportation company worldwide. It operates through six segments: Pork, Commodity Trading and Milling (CT&M), Marine, Sugar and Alcohol, Power, and Turkey.                                                                                         218\n",
       "Kimberly-Clark Corporation, together with its subsidiaries, manufactures and markets personal care and consumer tissue products worldwide. It operates through three segments: Personal Care, Consumer Tissue, and K-C Professional.                                                                       218\n",
       "Winnebago Industries, Inc. manufactures and sells recreation vehicles and marine products primarily for use in leisure travel and outdoor recreation activities.                                                                                                                                           218\n",
       "                                                                                                                                                                                                                                                                                                          ... \n",
       "Far Peak Acquisition Corporation intends to effect a merger, share exchange, asset acquisition, share purchase, reorganization, or similar business combination with one or more businesses or entities in the financial technology, technology, or financial services industries.                           1\n",
       "Amplify ETF Trust - Amplify Pure Junior Gold Miners ETF is an exchange traded fund launched and managed by Amplify Investments LLC. It invests in public equity markets of global region. It invests in stocks of companies operating across junior and exploratory gold mining sectors.                     1\n",
       "Exchange Listed Funds Trust - Cabana Target Drawdown 10 ETF is an exchange traded fund of funds launched and managed by Exchange Traded Concepts, LLC. It is co-managed by Cabana, LLC.                                                                                                                      1\n",
       "KraneShares Trust - KFA Value Line Dynamic Core Equity Index ETF is an exchange traded fund launched by Ultimus Managers Trust. The fund is managed by Karner Blue Capital, LLC. It invests in public equity markets of the United States.                                                                   1\n",
       "Invesco S&P/TSX Composite ESG Index ETF is an exchange traded fund launched by Invesco Canada Ltd. The fund is managed by Invesco Capital Management LLC. It invests in public equity markets. It invests directly and through other funds in stocks of companies operating across diversified sectors.      1\n",
       "Name: business_description, Length: 34257, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see popular values in business description\n",
    "\n",
    "quarterly_comp_data['business_description'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if any value can be extracted from business description columns\n",
    "\n",
    "# Commenting out the word cloud block as it takes a lot of time to run\n",
    "#quarterly_comp_data['business_decription'] = quarterly_comp_data['business_decription'].astype(str)\n",
    "#text = \" \".join(x for x in quarterly_comp_data['business_description'])\n",
    "#wordcloud = WordCloud().generate(text)\n",
    "#wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
    "#plt.figure(figsize=(15,10))\n",
    "#plt.imshow(wordlcloud, interpolation='bilinear')\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "# Nice word cloud but can't spot any keywords immediately that I could extract & add structured columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q     1802133\n",
       "SA      26022\n",
       "Name: reporting_frequency, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As known that not every company reports quarterly results in this dataset. Let's see the distribution\n",
    "quarterly_comp_data['reporting_frequency'].value_counts()\n",
    "# Please note that even for SA companies, compustat sometimes creates quarterly rows (based on their data dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    444959\n",
       "3.0     12145\n",
       "2.0      7001\n",
       "1.0      6493\n",
       "Name: fiscal_quarter, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I need to know if Q2 is last for SA companies or if a lot of SA companies end in Q4\n",
    "# Tried to look at the last quarter for every year for every company\n",
    "dummy_comp = pd.DataFrame(quarterly_comp_data.groupby(['gvkey','fiscal_year'])['fiscal_quarter'].max()).reset_index()\n",
    "dummy_comp['fiscal_quarter'].value_counts()\n",
    "#Realized that for some companies, last Q is Q3 because that's when they went bankrupt (or) got acquired\n",
    "#So, just a simple check is not enough to identify the true last quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.788852e+06\n",
       "mean     9.157007e+01\n",
       "std      2.936928e+01\n",
       "min      0.000000e+00\n",
       "25%      9.100000e+01\n",
       "50%      9.200000e+01\n",
       "75%      9.200000e+01\n",
       "max      7.487000e+03\n",
       "Name: days_gap_prev_quarter, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at how many days gap can exsist between 2 subsequent quarters\n",
    "\n",
    "quarterly_comp_data = quarterly_comp_data.sort_values(by=['gvkey','quarter_end_date'],ascending = True)\n",
    "\n",
    "quarterly_comp_data['quarter_end_date'] = pd.to_datetime(quarterly_comp_data['quarter_end_date'], errors='coerce')\n",
    "quarterly_comp_data['quarter_end_date_shift'] = quarterly_comp_data['quarter_end_date'].shift(1)\n",
    "quarterly_comp_data['quarter_end_date_shift'] = pd.to_datetime(quarterly_comp_data['quarter_end_date_shift'],errors='coerce')\n",
    "quarterly_comp_data['days_gap_prev_quarter'] = quarterly_comp_data['quarter_end_date'] - quarterly_comp_data['quarter_end_date_shift']\n",
    "quarterly_comp_data['days_gap_prev_quarter'] = quarterly_comp_data['days_gap_prev_quarter'].dt.days\n",
    "\n",
    "quarterly_comp_data['gvkey_shift'] = quarterly_comp_data['gvkey'].shift(1)\n",
    "quarterly_comp_data['days_gap_prev_quarter'] = np.where(quarterly_comp_data['gvkey'] == quarterly_comp_data['gvkey_shift'], \n",
    "                                                       quarterly_comp_data['days_gap_prev_quarter'], None)\n",
    "pd.to_numeric(quarterly_comp_data['days_gap_prev_quarter']).describe()\n",
    "# Realized that it's possible for the gap btw. 2 quarters > 180 days\n",
    "# This is because a gvkey could get de-listed in 1990 and appear again in 2010\n",
    "# Also realized that it's possible for the gap to be < 90 days\n",
    "# This is because sometimes, companies restate financial statements in the middle of quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    1828137\n",
       "2         18\n",
       "Name: final_quarter_id, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying the correct last quarter in a financial year for every company. \n",
    "quarterly_comp_data['final_quarter_id'] = np.where(quarterly_comp_data['reporting_frequency'] == 'Q' , 4, None)\n",
    "quarterly_comp_data['final_quarter_id'] = np.where((quarterly_comp_data['reporting_frequency'] == 'SA') &\n",
    "                                                   (quarterly_comp_data['days_gap_prev_quarter']>170) &\n",
    "                                                   (quarterly_comp_data['days_gap_prev_quarter']<190), 2, 4)\n",
    "quarterly_comp_data['final_quarter_id'].value_counts()\n",
    "# Realized that for almost all cases, compustat is actually creating rows for quarters when results are not declared\n",
    "# Basically, even for Semi annual companies, I almost always have Q1 & Q3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1828155, 82)\n",
      "412781\n",
      "(26022, 82)\n",
      "16818\n"
     ]
    }
   ],
   "source": [
    "# However, interestingly, for SA companies, Mar & Sep rows are almost never filled despite being created \n",
    "# For example, they haven't imputed Jun data into Sep row\n",
    "\n",
    "print(quarterly_comp_data.shape)\n",
    "print(quarterly_comp_data['asset_total'].isnull().sum())\n",
    "\n",
    "print(quarterly_comp_data[quarterly_comp_data['reporting_frequency'] == 'SA'].shape)\n",
    "print(quarterly_comp_data[quarterly_comp_data['reporting_frequency'] == 'SA']['asset_total'].isnull().sum())\n",
    "\n",
    "# For SA rows, more than half don't have total asset value - because as mentioned compustat created quarterly row but didn't impute financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>num_nulls_asset_by_year</th>\n",
       "      <th>num_non_nulls_asset_by_year</th>\n",
       "      <th>% null rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1966</td>\n",
       "      <td>315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1967</td>\n",
       "      <td>7174</td>\n",
       "      <td>354.0</td>\n",
       "      <td>95.297556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1968</td>\n",
       "      <td>9697</td>\n",
       "      <td>371.0</td>\n",
       "      <td>96.315058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1969</td>\n",
       "      <td>10071</td>\n",
       "      <td>389.0</td>\n",
       "      <td>96.281071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1970</td>\n",
       "      <td>10158</td>\n",
       "      <td>642.0</td>\n",
       "      <td>94.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1971</td>\n",
       "      <td>7814</td>\n",
       "      <td>3308.0</td>\n",
       "      <td>70.257148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1972</td>\n",
       "      <td>7870</td>\n",
       "      <td>3567.0</td>\n",
       "      <td>68.811751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1973</td>\n",
       "      <td>7899</td>\n",
       "      <td>3726.0</td>\n",
       "      <td>67.948387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1974</td>\n",
       "      <td>7703</td>\n",
       "      <td>3930.0</td>\n",
       "      <td>66.216797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1975</td>\n",
       "      <td>4161</td>\n",
       "      <td>7573.0</td>\n",
       "      <td>35.461053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1976</td>\n",
       "      <td>680</td>\n",
       "      <td>10925.0</td>\n",
       "      <td>5.859543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1977</td>\n",
       "      <td>305</td>\n",
       "      <td>11565.0</td>\n",
       "      <td>2.569503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1978</td>\n",
       "      <td>291</td>\n",
       "      <td>11363.0</td>\n",
       "      <td>2.496997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1979</td>\n",
       "      <td>266</td>\n",
       "      <td>11394.0</td>\n",
       "      <td>2.281304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1980</td>\n",
       "      <td>426</td>\n",
       "      <td>11165.0</td>\n",
       "      <td>3.675265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1981</td>\n",
       "      <td>6144</td>\n",
       "      <td>14146.0</td>\n",
       "      <td>30.280927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1982</td>\n",
       "      <td>5197</td>\n",
       "      <td>21949.0</td>\n",
       "      <td>19.144625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1983</td>\n",
       "      <td>3182</td>\n",
       "      <td>25441.0</td>\n",
       "      <td>11.116934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1984</td>\n",
       "      <td>2993</td>\n",
       "      <td>26387.0</td>\n",
       "      <td>10.187202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1985</td>\n",
       "      <td>4378</td>\n",
       "      <td>27090.0</td>\n",
       "      <td>13.912546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1986</td>\n",
       "      <td>5318</td>\n",
       "      <td>28218.0</td>\n",
       "      <td>15.857586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1987</td>\n",
       "      <td>4917</td>\n",
       "      <td>29203.0</td>\n",
       "      <td>14.410903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1988</td>\n",
       "      <td>5243</td>\n",
       "      <td>28842.0</td>\n",
       "      <td>15.382133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1989</td>\n",
       "      <td>5518</td>\n",
       "      <td>28329.0</td>\n",
       "      <td>16.302774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1990</td>\n",
       "      <td>6169</td>\n",
       "      <td>27943.0</td>\n",
       "      <td>18.084545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1991</td>\n",
       "      <td>7903</td>\n",
       "      <td>29103.0</td>\n",
       "      <td>21.355996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1992</td>\n",
       "      <td>9062</td>\n",
       "      <td>30392.0</td>\n",
       "      <td>22.968520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1993</td>\n",
       "      <td>9383</td>\n",
       "      <td>33229.0</td>\n",
       "      <td>22.019619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1994</td>\n",
       "      <td>7885</td>\n",
       "      <td>37077.0</td>\n",
       "      <td>17.537031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1995</td>\n",
       "      <td>10122</td>\n",
       "      <td>38793.0</td>\n",
       "      <td>20.693039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1996</td>\n",
       "      <td>8921</td>\n",
       "      <td>40460.0</td>\n",
       "      <td>18.065653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1997</td>\n",
       "      <td>7113</td>\n",
       "      <td>41471.0</td>\n",
       "      <td>14.640622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1998</td>\n",
       "      <td>8321</td>\n",
       "      <td>42056.0</td>\n",
       "      <td>16.517458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1999</td>\n",
       "      <td>8022</td>\n",
       "      <td>43081.0</td>\n",
       "      <td>15.697709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2000</td>\n",
       "      <td>5797</td>\n",
       "      <td>44172.0</td>\n",
       "      <td>11.601193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2001</td>\n",
       "      <td>5231</td>\n",
       "      <td>42460.0</td>\n",
       "      <td>10.968527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2002</td>\n",
       "      <td>5517</td>\n",
       "      <td>40451.0</td>\n",
       "      <td>12.001827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2003</td>\n",
       "      <td>6278</td>\n",
       "      <td>38787.0</td>\n",
       "      <td>13.930989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2004</td>\n",
       "      <td>6457</td>\n",
       "      <td>37967.0</td>\n",
       "      <td>14.534936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2005</td>\n",
       "      <td>6979</td>\n",
       "      <td>37415.0</td>\n",
       "      <td>15.720593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2006</td>\n",
       "      <td>7352</td>\n",
       "      <td>36877.0</td>\n",
       "      <td>16.622578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2007</td>\n",
       "      <td>8081</td>\n",
       "      <td>36135.0</td>\n",
       "      <td>18.276190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2008</td>\n",
       "      <td>8733</td>\n",
       "      <td>34932.0</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2009</td>\n",
       "      <td>9512</td>\n",
       "      <td>33713.0</td>\n",
       "      <td>22.005784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>10375</td>\n",
       "      <td>33403.0</td>\n",
       "      <td>23.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2011</td>\n",
       "      <td>11174</td>\n",
       "      <td>33374.0</td>\n",
       "      <td>25.083056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012</td>\n",
       "      <td>12176</td>\n",
       "      <td>35026.0</td>\n",
       "      <td>25.795517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013</td>\n",
       "      <td>12394</td>\n",
       "      <td>35065.0</td>\n",
       "      <td>26.115173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014</td>\n",
       "      <td>12081</td>\n",
       "      <td>34889.0</td>\n",
       "      <td>25.720673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015</td>\n",
       "      <td>12332</td>\n",
       "      <td>33956.0</td>\n",
       "      <td>26.641894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>13321</td>\n",
       "      <td>32405.0</td>\n",
       "      <td>29.132222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>14011</td>\n",
       "      <td>31404.0</td>\n",
       "      <td>30.851040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>14744</td>\n",
       "      <td>30712.0</td>\n",
       "      <td>32.435762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>14915</td>\n",
       "      <td>29729.0</td>\n",
       "      <td>33.408745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>14472</td>\n",
       "      <td>27969.0</td>\n",
       "      <td>34.099102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2021</td>\n",
       "      <td>228</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>17.826427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fiscal_year  num_nulls_asset_by_year  num_non_nulls_asset_by_year  \\\n",
       "51         1966                      315                          NaN   \n",
       "30         1967                     7174                        354.0   \n",
       "14         1968                     9697                        371.0   \n",
       "13         1969                    10071                        389.0   \n",
       "11         1970                    10158                        642.0   \n",
       "27         1971                     7814                       3308.0   \n",
       "26         1972                     7870                       3567.0   \n",
       "24         1973                     7899                       3726.0   \n",
       "28         1974                     7703                       3930.0   \n",
       "46         1975                     4161                       7573.0   \n",
       "49         1976                      680                      10925.0   \n",
       "52         1977                      305                      11565.0   \n",
       "53         1978                      291                      11363.0   \n",
       "54         1979                      266                      11394.0   \n",
       "50         1980                      426                      11165.0   \n",
       "36         1981                     6144                      14146.0   \n",
       "43         1982                     5197                      21949.0   \n",
       "47         1983                     3182                      25441.0   \n",
       "48         1984                     2993                      26387.0   \n",
       "45         1985                     4378                      27090.0   \n",
       "40         1986                     5318                      28218.0   \n",
       "44         1987                     4917                      29203.0   \n",
       "41         1988                     5243                      28842.0   \n",
       "38         1989                     5518                      28329.0   \n",
       "35         1990                     6169                      27943.0   \n",
       "23         1991                     7903                      29103.0   \n",
       "17         1992                     9062                      30392.0   \n",
       "16         1993                     9383                      33229.0   \n",
       "25         1994                     7885                      37077.0   \n",
       "12         1995                    10122                      38793.0   \n",
       "18         1996                     8921                      40460.0   \n",
       "31         1997                     7113                      41471.0   \n",
       "20         1998                     8321                      42056.0   \n",
       "22         1999                     8022                      43081.0   \n",
       "37         2000                     5797                      44172.0   \n",
       "42         2001                     5231                      42460.0   \n",
       "39         2002                     5517                      40451.0   \n",
       "34         2003                     6278                      38787.0   \n",
       "33         2004                     6457                      37967.0   \n",
       "32         2005                     6979                      37415.0   \n",
       "29         2006                     7352                      36877.0   \n",
       "21         2007                     8081                      36135.0   \n",
       "19         2008                     8733                      34932.0   \n",
       "15         2009                     9512                      33713.0   \n",
       "10         2010                    10375                      33403.0   \n",
       "9          2011                    11174                      33374.0   \n",
       "7          2012                    12176                      35026.0   \n",
       "5          2013                    12394                      35065.0   \n",
       "8          2014                    12081                      34889.0   \n",
       "6          2015                    12332                      33956.0   \n",
       "4          2016                    13321                      32405.0   \n",
       "3          2017                    14011                      31404.0   \n",
       "1          2018                    14744                      30712.0   \n",
       "0          2019                    14915                      29729.0   \n",
       "2          2020                    14472                      27969.0   \n",
       "55         2021                      228                       1051.0   \n",
       "\n",
       "    % null rows  \n",
       "51          NaN  \n",
       "30    95.297556  \n",
       "14    96.315058  \n",
       "13    96.281071  \n",
       "11    94.055556  \n",
       "27    70.257148  \n",
       "26    68.811751  \n",
       "24    67.948387  \n",
       "28    66.216797  \n",
       "46    35.461053  \n",
       "49     5.859543  \n",
       "52     2.569503  \n",
       "53     2.496997  \n",
       "54     2.281304  \n",
       "50     3.675265  \n",
       "36    30.280927  \n",
       "43    19.144625  \n",
       "47    11.116934  \n",
       "48    10.187202  \n",
       "45    13.912546  \n",
       "40    15.857586  \n",
       "44    14.410903  \n",
       "41    15.382133  \n",
       "38    16.302774  \n",
       "35    18.084545  \n",
       "23    21.355996  \n",
       "17    22.968520  \n",
       "16    22.019619  \n",
       "25    17.537031  \n",
       "12    20.693039  \n",
       "18    18.065653  \n",
       "31    14.640622  \n",
       "20    16.517458  \n",
       "22    15.697709  \n",
       "37    11.601193  \n",
       "42    10.968527  \n",
       "39    12.001827  \n",
       "34    13.930989  \n",
       "33    14.534936  \n",
       "32    15.720593  \n",
       "29    16.622578  \n",
       "21    18.276190  \n",
       "19    20.000000  \n",
       "15    22.005784  \n",
       "10    23.699118  \n",
       "9     25.083056  \n",
       "7     25.795517  \n",
       "5     26.115173  \n",
       "8     25.720673  \n",
       "6     26.641894  \n",
       "4     29.132222  \n",
       "3     30.851040  \n",
       "1     32.435762  \n",
       "0     33.408745  \n",
       "2     34.099102  \n",
       "55    17.826427  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I plan to create a lot of financial ratios based on total asset\n",
    "# Let us see % nulls in total asset column across years\n",
    "num_nulls_asset_by_year = pd.DataFrame(quarterly_comp_data[quarterly_comp_data['asset_total'].isnull()]['fiscal_year'].value_counts()).reset_index()\n",
    "num_nulls_asset_by_year.columns = ['fiscal_year', 'num_nulls_asset_by_year']\n",
    "\n",
    "num_non_nulls_asset_by_year = pd.DataFrame(quarterly_comp_data[~quarterly_comp_data['asset_total'].isnull()]['fiscal_year'].value_counts()).reset_index()\n",
    "num_non_nulls_asset_by_year.columns = ['fiscal_year', 'num_non_nulls_asset_by_year']\n",
    "\n",
    "nulls_distribution_asset_by_year = pd.merge(num_nulls_asset_by_year,\n",
    "                                            num_non_nulls_asset_by_year,\n",
    "                                            left_on = 'fiscal_year',\n",
    "                                            right_on = 'fiscal_year',\n",
    "                                            how = 'outer')\n",
    "\n",
    "nulls_distribution_asset_by_year['% null rows'] = nulls_distribution_asset_by_year['num_nulls_asset_by_year']/(nulls_distribution_asset_by_year['num_nulls_asset_by_year'] + nulls_distribution_asset_by_year['num_non_nulls_asset_by_year'])\n",
    "nulls_distribution_asset_by_year['% null rows'] = nulls_distribution_asset_by_year['% null rows']*100\n",
    "nulls_distribution_asset_by_year.sort_values(by = ['fiscal_year'], ascending = True)\n",
    "# Discovered that even in most recent years, I have lot of companies that declare balance sheet only once a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_current_asset        645144\n",
       "share_holder_equity        381027\n",
       "cash_st_investment         431081\n",
       "total_LT_debt              397026\n",
       "asset_total                412781\n",
       "intangible_asset_total    1115929\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting only the most imp balance sheet columns\n",
    "quarterly_comp_data[['total_current_asset','share_holder_equity','cash_st_investment','total_LT_debt','asset_total',\n",
    "                     'intangible_asset_total']].isnull().sum()\n",
    "# Total current asset has significantly more nulls than asset total because financial companies don't really have current assets\n",
    "# Intangible asset column has lot of nulls but that is expected (not all companies have intangible assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intangible_asset_total\n",
      "182816\n",
      "365631\n",
      "548446\n",
      "731262\n",
      "914078\n",
      "1096893\n",
      "1279708\n",
      "1462524\n",
      "1645340\n",
      "asset_total\n",
      "182816\n",
      "365631\n",
      "548446\n",
      "731262\n",
      "914078\n",
      "1096893\n",
      "1279708\n",
      "1462524\n",
      "1645340\n",
      "total_current_asset\n",
      "182816\n",
      "365631\n",
      "548446\n",
      "731262\n",
      "914078\n",
      "1096893\n",
      "1279708\n",
      "1462524\n",
      "1645340\n",
      "share_holder_equity\n",
      "182816\n",
      "365631\n",
      "548446\n",
      "731262\n",
      "914078\n",
      "1096893\n",
      "1279708\n",
      "1462524\n",
      "1645340\n",
      "cash_st_investment\n",
      "182816\n",
      "365631\n",
      "548446\n",
      "731262\n",
      "914078\n",
      "1096893\n",
      "1279708\n",
      "1462524\n",
      "1645340\n",
      "total_LT_debt\n",
      "182816\n",
      "365631\n",
      "548446\n",
      "731262\n",
      "914078\n",
      "1096893\n",
      "1279708\n",
      "1462524\n",
      "1645340\n"
     ]
    }
   ],
   "source": [
    "# If I want to use total asset colum to create financial ratios, I have 2 problems\n",
    "# 1) Just realised for SA companies compustat created March, Sep rows but did not fill in financial info\n",
    "# 2) Lots of companies declare balance sheet once a year\n",
    "\n",
    "# So, I am going to write a for loop to forward fill certain imp balance sheet columns\n",
    "cols_to_ffill = ['intangible_asset_total', 'asset_total', 'total_current_asset','share_holder_equity','cash_st_investment',\n",
    "                 'total_LT_debt']\n",
    "\n",
    "# for j in cols_to_ffill:\n",
    "#    quarterly_comp_data[j] = quarterly_comp_data.groupby('gvkey')[j].transform(lambda v: v.ffill())\n",
    "\n",
    "# I could potentially just ffill every balance sheet information upto the next 4 quarters\n",
    "# But that's not really ideal. Because some SA companies have 2 quarters (remember Compustat didn't create Q1,Q2,Q3,Q4 for all SA companies)\n",
    "# Rather, I will write a for loop, fill only rows where gap btw. data in current row & closest non null date is 12 months\n",
    "\n",
    "quarterly_comp_data = quarterly_comp_data.reset_index()\n",
    "quarterly_comp_data = quarterly_comp_data.drop(['index'],axis = 1)\n",
    "quarterly_comp_data['quarter_end_date'] = pd.to_datetime(quarterly_comp_data['quarter_end_date'])\n",
    "\n",
    "quarterly_comp_data = quarterly_comp_data.sort_values(by = ['gvkey', 'fiscal_year', 'fiscal_quarter'], ascending = True)\n",
    "\n",
    "tracking_rows = []\n",
    "for i in range(1,10):\n",
    "    tracking_rows = tracking_rows + [round(i*len(quarterly_comp_data)/10)] \n",
    "    # To track progress of for loop, creating the list of rows where I want code to display the row number\n",
    "\n",
    "for j in cols_to_ffill:\n",
    "#    print(\"printing j\")\n",
    "    print(j)\n",
    "\n",
    "    quarterly_comp_data['closest_non_null_date'] = quarterly_comp_data['quarter_end_date']\n",
    "    quarterly_comp_data['j_null_ind'] = np.where(quarterly_comp_data[j].isnull(), 1, 0)\n",
    "    \n",
    "\n",
    "    for i in quarterly_comp_data.index:\n",
    "#        print(\"printing i\")\n",
    "#        print(i)\n",
    "        for k in tracking_rows:\n",
    "            if i == k:\n",
    "                print(i) # If ith row happens to be one of the row numbers I wanna track, asking to display (to track loop progress)\n",
    "        if quarterly_comp_data['j_null_ind'][i] == 1:\n",
    "            gvkey_iter = quarterly_comp_data['gvkey'][i]\n",
    "#            print(\"printing gvkey_iter\")\n",
    "#            print(gvkey_iter)\n",
    "            date_iter = quarterly_comp_data['quarter_end_date'][i]\n",
    "            fiscal_year_iter = quarterly_comp_data['fiscal_year'][i]\n",
    "#            print(\"printing fiscal_year_iter\")\n",
    "#            print(fiscal_year_iter)\n",
    "#            print(\"printing fiscal quarter\")\n",
    "#            print(quarterly_comp_data['fiscal_quarter'][i])\n",
    "            possible_matches = quarterly_comp_data[((quarterly_comp_data['gvkey'] == quarterly_comp_data['gvkey'][i]) &\n",
    "                                                    (quarterly_comp_data['quarter_end_date'] <= quarterly_comp_data['quarter_end_date'][i]) &\n",
    "                                                    (quarterly_comp_data['fiscal_year'] >= quarterly_comp_data['fiscal_year'][i] - 1)\n",
    "                                                    )][['gvkey','quarter_end_date',j]]\n",
    "            possible_matches['date_for_comparison'] = date_iter\n",
    "            possible_matches = possible_matches.dropna(subset = [j])\n",
    "            possible_matches['gap_btw_curr_row_earliest_non_null'] = possible_matches['date_for_comparison'] - possible_matches['quarter_end_date']\n",
    "                \n",
    "        \n",
    "            possible_matches = possible_matches.sort_values(by = ['gap_btw_curr_row_earliest_non_null'],  ascending = True)\n",
    "#            print(possible_matches)\n",
    "#            print(\"printing possible_matches shape\")\n",
    "#            print(possible_matches.shape)\n",
    "        \n",
    "            if len(possible_matches) > 0:\n",
    "#                print(\"printing possible_matches quarter_end_date 0th element\")\n",
    "#                print(possible_matches['quarter_end_date'].iloc[0])\n",
    "                quarterly_comp_data['closest_non_null_date'][i] = possible_matches['quarter_end_date'].iloc[0]\n",
    "                quarterly_comp_data[j][i] = possible_matches[j].iloc[0]\n",
    "\n",
    "    quarterly_comp_data['diff_quarter_end_date_non_null'] = quarterly_comp_data['quarter_end_date'] - quarterly_comp_data['closest_non_null_date']\n",
    "    quarterly_comp_data['diff_quarter_end_date_non_null'] = quarterly_comp_data['diff_quarter_end_date_non_null'].dt.days\n",
    "    quarterly_comp_data[j] = np.where((quarterly_comp_data['diff_quarter_end_date_non_null'] > 370), None, \n",
    "                                      quarterly_comp_data[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writting out the correct ffill balance sheet columns\n",
    "quarterly_comp_data[['gvkey','fiscal_year','fiscal_quarter','intangible_asset_total', 'asset_total', 'total_current_asset',\n",
    "                     'share_holder_equity','cash_st_investment','total_LT_debt']].to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\qtr_data_bs_columns_ffill.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end of above loop some of the numeric columns got convereted as objects. Let's make them float\n",
    "for i in cols_to_ffill:\n",
    "    quarterly_comp_data[i] = quarterly_comp_data[i].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_current_asset        516340\n",
       "share_holder_equity        284706\n",
       "cash_st_investment         315821\n",
       "total_LT_debt              289006\n",
       "asset_total                307162\n",
       "intangible_asset_total    1062893\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many nulls are still left in the important columns\n",
    "quarterly_comp_data[['total_current_asset','share_holder_equity','cash_st_investment','total_LT_debt','asset_total',\n",
    "                     'intangible_asset_total']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating financial ratios from earlier selected columns\n",
    "\n",
    "quarterly_comp_data['percentage_current_asset'] = (quarterly_comp_data['total_current_asset']/quarterly_comp_data['asset_total'])*100\n",
    "quarterly_comp_data['percentage_current_asset'][np.isinf(quarterly_comp_data['percentage_current_asset'])] = None\n",
    "quarterly_comp_data['percentage_equity'] = (quarterly_comp_data['share_holder_equity']/quarterly_comp_data['asset_total'])*100\n",
    "quarterly_comp_data['percentage_equity'][np.isinf(quarterly_comp_data['percentage_equity'])] = None\n",
    "quarterly_comp_data['percentage_cash_st'] = (quarterly_comp_data['cash_st_investment']/quarterly_comp_data['asset_total'])*100\n",
    "quarterly_comp_data['percentage_cash_st'][np.isinf(quarterly_comp_data['percentage_cash_st'])] = None\n",
    "quarterly_comp_data['percentage_LT_debt'] = (quarterly_comp_data['total_LT_debt']/quarterly_comp_data['share_holder_equity'])*100\n",
    "quarterly_comp_data['percentage_LT_debt'][np.isinf(quarterly_comp_data['percentage_LT_debt'])] = None\n",
    "\n",
    "\n",
    "# I noticed almost 70% intangible total asset column is null (because intangible asset doesn't apply for all companies)\n",
    "# So, for rows where asset total exists but intangible asset is null, I just filled 0\n",
    "quarterly_comp_data['intangible_asset_fill0'] = np.where((~quarterly_comp_data['asset_total'].isnull()) & \n",
    "                                                         (quarterly_comp_data['intangible_asset_total'].isnull()), \n",
    "                                                         0,quarterly_comp_data['intangible_asset_total'])\n",
    "\n",
    "quarterly_comp_data['percentage_current_asset_intan'] = (quarterly_comp_data['total_current_asset']/(quarterly_comp_data['asset_total']-quarterly_comp_data['intangible_asset_fill0']))*100\n",
    "quarterly_comp_data['percentage_current_asset_intan'][np.isinf(quarterly_comp_data['percentage_current_asset_intan'])] = None\n",
    "quarterly_comp_data['percentage_equity_intan'] = ((quarterly_comp_data['share_holder_equity']-quarterly_comp_data['intangible_asset_fill0'])/(quarterly_comp_data['asset_total'] - quarterly_comp_data['intangible_asset_fill0']))*100\n",
    "quarterly_comp_data['percentage_equity_intan'][np.isinf(quarterly_comp_data['percentage_equity_intan'])] = None\n",
    "quarterly_comp_data['percentage_cash_st_intan'] = (quarterly_comp_data['cash_st_investment']/(quarterly_comp_data['asset_total'] - quarterly_comp_data['intangible_asset_fill0']))*100\n",
    "quarterly_comp_data['percentage_cash_st_intan'][np.isinf(quarterly_comp_data['percentage_cash_st_intan'])] = None\n",
    "quarterly_comp_data['percentage_LT_debt_intan'] = (quarterly_comp_data['total_LT_debt']/(quarterly_comp_data['share_holder_equity'] - quarterly_comp_data['intangible_asset_fill0']))*100\n",
    "quarterly_comp_data['percentage_LT_debt_intan'][np.isinf(quarterly_comp_data['percentage_LT_debt_intan'])] = None\n",
    "\n",
    "quarterly_comp_data['tangible_equity'] = quarterly_comp_data['share_holder_equity'] - quarterly_comp_data['intangible_asset_fill0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1828155, 96)\n",
      "(1828155, 104)\n"
     ]
    }
   ],
   "source": [
    "# Creating some variables that capture the shifts in balance sheets\n",
    "# For example, if 2 companies have pct. current asset as 10%, coming from 50 to 10 & moving from 5 to 10 has different impact\n",
    "# So, I also want to capture the movement (difference) in balance sheet ratios\n",
    "\n",
    "quarterly_comp_data['fiscal_year_prev_1year'] = quarterly_comp_data['fiscal_year'] + 1\n",
    "quarterly_comp_data_bs_ratios = quarterly_comp_data[['gvkey','fiscal_year_prev_1year','fiscal_quarter','percentage_current_asset',\n",
    "                                                     'percentage_equity','percentage_cash_st','percentage_LT_debt',\n",
    "                                                     'percentage_current_asset_intan','percentage_equity_intan',\n",
    "                                                     'percentage_cash_st_intan','percentage_LT_debt_intan']]\n",
    "\n",
    "quarterly_comp_data_bs_ratios.rename(columns = {'percentage_current_asset' : 'percentage_current_asset_prev_1Y',\n",
    "                                                'percentage_equity': 'percentage_equity_prev_1Y',\n",
    "                                                'percentage_current_asset_intan':'percentage_current_asset_intan_prev_1Y',\n",
    "                                                'percentage_equity_intan': 'percentage_equity_intan_prev_1Y',\n",
    "                                                'percentage_cash_st_intan': 'percentage_cash_st_intan_prev_1Y',\n",
    "                                                'percentage_LT_debt_intan': 'percentage_LT_debt_intan_prev_1Y',\n",
    "                                                'percentage_cash_st': 'percentage_cash_st_prev_1Y',\n",
    "                                                'percentage_LT_debt': 'percentage_LT_debt_prev_1Y',\n",
    "                                                'fiscal_year_prev_1year': 'fiscal_year'},inplace = True)\n",
    "\n",
    "# Merging last year ratios\n",
    "print(quarterly_comp_data.shape)\n",
    "quarterly_comp_data = pd.merge(quarterly_comp_data,\n",
    "                              quarterly_comp_data_bs_ratios,\n",
    "                              left_on = ['gvkey','fiscal_year','fiscal_quarter'],\n",
    "                              right_on = ['gvkey','fiscal_year','fiscal_quarter'],\n",
    "                              how = 'left')\n",
    "print(quarterly_comp_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating differences between current and last year balance sheet ratios\n",
    "\n",
    "quarterly_comp_data['percentage_current_asset_1Y_change'] = quarterly_comp_data['percentage_current_asset'] - quarterly_comp_data['percentage_current_asset_prev_1Y']\n",
    "quarterly_comp_data['percentage_equity_1Y_change'] = quarterly_comp_data['percentage_equity'] - quarterly_comp_data['percentage_equity_prev_1Y']\n",
    "quarterly_comp_data['percentage_cash_st_1Y_change'] = quarterly_comp_data['percentage_cash_st'] - quarterly_comp_data['percentage_cash_st_prev_1Y']\n",
    "quarterly_comp_data['percentage_LT_debt_1Y_change'] = quarterly_comp_data['percentage_LT_debt'] - quarterly_comp_data['percentage_LT_debt_prev_1Y']\n",
    "\n",
    "quarterly_comp_data['percentage_current_asset_intan_1Y_change'] = quarterly_comp_data['percentage_current_asset_intan'] - quarterly_comp_data['percentage_current_asset_intan_prev_1Y']\n",
    "quarterly_comp_data['percentage_equity_intan_1Y_change'] = quarterly_comp_data['percentage_equity_intan'] - quarterly_comp_data['percentage_equity_intan_prev_1Y']\n",
    "quarterly_comp_data['percentage_cash_st_intan_1Y_change'] = quarterly_comp_data['percentage_cash_st_intan'] - quarterly_comp_data['percentage_cash_st_intan_prev_1Y']\n",
    "quarterly_comp_data['percentage_LT_debt_intan_1Y_change'] = quarterly_comp_data['percentage_LT_debt_intan'] - quarterly_comp_data['percentage_LT_debt_intan_prev_1Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting variables for currency exchange rate\n",
    "cols_tobe_adjusted_for_currency_exchange_rate = ['tangible_equity','share_holder_equity','intangible_asset_total','asset_total',\n",
    "                                                 'intangible_asset_fill0','revenue','eps_d_12M','eps_d_core_excl_extr','eps_d',\n",
    "                                                 'eps_d_core_excl_extr_12M','eps_12M','eps_core_excl_extr_12M','eps',\n",
    "                                                 'eps_core_excl_extr']\n",
    "\n",
    "for i in cols_tobe_adjusted_for_currency_exchange_rate:\n",
    "    quarterly_comp_data[i] = quarterly_comp_data[i] * quarterly_comp_data['currency_exchange_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>num_nulls_asset_by_year</th>\n",
       "      <th>num_non_nulls_asset_by_year</th>\n",
       "      <th>% null rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1966</td>\n",
       "      <td>315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1967</td>\n",
       "      <td>7244</td>\n",
       "      <td>284.0</td>\n",
       "      <td>96.227418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1968</td>\n",
       "      <td>9744</td>\n",
       "      <td>324.0</td>\n",
       "      <td>96.781883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1969</td>\n",
       "      <td>10113</td>\n",
       "      <td>347.0</td>\n",
       "      <td>96.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1970</td>\n",
       "      <td>10129</td>\n",
       "      <td>671.0</td>\n",
       "      <td>93.787037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1971</td>\n",
       "      <td>7598</td>\n",
       "      <td>3524.0</td>\n",
       "      <td>68.315051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1972</td>\n",
       "      <td>994</td>\n",
       "      <td>10443.0</td>\n",
       "      <td>8.691090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1973</td>\n",
       "      <td>736</td>\n",
       "      <td>10889.0</td>\n",
       "      <td>6.331183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1974</td>\n",
       "      <td>454</td>\n",
       "      <td>11179.0</td>\n",
       "      <td>3.902691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1975</td>\n",
       "      <td>290</td>\n",
       "      <td>11444.0</td>\n",
       "      <td>2.471450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1976</td>\n",
       "      <td>119</td>\n",
       "      <td>11486.0</td>\n",
       "      <td>1.025420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1977</td>\n",
       "      <td>105</td>\n",
       "      <td>11765.0</td>\n",
       "      <td>0.884583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1978</td>\n",
       "      <td>112</td>\n",
       "      <td>11542.0</td>\n",
       "      <td>0.961043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1979</td>\n",
       "      <td>198</td>\n",
       "      <td>11462.0</td>\n",
       "      <td>1.698113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1980</td>\n",
       "      <td>322</td>\n",
       "      <td>11269.0</td>\n",
       "      <td>2.778017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1981</td>\n",
       "      <td>8426</td>\n",
       "      <td>11864.0</td>\n",
       "      <td>41.527846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1982</td>\n",
       "      <td>6249</td>\n",
       "      <td>20897.0</td>\n",
       "      <td>23.019966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1983</td>\n",
       "      <td>3105</td>\n",
       "      <td>25518.0</td>\n",
       "      <td>10.847920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1984</td>\n",
       "      <td>2784</td>\n",
       "      <td>26596.0</td>\n",
       "      <td>9.475834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1985</td>\n",
       "      <td>4296</td>\n",
       "      <td>27172.0</td>\n",
       "      <td>13.651964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1986</td>\n",
       "      <td>5191</td>\n",
       "      <td>28345.0</td>\n",
       "      <td>15.478888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1987</td>\n",
       "      <td>4695</td>\n",
       "      <td>29425.0</td>\n",
       "      <td>13.760258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1988</td>\n",
       "      <td>4928</td>\n",
       "      <td>29157.0</td>\n",
       "      <td>14.457973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1989</td>\n",
       "      <td>5258</td>\n",
       "      <td>28589.0</td>\n",
       "      <td>15.534612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1990</td>\n",
       "      <td>5935</td>\n",
       "      <td>28177.0</td>\n",
       "      <td>17.398569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1991</td>\n",
       "      <td>7637</td>\n",
       "      <td>29369.0</td>\n",
       "      <td>20.637194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1992</td>\n",
       "      <td>8481</td>\n",
       "      <td>30973.0</td>\n",
       "      <td>21.495919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1993</td>\n",
       "      <td>8739</td>\n",
       "      <td>33873.0</td>\n",
       "      <td>20.508308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1994</td>\n",
       "      <td>7088</td>\n",
       "      <td>37874.0</td>\n",
       "      <td>15.764423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1995</td>\n",
       "      <td>9628</td>\n",
       "      <td>39287.0</td>\n",
       "      <td>19.683124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1996</td>\n",
       "      <td>8391</td>\n",
       "      <td>40990.0</td>\n",
       "      <td>16.992365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1997</td>\n",
       "      <td>6491</td>\n",
       "      <td>42093.0</td>\n",
       "      <td>13.360366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1998</td>\n",
       "      <td>8021</td>\n",
       "      <td>42356.0</td>\n",
       "      <td>15.921949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1999</td>\n",
       "      <td>7233</td>\n",
       "      <td>43870.0</td>\n",
       "      <td>14.153768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2000</td>\n",
       "      <td>5233</td>\n",
       "      <td>44736.0</td>\n",
       "      <td>10.472493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2001</td>\n",
       "      <td>4795</td>\n",
       "      <td>42896.0</td>\n",
       "      <td>10.054308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2002</td>\n",
       "      <td>5321</td>\n",
       "      <td>40647.0</td>\n",
       "      <td>11.575444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2003</td>\n",
       "      <td>6129</td>\n",
       "      <td>38936.0</td>\n",
       "      <td>13.600355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2004</td>\n",
       "      <td>6255</td>\n",
       "      <td>38169.0</td>\n",
       "      <td>14.080227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2005</td>\n",
       "      <td>6795</td>\n",
       "      <td>37599.0</td>\n",
       "      <td>15.306122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2006</td>\n",
       "      <td>6941</td>\n",
       "      <td>37288.0</td>\n",
       "      <td>15.693323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2007</td>\n",
       "      <td>7828</td>\n",
       "      <td>36388.0</td>\n",
       "      <td>17.703999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2008</td>\n",
       "      <td>8608</td>\n",
       "      <td>35057.0</td>\n",
       "      <td>19.713730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2009</td>\n",
       "      <td>9210</td>\n",
       "      <td>34015.0</td>\n",
       "      <td>21.307114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>10145</td>\n",
       "      <td>33633.0</td>\n",
       "      <td>23.173740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2011</td>\n",
       "      <td>11059</td>\n",
       "      <td>33489.0</td>\n",
       "      <td>24.824908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012</td>\n",
       "      <td>12179</td>\n",
       "      <td>35023.0</td>\n",
       "      <td>25.801873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013</td>\n",
       "      <td>12235</td>\n",
       "      <td>35224.0</td>\n",
       "      <td>25.780147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014</td>\n",
       "      <td>11795</td>\n",
       "      <td>35175.0</td>\n",
       "      <td>25.111773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015</td>\n",
       "      <td>12126</td>\n",
       "      <td>34162.0</td>\n",
       "      <td>26.196854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>13152</td>\n",
       "      <td>32574.0</td>\n",
       "      <td>28.762630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>13735</td>\n",
       "      <td>31680.0</td>\n",
       "      <td>30.243312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>15299</td>\n",
       "      <td>30157.0</td>\n",
       "      <td>33.656723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>14938</td>\n",
       "      <td>29706.0</td>\n",
       "      <td>33.460263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>14233</td>\n",
       "      <td>28208.0</td>\n",
       "      <td>33.535968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2021</td>\n",
       "      <td>212</td>\n",
       "      <td>1067.0</td>\n",
       "      <td>16.575450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fiscal_year  num_nulls_asset_by_year  num_non_nulls_asset_by_year  \\\n",
       "49         1966                      315                          NaN   \n",
       "25         1967                     7244                        284.0   \n",
       "13         1968                     9744                        324.0   \n",
       "12         1969                    10113                        347.0   \n",
       "11         1970                    10129                        671.0   \n",
       "24         1971                     7598                       3524.0   \n",
       "45         1972                      994                      10443.0   \n",
       "46         1973                      736                      10889.0   \n",
       "47         1974                      454                      11179.0   \n",
       "50         1975                      290                      11444.0   \n",
       "53         1976                      119                      11486.0   \n",
       "55         1977                      105                      11765.0   \n",
       "54         1978                      112                      11542.0   \n",
       "52         1979                      198                      11462.0   \n",
       "48         1980                      322                      11269.0   \n",
       "19         1981                     8426                      11864.0   \n",
       "32         1982                     6249                      20897.0   \n",
       "43         1983                     3105                      25518.0   \n",
       "44         1984                     2784                      26596.0   \n",
       "42         1985                     4296                      27172.0   \n",
       "38         1986                     5191                      28345.0   \n",
       "41         1987                     4695                      29425.0   \n",
       "39         1988                     4928                      29157.0   \n",
       "36         1989                     5258                      28589.0   \n",
       "34         1990                     5935                      28177.0   \n",
       "23         1991                     7637                      29369.0   \n",
       "18         1992                     8481                      30973.0   \n",
       "16         1993                     8739                      33873.0   \n",
       "27         1994                     7088                      37874.0   \n",
       "14         1995                     9628                      39287.0   \n",
       "20         1996                     8391                      40990.0   \n",
       "30         1997                     6491                      42093.0   \n",
       "21         1998                     8021                      42356.0   \n",
       "26         1999                     7233                      43870.0   \n",
       "37         2000                     5233                      44736.0   \n",
       "40         2001                     4795                      42896.0   \n",
       "35         2002                     5321                      40647.0   \n",
       "33         2003                     6129                      38936.0   \n",
       "31         2004                     6255                      38169.0   \n",
       "29         2005                     6795                      37599.0   \n",
       "28         2006                     6941                      37288.0   \n",
       "22         2007                     7828                      36388.0   \n",
       "17         2008                     8608                      35057.0   \n",
       "15         2009                     9210                      34015.0   \n",
       "10         2010                    10145                      33633.0   \n",
       "9          2011                    11059                      33489.0   \n",
       "6          2012                    12179                      35023.0   \n",
       "5          2013                    12235                      35224.0   \n",
       "8          2014                    11795                      35175.0   \n",
       "7          2015                    12126                      34162.0   \n",
       "4          2016                    13152                      32574.0   \n",
       "3          2017                    13735                      31680.0   \n",
       "0          2018                    15299                      30157.0   \n",
       "1          2019                    14938                      29706.0   \n",
       "2          2020                    14233                      28208.0   \n",
       "51         2021                      212                       1067.0   \n",
       "\n",
       "    % null rows  \n",
       "49          NaN  \n",
       "25    96.227418  \n",
       "13    96.781883  \n",
       "12    96.682600  \n",
       "11    93.787037  \n",
       "24    68.315051  \n",
       "45     8.691090  \n",
       "46     6.331183  \n",
       "47     3.902691  \n",
       "50     2.471450  \n",
       "53     1.025420  \n",
       "55     0.884583  \n",
       "54     0.961043  \n",
       "52     1.698113  \n",
       "48     2.778017  \n",
       "19    41.527846  \n",
       "32    23.019966  \n",
       "43    10.847920  \n",
       "44     9.475834  \n",
       "42    13.651964  \n",
       "38    15.478888  \n",
       "41    13.760258  \n",
       "39    14.457973  \n",
       "36    15.534612  \n",
       "34    17.398569  \n",
       "23    20.637194  \n",
       "18    21.495919  \n",
       "16    20.508308  \n",
       "27    15.764423  \n",
       "14    19.683124  \n",
       "20    16.992365  \n",
       "30    13.360366  \n",
       "21    15.921949  \n",
       "26    14.153768  \n",
       "37    10.472493  \n",
       "40    10.054308  \n",
       "35    11.575444  \n",
       "33    13.600355  \n",
       "31    14.080227  \n",
       "29    15.306122  \n",
       "28    15.693323  \n",
       "22    17.703999  \n",
       "17    19.713730  \n",
       "15    21.307114  \n",
       "10    23.173740  \n",
       "9     24.824908  \n",
       "6     25.801873  \n",
       "5     25.780147  \n",
       "8     25.111773  \n",
       "7     26.196854  \n",
       "4     28.762630  \n",
       "3     30.243312  \n",
       "0     33.656723  \n",
       "1     33.460263  \n",
       "2     33.535968  \n",
       "51    16.575450  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nulls_asset_by_year = pd.DataFrame(quarterly_comp_data[quarterly_comp_data['asset_total'].isnull()]['fiscal_year'].value_counts()).reset_index()\n",
    "num_nulls_asset_by_year.columns = ['fiscal_year', 'num_nulls_asset_by_year']\n",
    "\n",
    "num_non_nulls_asset_by_year = pd.DataFrame(quarterly_comp_data[~quarterly_comp_data['asset_total'].isnull()]['fiscal_year'].value_counts()).reset_index()\n",
    "num_non_nulls_asset_by_year.columns = ['fiscal_year', 'num_non_nulls_asset_by_year']\n",
    "\n",
    "nulls_distribution_asset_by_year = pd.merge(num_nulls_asset_by_year,\n",
    "                                            num_non_nulls_asset_by_year,\n",
    "                                            left_on = 'fiscal_year',\n",
    "                                            right_on = 'fiscal_year',\n",
    "                                            how = 'outer')\n",
    "\n",
    "nulls_distribution_asset_by_year['% null rows'] = nulls_distribution_asset_by_year['num_nulls_asset_by_year']/(nulls_distribution_asset_by_year['num_nulls_asset_by_year'] + nulls_distribution_asset_by_year['num_non_nulls_asset_by_year'])\n",
    "nulls_distribution_asset_by_year['% null rows'] = nulls_distribution_asset_by_year['% null rows']*100\n",
    "nulls_distribution_asset_by_year.sort_values(by = ['fiscal_year'], ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating quarterly compustat data final grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Grouping columns\n",
    "\n",
    "id_group_col = ['cusip','gvkey','ticker','company_name']\n",
    "\n",
    "date_group_col = ['quarter_end_date', 'fiscal_quarter','fiscal_year','reporting_frequency','result_reported_date']\n",
    "\n",
    "general_group_col = ['split_adjusting_factor','S&P_grade','sic_code', 'NAICS', 'Mcap']\n",
    "\n",
    "balance_sheet_group = ['percentage_current_asset','percentage_current_asset_1Y_change','percentage_equity_1Y_change',\n",
    "                       'percentage_current_asset_intan_1Y_change','percentage_equity_intan_1Y_change',\n",
    "                       'percentage_cash_st_intan_1Y_change','percentage_LT_debt_intan_1Y_change','percentage_cash_st_1Y_change',\n",
    "                       'percentage_LT_debt_1Y_change','percentage_equity','percentage_cash_st','percentage_LT_debt',\n",
    "                       'percentage_current_asset_intan','percentage_equity_intan','percentage_cash_st_intan',\n",
    "                       'percentage_LT_debt_intan','tangible_equity','share_holder_equity', 'intangible_asset_total',\n",
    "                       'intangible_asset_fill0', 'asset_total']\n",
    "# So calculate Revenue 5Y CAGR columns further below, realized I need to add intangible_asset_fill0 & asset_total to the final group\n",
    "\n",
    "eps_group = ['eps_d_12M','eps_d_core_excl_extr','eps_d','eps_d_core_excl_extr_12M','eps_12M','eps_core_excl_extr_12M','eps',\n",
    "             'eps_core_excl_extr']\n",
    "\n",
    "num_shares_group = ['num_shares_eps_12','num_d_shares_eps_12','num_d_shares_eps','num_shares_eps','num_shares_eps_split_adj',\n",
    "                   'num_shares_eps_12_split_adj']\n",
    "\n",
    "additional_income_group = ['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_comp_data = quarterly_comp_data[id_group_col + date_group_col + general_group_col + balance_sheet_group + eps_group + num_shares_group + additional_income_group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting additional fundemental information from yearly compustat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_comp_data = pd.read_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\yearly comp data.csv')\n",
    "yearly_comp_data = yearly_comp_data[yearly_comp_data['indfmt'] == 'INDL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at null values in each column\n",
    "year_null_df = pd.DataFrame(yearly_comp_data.isnull().sum()).reset_index()\n",
    "year_null_df.rename(columns = {'index': 'Column_name',0:'Num_of_nulls'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_comp_data = yearly_comp_data.dropna(subset = ['gvkey', 'fyear'])\n",
    "yearly_comp_data['gvkey_fy_str'] = yearly_comp_data['gvkey'].astype(str) + yearly_comp_data['fyear'].astype(str)\n",
    "yearly_comp_data['gvkey_fy_str'].value_counts()[0:5]\n",
    "# Realised that there are very few rows which are not unique at gvkey and fiscal year level\n",
    "# Let's just drop duplicates\n",
    "yearly_comp_data = yearly_comp_data.drop_duplicates(subset=['gvkey','fyear'], keep=\"last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping and renaming columns in yearly compustat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "# dropping indfmt because I already used this column to subset the data in the beginning \n",
    "# dropping consol because level of consolidation data is not required\n",
    "# dropping popsrc because the source of the data is not required\n",
    "# dropping tic because I already have gvkey\n",
    "# dropping curcd because I already have currtr(currency translator)\n",
    "# dropping curuscn because 90% of the data is null and I already have currtr\n",
    "# dropping costat because I dont need active or inactive column\n",
    "\n",
    "yearly_comp_data = yearly_comp_data.drop(['indfmt','consol','popsrc','tic','curcd','curuscn','costat'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "yearly_comp_data.rename(columns = {'datadate': 'financial_year_end_date','fyear':'fiscal_year','fdate': 'final_date',\n",
    "                                   'currtr':'currency_exchange_rate',\n",
    "                                   'acqao':'aquired_assets_other_long_term', \n",
    "                                   # Started collection in 2011. Don't need because I have acqintan\n",
    "                                   'acqgdwl':'aquired_assets_goodwill',\n",
    "                                   # Started collection in 2011. Don't need because I have acqintan\n",
    "                                   'acqic':'acquisition_current_income_contribution',\n",
    "                                   # Better to use aqi than acqic\n",
    "                                   'acqintan':'aquired_assets_intangible',\n",
    "                                   # Started collecting from 2011.let's see I can use this\n",
    "                                   # There are few companies where I have acqintan info but not aqs\n",
    "                                   # But there are also many companies which don't provide acqintan info after acquisition\n",
    "                                   # I think it's better if I just use the intangible column from quarterly data to detect-\n",
    "                                   # -acquisitions\n",
    "                                   'acqppe':'aquired_assets_ppe',\n",
    "                                   # Started collecting from 2011, probably won't be using this\n",
    "                                   'acqsc':'acquisition_current_sale_contribution',\n",
    "                                   # Better to use aqs than acqsc\n",
    "                                   'aqa':'acquisition_or_merger_after_tax',\n",
    "                                   # This variable is cost(profit) associated with failed acquisition\n",
    "                                   # Hence very low fill rate. let's just ignore this variable\n",
    "                                   'aqc':'acquisition',\n",
    "                                   # This acquisition variable comes from cash flow statement\n",
    "                                   # I thought of using this variable but realised that this variable is accurate only -\n",
    "                                   # -if the acquisition is a pure cash deal\n",
    "                                   # If salesforce buys tablue in a stock deal I wouldn't know that\n",
    "                                   'aqd':'acquisition_or_merger_d_eps',\n",
    "                                   # Started from 2000. Don't need this column because I have aqi\n",
    "                                   'aqeps':'acquisition_or_merger_eps',\n",
    "                                   # Started from 2000. Don't need this column because I have aqi\n",
    "                                   'aqi':'acquisition_income_contribution',\n",
    "                                   # Contribution of acquisition income(loss)\n",
    "                                   'aqp':'acquisition_or_merger_pretax',\n",
    "                                   # Don't need this because I already have aqi\n",
    "                                   'aqs':'acquisition_sales_contribution',\n",
    "                                   # contributions of acquisition revenue\n",
    "                                   # aqs is not always available (even when aqc is available)\n",
    "                                   # But for non null rows, it is the best info on acquisitions\n",
    "                                   'intan':'intangible_asset_total',\n",
    "                                   'capx':'capital_expenditure',\n",
    "                                   'emp':'employees', \n",
    "                                   'at': 'asset_total', \n",
    "                                   'revt': 'revenue'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial grouping of columns in yearly compustat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id_group = ['gvkey','cusip']\n",
    "date_group = ['fiscal_year','financial_year_end_date','final_date']\n",
    "general_group = ['currency_exchange_rate','capital_expenditure','employees','intangible_asset_total','asset_total','revenue']\n",
    "acquisition_group = ['aquired_assets_other_long_term','aquired_assets_goodwill','acquisition_current_income_contribution',\n",
    "                    'aquired_assets_intangible','aquired_assets_ppe','acquisition_current_sale_contribution',\n",
    "                    'acquisition_or_merger_after_tax','acquisition','acquisition_or_merger_d_eps','acquisition_or_merger_eps',\n",
    "                    'acquisition_income_contribution','acquisition_or_merger_pretax','acquisition_sales_contribution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999.0    9567\n",
       "2000.0    9434\n",
       "2001.0    9052\n",
       "1996.0    8827\n",
       "1997.0    8809\n",
       "1998.0    8727\n",
       "2002.0    8576\n",
       "2003.0    8399\n",
       "1995.0    8388\n",
       "2004.0    8334\n",
       "2005.0    8329\n",
       "2006.0    8197\n",
       "1994.0    8097\n",
       "2007.0    7987\n",
       "1993.0    7865\n",
       "2008.0    7659\n",
       "2013.0    7548\n",
       "2009.0    7468\n",
       "2012.0    7438\n",
       "2010.0    7411\n",
       "2014.0    7371\n",
       "2011.0    7351\n",
       "1992.0    7295\n",
       "1987.0    7174\n",
       "2015.0    7078\n",
       "1988.0    7034\n",
       "1986.0    7030\n",
       "1991.0    6958\n",
       "1989.0    6890\n",
       "2016.0    6819\n",
       "1990.0    6794\n",
       "1985.0    6736\n",
       "2017.0    6695\n",
       "2018.0    6589\n",
       "1984.0    6468\n",
       "1983.0    6395\n",
       "2019.0    6335\n",
       "1982.0    6118\n",
       "1977.0    6010\n",
       "1978.0    5916\n",
       "1976.0    5899\n",
       "1981.0    5879\n",
       "1975.0    5833\n",
       "1980.0    5818\n",
       "1979.0    5796\n",
       "2020.0    5736\n",
       "1974.0    5735\n",
       "1973.0    4276\n",
       "1972.0    3821\n",
       "1971.0    3682\n",
       "1970.0    3033\n",
       "1969.0    2636\n",
       "1968.0    2418\n",
       "1967.0    2097\n",
       "1966.0    2061\n",
       "1965.0     226\n",
       "Name: fiscal_year, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realised that final date is filled in only from 2004\n",
    "yearly_comp_data[~yearly_comp_data['employees'].isnull()]['fiscal_year'].value_counts()\n",
    "# Need to use the Q4 result date in quarterly file as the same yearly result declaration date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating derived variables in yearly compustat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables required to create organic revenue CAGR columns\n",
    "\n",
    "# Even though the plan is to create the 5Y growth rate column in quarterly file, i need to prepare few columns in yearly data\n",
    "# For example, quarterly data doesn't have acquisition columns\n",
    "# It's easier to calculate acquisition variables when data is in yearly format rather than merging with quarterly\n",
    "# These columns are much easier to prepare in annual data than in quarterly data\n",
    "\n",
    "yearly_comp_data = yearly_comp_data.sort_values(by = ['gvkey', 'fiscal_year'])\n",
    "\n",
    "# Calculating sum of revenue contribution by acquisitions in the last 5 years (which can be subtracted from 5Y growth rate)\n",
    "yearly_comp_data['acquisition_sales_contribution_fill0'] = yearly_comp_data['acquisition_sales_contribution'].fillna(0)\n",
    "\n",
    "yearly_comp_data['acquisition_sales_contribution_shift1'] = yearly_comp_data['acquisition_sales_contribution_fill0'].shift(1)\n",
    "yearly_comp_data['acquisition_sales_contribution_shift2'] = yearly_comp_data['acquisition_sales_contribution_fill0'].shift(2)\n",
    "yearly_comp_data['acquisition_sales_contribution_shift3'] = yearly_comp_data['acquisition_sales_contribution_fill0'].shift(3)\n",
    "yearly_comp_data['acquisition_sales_contribution_shift4'] = yearly_comp_data['acquisition_sales_contribution_fill0'].shift(4)\n",
    "\n",
    "yearly_comp_data['gvkey_shift4'] = yearly_comp_data['gvkey'].shift(4)\n",
    "yearly_comp_data['fiscal_year_shift4'] = yearly_comp_data['fiscal_year'].shift(4)\n",
    "\n",
    "yearly_comp_data['acquisition_sales_5Y'] = yearly_comp_data['acquisition_sales_contribution'] + yearly_comp_data['acquisition_sales_contribution_shift1'] + yearly_comp_data['acquisition_sales_contribution_shift2']+yearly_comp_data['acquisition_sales_contribution_shift3']+yearly_comp_data['acquisition_sales_contribution_shift4']\n",
    "\n",
    "yearly_comp_data['acquisition_sales_5Y'] = np.where(((yearly_comp_data['gvkey'] == yearly_comp_data['gvkey_shift4']) &\n",
    "                                                    (yearly_comp_data['fiscal_year'] - yearly_comp_data['fiscal_year_shift4'] == 4)),\n",
    "                                                    yearly_comp_data['acquisition_sales_5Y'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already know that acquisition sales data is not always available\n",
    "# So, identifying years where intangible assets grew massively (Intangible assets often grow beacause of acquisition)\n",
    "# The idea is: once a year row has been identified as problematic, any 5Y CAGR that passes through this problematic row won't be considered\n",
    "# For example, if salesforce acquired both tableau & mulesoft, I ideally need to know revenue contribution of both companies\n",
    "# If I knew both numberss, I can simply subtract both acquisition sale contribution revenue 5Y later\n",
    "# But if mulesoft revenue number is not available, the year of mulesoft acquisition is problematic\n",
    "# Need to identify those rows which are problematic\n",
    "\n",
    "# First, intangible columns has to be imputed properly though\n",
    "yearly_comp_data['intangible_asset_fill0'] = np.where((~yearly_comp_data['asset_total'].isnull()) & \n",
    "                                                    (yearly_comp_data['intangible_asset_total'].isnull()), 0,\n",
    "                                                    yearly_comp_data['intangible_asset_total'])\n",
    "\n",
    "yearly_comp_data['intangible_asset_total_shift1'] = yearly_comp_data['intangible_asset_fill0'].shift(1)\n",
    "yearly_comp_data['gvkey_shift1'] = yearly_comp_data['gvkey'].shift(1)\n",
    "yearly_comp_data['fiscal_year_shift1'] = yearly_comp_data['fiscal_year'].shift(1)\n",
    "\n",
    "# intan_asset_1Y_growth looks at growth in intangible assets as a proportion of total assets\n",
    "yearly_comp_data['intan_asset_1Y_growth'] =((yearly_comp_data['intangible_asset_fill0'] - \n",
    "                                             yearly_comp_data['intangible_asset_total_shift1'])/yearly_comp_data['asset_total'])*100\n",
    "yearly_comp_data['intan_asset_1Y_growth'] = np.where(((yearly_comp_data['gvkey'] == yearly_comp_data['gvkey_shift1']) &\n",
    "                                                     (yearly_comp_data['fiscal_year'] - yearly_comp_data['fiscal_year_shift1'] == 1)),\n",
    "                                                     yearly_comp_data['intan_asset_1Y_growth'], None)\n",
    "\n",
    "yearly_comp_data['acq_rev_percent'] = yearly_comp_data['acquisition_sales_contribution'].fillna(0)*100/yearly_comp_data['revenue']\n",
    "\n",
    "# Tagging years where a big acquisition may have happened but acquisition sales contribution data doesn't indicate that\n",
    "yearly_comp_data['possible_acq_but_no_data'] = np.where(((yearly_comp_data['intan_asset_1Y_growth']>10) &\n",
    "                                                         # Rows where intan assets growth is more than 10% of assets (significant)\n",
    "                                                         (yearly_comp_data['acq_rev_percent'] < 2))\n",
    "                                                         # Ensuring that acquisition amount has not been specified clealy\n",
    "                                                         # If acq_revenue/total revenue >5%, acq contribution is probably filled\n",
    "                                                         # If acq contribution filled, there's no problem at all\n",
    "                                                         , 1, 0)\n",
    "\n",
    "yearly_comp_data['possible_acq_but_no_data_shift1'] = yearly_comp_data['possible_acq_but_no_data'].shift(1)\n",
    "yearly_comp_data['possible_acq_but_no_data_shift2'] = yearly_comp_data['possible_acq_but_no_data'].shift(2)\n",
    "yearly_comp_data['possible_acq_but_no_data_shift3'] = yearly_comp_data['possible_acq_but_no_data'].shift(3)\n",
    "yearly_comp_data['possible_acq_but_no_data_shift4'] = yearly_comp_data['possible_acq_but_no_data'].shift(4)\n",
    "\n",
    "yearly_comp_data['possible_acq_but_no_data_prev_5Y'] = yearly_comp_data['possible_acq_but_no_data'] + yearly_comp_data['possible_acq_but_no_data_shift1']+yearly_comp_data['possible_acq_but_no_data_shift2']+yearly_comp_data['possible_acq_but_no_data_shift3']+yearly_comp_data['possible_acq_but_no_data_shift4']\n",
    "\n",
    "yearly_comp_data['possible_acq_but_no_data_prev_5Y'] = np.where(((yearly_comp_data['gvkey'] == yearly_comp_data['gvkey_shift4']) &\n",
    "                                                                (yearly_comp_data['fiscal_year'] - yearly_comp_data['fiscal_year_shift4'] == 4)),\n",
    "                                                                yearly_comp_data['possible_acq_but_no_data_prev_5Y'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to create similar set of variables for calculation of 3Y revenue growth rate\n",
    "yearly_comp_data['gvkey_shift2'] = yearly_comp_data['gvkey'].shift(2)\n",
    "yearly_comp_data['fiscal_year_shift2'] = yearly_comp_data['fiscal_year'].shift(2)\n",
    "\n",
    "yearly_comp_data['acquisition_sales_3Y'] = yearly_comp_data['acquisition_sales_contribution'] + yearly_comp_data['acquisition_sales_contribution_shift1'] + yearly_comp_data['acquisition_sales_contribution_shift2']\n",
    "\n",
    "yearly_comp_data['acquisition_sales_3Y'] = np.where(yearly_comp_data['gvkey'] == yearly_comp_data['gvkey_shift2'],\n",
    "                                                    yearly_comp_data['acquisition_sales_3Y'], None)\n",
    "\n",
    "yearly_comp_data['possible_acq_but_no_data_prev_3Y'] = yearly_comp_data['possible_acq_but_no_data'] + yearly_comp_data['possible_acq_but_no_data_shift1']+yearly_comp_data['possible_acq_but_no_data_shift2']\n",
    "\n",
    "yearly_comp_data['possible_acq_but_no_data_prev_3Y'] = np.where(((yearly_comp_data['gvkey'] == yearly_comp_data['gvkey_shift2']) &\n",
    "                                                                (yearly_comp_data['fiscal_year'] - yearly_comp_data['fiscal_year_shift2'] == 2)),\n",
    "                                                                yearly_comp_data['possible_acq_but_no_data_prev_3Y'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to create similar set of variables for calculation of 1Y revenue growth rate\n",
    "yearly_comp_data['acquisition_sales_1Y'] = yearly_comp_data['acquisition_sales_contribution']\n",
    "\n",
    "yearly_comp_data['acquisition_sales_1Y'] = np.where(yearly_comp_data['gvkey'] == yearly_comp_data['gvkey'],\n",
    "                                                    yearly_comp_data['acquisition_sales_1Y'], None)\n",
    "\n",
    "yearly_comp_data['possible_acq_but_no_data_prev_1Y'] = yearly_comp_data['possible_acq_but_no_data']\n",
    "\n",
    "yearly_comp_data['possible_acq_but_no_data_prev_1Y'] = np.where(((yearly_comp_data['gvkey'] == yearly_comp_data['gvkey']) &\n",
    "                                                                (yearly_comp_data['fiscal_year'] == yearly_comp_data['fiscal_year'])),\n",
    "                                                                yearly_comp_data['possible_acq_but_no_data_prev_1Y'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_tobe_adjusted_for_currency_exchange_rate = ['acquisition_income_contribution','acquisition_sales_contribution',\n",
    "                                                 'acquisition_sales_5Y', 'acquisition_sales_3Y', 'acquisition_sales_1Y']\n",
    "\n",
    "for i in cols_tobe_adjusted_for_currency_exchange_rate:\n",
    "    yearly_comp_data[i] = yearly_comp_data[i] * yearly_comp_data['currency_exchange_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final grouping of yearly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id_group = ['gvkey']\n",
    "date_group = ['fiscal_year','financial_year_end_date']\n",
    "general_group = ['employees']\n",
    "acquisition_group = ['acquisition_sales_5Y', 'possible_acq_but_no_data_prev_5Y', 'acquisition_sales_3Y',\n",
    "                     'possible_acq_but_no_data_prev_3Y','acquisition_sales_1Y','possible_acq_but_no_data_prev_1Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_comp_data = yearly_comp_data[Id_group + date_group + general_group + acquisition_group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining quarterly and yearly fundemental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1828155, 51)\n",
      "(1828155, 60)\n"
     ]
    }
   ],
   "source": [
    "# For Q1, Q2, Q3 I need to merge last year annual characteristics (2018Q2 should be merged with 2017 annual)\n",
    "# For Q4, I need to merge current year annual characteristics (2018Q4 should be merged with 2018 annual)\n",
    "quarterly_comp_data['fiscal_year_mod'] = np.where(quarterly_comp_data['fiscal_quarter'] == 4.0, quarterly_comp_data['fiscal_year'], quarterly_comp_data['fiscal_year']-1)\n",
    "\n",
    "print(quarterly_comp_data.shape)\n",
    "\n",
    "quarterly_comp_data =  pd.merge(quarterly_comp_data,\n",
    "                                yearly_comp_data,\n",
    "                                left_on = ['gvkey','fiscal_year_mod'],\n",
    "                                right_on = ['gvkey','fiscal_year'],\n",
    "                                how = 'left')\n",
    "\n",
    "print(quarterly_comp_data.shape)\n",
    "\n",
    "quarterly_comp_data.rename(columns = {'fiscal_year_x':'fiscal_year'}, inplace = True)\n",
    "quarterly_comp_data = quarterly_comp_data.drop(['fiscal_year_y'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating derived variables from the combined quarterly and annual fundemental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to calculate revenue 1Y, 3Y, 5Y growth rate but to do that, I also need acquisition columns from yearly data\n",
    "# So, didn't create the rev. growth rate columns in quarterly data itself\n",
    "\n",
    "quarterly_comp_data = quarterly_comp_data.sort_values(by=['gvkey','fiscal_year','fiscal_quarter'],ascending = True)\n",
    "\n",
    "quarterly_comp_data['revenue_shift1'] = quarterly_comp_data['revenue'].shift(1)\n",
    "quarterly_comp_data['revenue_shift2'] = quarterly_comp_data['revenue'].shift(2)\n",
    "quarterly_comp_data['revenue_shift3'] = quarterly_comp_data['revenue'].shift(3)\n",
    "\n",
    "# I also need to calculate average of last 12M split adjusted shares\n",
    "# Because I intend to adjust my revenue 5Y CAGR for dilutions\n",
    "\n",
    "quarterly_comp_data['num_shares_eps_split_adj_shift1'] = quarterly_comp_data['num_shares_eps_split_adj'].shift(1)\n",
    "quarterly_comp_data['num_shares_eps_split_adj_shift2'] = quarterly_comp_data['num_shares_eps_split_adj'].shift(2)\n",
    "quarterly_comp_data['num_shares_eps_split_adj_shift3'] = quarterly_comp_data['num_shares_eps_split_adj'].shift(3)\n",
    "\n",
    "# Need to check gvkey & fiscal year are still the same\n",
    "quarterly_comp_data['gvkey_shift1'] = quarterly_comp_data['gvkey'].shift(1)\n",
    "quarterly_comp_data['gvkey_shift2'] = quarterly_comp_data['gvkey'].shift(2)\n",
    "quarterly_comp_data['gvkey_shift3'] = quarterly_comp_data['gvkey'].shift(3)\n",
    "quarterly_comp_data['gvkey_shift4'] = quarterly_comp_data['gvkey'].shift(4)\n",
    "\n",
    "quarterly_comp_data['fiscal_year_shift3'] = quarterly_comp_data['fiscal_year'].shift(3)\n",
    "\n",
    "# Calculating 12M trailing revenue for every quarter\n",
    "quarterly_comp_data['revenue_12M_nofill'] = quarterly_comp_data['revenue'] + quarterly_comp_data['revenue_shift1'] + quarterly_comp_data['revenue_shift2'] + quarterly_comp_data['revenue_shift3']\n",
    "quarterly_comp_data['revenue_12M_fill0'] = quarterly_comp_data['revenue'].fillna(0) + quarterly_comp_data['revenue_shift1'].fillna(0) + quarterly_comp_data['revenue_shift2'].fillna(0) + quarterly_comp_data['revenue_shift3'].fillna(0)\n",
    "\n",
    "quarterly_comp_data['revenue_12M'] = np.where(quarterly_comp_data['reporting_frequency'] == 'Q', \n",
    "                                              quarterly_comp_data['revenue_12M_nofill'], quarterly_comp_data['revenue_12M_fill0'])\n",
    "\n",
    "# However, If a SA company has more than 2 nulls in the preceding four revenue rows, should'nt be calculating revenue 12M in such cases\n",
    "quarterly_comp_data['rev_null_last_12M_count'] = quarterly_comp_data[['revenue','revenue_shift1','revenue_shift2','revenue_shift3']].isnull().sum(axis=1)\n",
    "quarterly_comp_data['revenue_12M'] = np.where(((quarterly_comp_data['reporting_frequency'] == 'SA') &\n",
    "                                               (quarterly_comp_data['rev_null_last_12M_count']>2)), None, quarterly_comp_data['revenue_12M'])\n",
    "\n",
    "quarterly_comp_data['revenue_12M'] = np.where(quarterly_comp_data['gvkey'] == quarterly_comp_data['gvkey_shift3'],\n",
    "                                             quarterly_comp_data['revenue_12M'], None)\n",
    "# The gap in yearly column btw current row and 4Q back row has to be either 0 (Q4) or 1 (for Q1,Q2,Q3)\n",
    "quarterly_comp_data['revenue_12M'] = np.where(quarterly_comp_data['fiscal_year_shift3']-quarterly_comp_data['fiscal_year']>1,\n",
    "                                              None,quarterly_comp_data['revenue_12M'])\n",
    "# Getting average of 12M \n",
    "quarterly_comp_data['num_shares_eps_split_adj_12M_avg'] = (quarterly_comp_data['num_shares_eps_split_adj']+ quarterly_comp_data['num_shares_eps_split_adj_shift1'] + quarterly_comp_data['num_shares_eps_split_adj_shift2'] + quarterly_comp_data['num_shares_eps_split_adj_shift3'])/4\n",
    "quarterly_comp_data['num_shares_eps_split_adj_12M_avg'] = np.where(quarterly_comp_data['gvkey'] == quarterly_comp_data['gvkey_shift3'],\n",
    "                                                                   quarterly_comp_data['num_shares_eps_split_adj_12M_avg'], None)\n",
    "\n",
    "quarterly_comp_data['num_shares_eps_split_adj_12M_avg'] = np.where(quarterly_comp_data['fiscal_year_shift3']-quarterly_comp_data['fiscal_year']>1,\n",
    "                                                                   None,quarterly_comp_data['num_shares_eps_split_adj_12M_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the biggest problems with yearly data is that it doesn't have acquisition dates\n",
    "# This is really problematic when calculating 5Y CAGR organic revenue TTM\n",
    "\n",
    "# Ex. for 2020Q2 row, it is really imp that no acquisition's happened between 2014Q2 and 2014Q4. \n",
    "# For the prev 5Y revenue, the denominator I am going to use includes 2014Q3 + 2014Q4 + 2015Q1 + 2015Q2\n",
    "# What if denominator got an unfair boost because an acquisition happened sometime in 2014Q4?\n",
    "\n",
    "# Similarly, it is really imp that no acquisition's happened during 2020.\n",
    "# What if an acquisition happened in 2020Q1 and is unfarily boosting numerator?\n",
    "\n",
    "# To mitigate the issue described above, need to identify the exact quarters where a major acquisition has happened\n",
    "# If there's atleast one quarter at the begining or end where intan grew massively, won't calculate revenue CAGR's for such rows\n",
    "\n",
    "quarterly_comp_data = quarterly_comp_data.sort_values(by=['gvkey','fiscal_year','fiscal_quarter'],ascending = True)\n",
    "\n",
    "quarterly_comp_data['intan_shift1'] = quarterly_comp_data['intangible_asset_fill0'].shift(1)\n",
    "quarterly_comp_data['intan_shift2'] = quarterly_comp_data['intangible_asset_fill0'].shift(2)\n",
    "quarterly_comp_data['intan_shift3'] = quarterly_comp_data['intangible_asset_fill0'].shift(3)\n",
    "quarterly_comp_data['intan_shift4'] = quarterly_comp_data['intangible_asset_fill0'].shift(4)\n",
    "\n",
    "quarterly_comp_data['asset_total_shift1'] = quarterly_comp_data['asset_total'].shift(1)\n",
    "quarterly_comp_data['asset_total_shift2'] = quarterly_comp_data['asset_total'].shift(2)\n",
    "quarterly_comp_data['asset_total_shift3'] = quarterly_comp_data['asset_total'].shift(3)\n",
    "\n",
    "# intan_asset_1Q_growth looks at growth in intangible assets as a proportion of total assets\n",
    "quarterly_comp_data['intan_asset_growth_check1'] =((quarterly_comp_data['intangible_asset_fill0'] - quarterly_comp_data['intan_shift1'])/quarterly_comp_data['asset_total'])*100\n",
    "quarterly_comp_data['intan_asset_growth_check1'] = np.where(quarterly_comp_data['gvkey'] == quarterly_comp_data['gvkey_shift1'], \n",
    "                                                            quarterly_comp_data['intan_asset_growth_check1'], None)\n",
    "\n",
    "quarterly_comp_data['intan_asset_growth_check2'] =((quarterly_comp_data['intan_shift1'] - quarterly_comp_data['intan_shift2'])/quarterly_comp_data['asset_total_shift1'])*100\n",
    "quarterly_comp_data['intan_asset_growth_check2'] = np.where(quarterly_comp_data['gvkey'] == quarterly_comp_data['gvkey_shift2'], \n",
    "                                                            quarterly_comp_data['intan_asset_growth_check2'], None)\n",
    "\n",
    "quarterly_comp_data['intan_asset_growth_check3'] =((quarterly_comp_data['intan_shift2'] - quarterly_comp_data['intan_shift3'])/quarterly_comp_data['asset_total_shift2'])*100\n",
    "quarterly_comp_data['intan_asset_growth_check3'] = np.where(quarterly_comp_data['gvkey'] == quarterly_comp_data['gvkey_shift3'], \n",
    "                                                            quarterly_comp_data['intan_asset_growth_check3'], None)\n",
    "\n",
    "quarterly_comp_data['intan_asset_growth_check4'] =((quarterly_comp_data['intan_shift3'] - quarterly_comp_data['intan_shift4'])/quarterly_comp_data['asset_total_shift3'])*100\n",
    "quarterly_comp_data['intan_asset_growth_check4'] = np.where(quarterly_comp_data['gvkey'] == quarterly_comp_data['gvkey_shift4'], \n",
    "                                                            quarterly_comp_data['intan_asset_growth_check4'], None)\n",
    "\n",
    "quarterly_comp_data['intan_asset_growth_max_over_4Q'] = quarterly_comp_data[['intan_asset_growth_check1', \n",
    "                                                                             'intan_asset_growth_check2', \n",
    "                                                                             'intan_asset_growth_check3', \n",
    "                                                                             'intan_asset_growth_check4']].max(axis=1)\n",
    "\n",
    "# intan_asset_growth_max_over_4Q will indicate if there's any quarter among the last 4 that had extraordinary bump\n",
    "quarterly_comp_data['intan_big_bump_last_4Q'] = np.where(quarterly_comp_data['intan_asset_growth_max_over_4Q'] > 10, 1, 0)\n",
    "\n",
    "quarterly_comp_data['intan_big_bump_last_4Q'] = np.where(quarterly_comp_data['fiscal_year_shift3']-quarterly_comp_data['fiscal_year']>1,\n",
    "                                                         None,quarterly_comp_data['intan_big_bump_last_4Q'])\n",
    "# Ofcourse, despite my best attempt's there remains one boundary condition which is very hard to tackle\n",
    "# What it, an acquisition is unfairly boosting numerator or denominator but that acq didn't increase intangible\n",
    "# Extremely hard to spot such acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1828155, 91)\n",
      "(1828155, 100)\n"
     ]
    }
   ],
   "source": [
    "# Merging 5Y, 3Y, 1Y back 12M revenue into quarterly data itself\n",
    "\n",
    "quarterly_comp_data['fiscal_year_prev_5years'] = quarterly_comp_data['fiscal_year'] + 5\n",
    "quarterly_comp_data['fiscal_year_prev_3years'] = quarterly_comp_data['fiscal_year'] + 3\n",
    "quarterly_comp_data['fiscal_year_prev_1year'] = quarterly_comp_data['fiscal_year'] + 1\n",
    "\n",
    "rev_prev_5Y_df = quarterly_comp_data[['gvkey', 'fiscal_year_prev_5years','fiscal_quarter', 'revenue_12M', \n",
    "                                      'intan_big_bump_last_4Q', 'num_shares_eps_split_adj_12M_avg']]\n",
    "rev_prev_5Y_df.rename(columns = {'fiscal_year_prev_5years': 'fiscal_year', 'revenue_12M':'revenue_12M_prev_5Y', \n",
    "                                 'intan_big_bump_last_4Q': 'intan_big_bump_last_4Q_5Y',\n",
    "                                'num_shares_eps_split_adj_12M_avg' : 'num_shares_eps_split_adj_12M_avg_5Y'}, inplace = True)\n",
    "\n",
    "rev_prev_3Y_df = quarterly_comp_data[['gvkey', 'fiscal_year_prev_3years','fiscal_quarter', 'revenue_12M', \n",
    "                                      'intan_big_bump_last_4Q','num_shares_eps_split_adj_12M_avg']]\n",
    "rev_prev_3Y_df.rename(columns = {'fiscal_year_prev_3years': 'fiscal_year', 'revenue_12M':'revenue_12M_prev_3Y',\n",
    "                                 'intan_big_bump_last_4Q': 'intan_big_bump_last_4Q_3Y',\n",
    "                                'num_shares_eps_split_adj_12M_avg' : 'num_shares_eps_split_adj_12M_avg_3Y'}, inplace = True)\n",
    "\n",
    "rev_prev_1Y_df = quarterly_comp_data[['gvkey', 'fiscal_year_prev_1year','fiscal_quarter', 'revenue_12M', \n",
    "                                      'intan_big_bump_last_4Q','num_shares_eps_split_adj_12M_avg']]\n",
    "rev_prev_1Y_df.rename(columns = {'fiscal_year_prev_1year': 'fiscal_year', 'revenue_12M':'revenue_12M_prev_1Y',\n",
    "                                 'intan_big_bump_last_4Q': 'intan_big_bump_last_4Q_1Y',\n",
    "                                 'num_shares_eps_split_adj_12M_avg': 'num_shares_eps_split_adj_12M_avg_1Y'}, inplace = True)\n",
    "\n",
    "rev_prev_df = [ rev_prev_1Y_df, rev_prev_3Y_df, rev_prev_5Y_df]\n",
    "rev_prev_df = reduce(lambda left, right: pd.merge(left, right,on = ['gvkey','fiscal_year','fiscal_quarter'], how = 'outer'), \n",
    "                     rev_prev_df)\n",
    "\n",
    "print(quarterly_comp_data.shape)\n",
    "quarterly_comp_data = pd.merge(quarterly_comp_data,\n",
    "                               rev_prev_df,\n",
    "                               left_on = ['gvkey','fiscal_year','fiscal_quarter'],\n",
    "                               right_on = ['gvkey','fiscal_year','fiscal_quarter'],\n",
    "                               how = 'left')\n",
    "print(quarterly_comp_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_comp_data[['revenue_12M', 'acquisition_sales_5Y', \n",
    "                     'revenue_12M_prev_5Y']] = quarterly_comp_data[['revenue_12M', 'acquisition_sales_5Y', \n",
    "                                                                    'revenue_12M_prev_5Y']].astype('float64')\n",
    "\n",
    "# I intend to calculate 3 versions of revenue 5Y CAGR\n",
    "# 1) Revenue 5Y CAGR (actual without any dilution/buy-back adjustments)\n",
    "# 2) Revenue 5Y CAGR adjusted for dilutions/buy-back (if a company has diluted their equity to grow revenue, I want to adjust for that)\n",
    "# 3) Revenue 5Y CAGR adjusted for dilutions but not buy-backs\n",
    "\n",
    "# Writing formulae to calculate 5Y Revenue CAGR\n",
    "# 3 most important steps in creating formulaes (formulation)\n",
    "# 1) Imagine multiple situations and think about whether the output should increase or decrease\n",
    "# 2) Brainstorm multiple formulae that capture the essence of required output\n",
    "# 3) Think very carefully about boundary conditions\n",
    "\n",
    "quarterly_comp_data['revenue_5Y_CAGR_no_dilution'] = ((quarterly_comp_data['revenue_12M'] - quarterly_comp_data['acquisition_sales_5Y'].fillna(0))/quarterly_comp_data['revenue_12M_prev_5Y'])**0.2 - 1\n",
    "quarterly_comp_data['revenue_5Y_CAGR_no_dilution'] = pd.to_numeric(quarterly_comp_data['revenue_5Y_CAGR_no_dilution'])\n",
    "quarterly_comp_data['revenue_5Y_CAGR_no_dilution'][np.isinf(quarterly_comp_data['revenue_5Y_CAGR_no_dilution'])] = None\n",
    "quarterly_comp_data['revenue_5Y_CAGR_no_dilution'] = quarterly_comp_data['revenue_5Y_CAGR_no_dilution']*100\n",
    "\n",
    "# As discussed earlier, not calculating revenue CAGR for rows which involve years with possible acq but no exact acq sale amt.\n",
    "quarterly_comp_data['revenue_5Y_CAGR_no_dilution'] = np.where(quarterly_comp_data['possible_acq_but_no_data_prev_5Y'] > 0, None, \n",
    "                                                              quarterly_comp_data['revenue_5Y_CAGR_no_dilution'])\n",
    "\n",
    "# Also, erasing revenue_5Y_CAGR calculation for rows where a major acquisition happened during quarters which are in numerator/denominator\n",
    "quarterly_comp_data['revenue_5Y_CAGR_no_dilution'] = np.where(((quarterly_comp_data['intan_big_bump_last_4Q_5Y'] == 1) | \n",
    "                                                               (quarterly_comp_data['intan_big_bump_last_4Q'] == 1)), None, \n",
    "                                                               quarterly_comp_data['revenue_5Y_CAGR_no_dilution'])\n",
    "\n",
    "\n",
    "# Second version of 5Y CAGR\n",
    "quarterly_comp_data['num_shares_eps_split_adj_12M_avg_5Y'] = pd.to_numeric(quarterly_comp_data['num_shares_eps_split_adj_12M_avg_5Y']).astype('float64')\n",
    "quarterly_comp_data['num_shares_eps_split_adj_12M_avg'] = pd.to_numeric(quarterly_comp_data['num_shares_eps_split_adj_12M_avg']).astype('float64')\n",
    "quarterly_comp_data['shares_dilution_factor_5Y'] = quarterly_comp_data['num_shares_eps_split_adj_12M_avg_5Y']/quarterly_comp_data['num_shares_eps_split_adj_12M_avg']\n",
    "quarterly_comp_data['shares_dilution_factor_5Y'][np.isinf(quarterly_comp_data['shares_dilution_factor_5Y'])] = None\n",
    "\n",
    "\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'] = (((quarterly_comp_data['revenue_12M'] - quarterly_comp_data['acquisition_sales_5Y'].fillna(0))/quarterly_comp_data['revenue_12M_prev_5Y'])*quarterly_comp_data['shares_dilution_factor_5Y'])**0.2 - 1\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'] = pd.to_numeric(quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'])\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'][np.isinf(quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'])] = None\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'] = quarterly_comp_data['revenue_5Y_CAGR_adj_dilution']*100\n",
    "\n",
    "# As discussed earlier, not calculating revenue CAGR for rows which involve years with possible acq but no exact acq sale amt.\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'] = np.where(quarterly_comp_data['possible_acq_but_no_data_prev_5Y'] > 0, None, \n",
    "                                                               quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'])\n",
    "\n",
    "# Also, erasing revenue_5Y_CAGR calculation for rows where a major acquisition happened during quarters which are in numerator/denominator\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'] = np.where(((quarterly_comp_data['intan_big_bump_last_4Q_5Y'] == 1) | \n",
    "                                                                (quarterly_comp_data['intan_big_bump_last_4Q'] == 1)), None, \n",
    "                                                                 quarterly_comp_data['revenue_5Y_CAGR_adj_dilution'])\n",
    "\n",
    "\n",
    "# Third version of 5Y CAGR\n",
    "quarterly_comp_data['shares_dilution_factor_5Y_max1'] = np.where(quarterly_comp_data['shares_dilution_factor_5Y'] > 1, 1,\n",
    "                                                                quarterly_comp_data['shares_dilution_factor_5Y'])\n",
    "\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'] = (((quarterly_comp_data['revenue_12M'] - quarterly_comp_data['acquisition_sales_5Y'].fillna(0))/quarterly_comp_data['revenue_12M_prev_5Y'])*quarterly_comp_data['shares_dilution_factor_5Y_max1'])**0.2 - 1\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'] = pd.to_numeric(quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'])\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'][np.isinf(quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'])] = None\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'] = quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1']*100\n",
    "\n",
    "# As discussed earlier, not calculating revenue CAGR for rows which involve years with possible acq but no exact acq sale amt.\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'] = np.where(quarterly_comp_data['possible_acq_but_no_data_prev_5Y'] > 0, None, \n",
    "                                                                    quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'])\n",
    "\n",
    "# Also, erasing revenue_1Y_CAGR calculation for rows where a major acquisition happened during quarters which are in numerator/denominator\n",
    "quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'] = np.where(((quarterly_comp_data['intan_big_bump_last_4Q_5Y'] == 1) | \n",
    "                                                                     (quarterly_comp_data['intan_big_bump_last_4Q'] == 1)), None, \n",
    "                                                                      quarterly_comp_data['revenue_5Y_CAGR_adj_dilution_max1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_comp_data[['revenue_12M', 'acquisition_sales_3Y', \n",
    "                     'revenue_12M_prev_3Y']] = quarterly_comp_data[['revenue_12M', 'acquisition_sales_3Y', \n",
    "                                                                    'revenue_12M_prev_3Y']].astype('float64')\n",
    "\n",
    "# I intend to calculate 3 versions of revenue 3Y CAGR\n",
    "# 1) Revenue 3Y CAGR (actual without any dilution/buy-back adjustments)\n",
    "# 2) Revenue 3Y CAGR adjusted for dilutions/buy-back (if a company has diluted their equity to grow revenue, I want to adjust for that)\n",
    "# 3) Revenue 3Y CAGR adjusted for dilutions but not buy-backs\n",
    "\n",
    "quarterly_comp_data['revenue_3Y_CAGR_no_dilution'] = ((quarterly_comp_data['revenue_12M'] - quarterly_comp_data['acquisition_sales_3Y'].fillna(0))/quarterly_comp_data['revenue_12M_prev_3Y'])**0.333 - 1\n",
    "quarterly_comp_data['revenue_3Y_CAGR_no_dilution'] = pd.to_numeric(quarterly_comp_data['revenue_3Y_CAGR_no_dilution'])\n",
    "quarterly_comp_data['revenue_3Y_CAGR_no_dilution'][np.isinf(quarterly_comp_data['revenue_3Y_CAGR_no_dilution'])] = None\n",
    "quarterly_comp_data['revenue_3Y_CAGR_no_dilution'] = quarterly_comp_data['revenue_3Y_CAGR_no_dilution']*100\n",
    "\n",
    "# As discussed earlier, not calculating revenue CAGR for rows which involve years with possible acq but no exact acq sale amt.\n",
    "quarterly_comp_data['revenue_3Y_CAGR_no_dilution'] = np.where(quarterly_comp_data['possible_acq_but_no_data_prev_3Y'] > 0, None, \n",
    "                                                              quarterly_comp_data['revenue_3Y_CAGR_no_dilution'])\n",
    "\n",
    "# Also, erasing revenue_3Y_CAGR calculation for rows where a major acquisition happened during quarters which are in numerator/denominator\n",
    "quarterly_comp_data['revenue_3Y_CAGR_no_dilution'] = np.where(((quarterly_comp_data['intan_big_bump_last_4Q_3Y'] == 1) | \n",
    "                                                               (quarterly_comp_data['intan_big_bump_last_4Q'] == 1)), None, \n",
    "                                                               quarterly_comp_data['revenue_3Y_CAGR_no_dilution'])\n",
    "\n",
    "\n",
    "# Second version of 3Y CAGR\n",
    "quarterly_comp_data['num_shares_eps_split_adj_12M_avg_3Y'] = pd.to_numeric(quarterly_comp_data['num_shares_eps_split_adj_12M_avg_3Y']).astype('float64')\n",
    "quarterly_comp_data['num_shares_eps_split_adj_12M_avg'] = pd.to_numeric(quarterly_comp_data['num_shares_eps_split_adj_12M_avg']).astype('float64')\n",
    "\n",
    "quarterly_comp_data['shares_dilution_factor_3Y'] = quarterly_comp_data['num_shares_eps_split_adj_12M_avg_3Y']/quarterly_comp_data['num_shares_eps_split_adj_12M_avg']\n",
    "quarterly_comp_data['shares_dilution_factor_3Y'][np.isinf(quarterly_comp_data['shares_dilution_factor_3Y'])] = None\n",
    "\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'] = (((quarterly_comp_data['revenue_12M'] - quarterly_comp_data['acquisition_sales_3Y'].fillna(0))/quarterly_comp_data['revenue_12M_prev_3Y'])*quarterly_comp_data['shares_dilution_factor_3Y'])**0.333 - 1\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'] = pd.to_numeric(quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'])\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'][np.isinf(quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'])] = None\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'] = quarterly_comp_data['revenue_3Y_CAGR_adj_dilution']*100\n",
    "\n",
    "# As discussed earlier, not calculating revenue CAGR for rows which involve years with possible acq but no exact acq sale amt.\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'] = np.where(quarterly_comp_data['possible_acq_but_no_data_prev_3Y'] > 0, None, \n",
    "                                                               quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'])\n",
    "\n",
    "# Also, erasing revenue_3Y_CAGR calculation for rows where a major acquisition happened during quarters which are in numerator/denominator\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'] = np.where(((quarterly_comp_data['intan_big_bump_last_4Q_3Y'] == 1) | \n",
    "                                                                (quarterly_comp_data['intan_big_bump_last_4Q'] == 1)), None, \n",
    "                                                                 quarterly_comp_data['revenue_3Y_CAGR_adj_dilution'])\n",
    "\n",
    "\n",
    "# Third version of 3Y CAGR\n",
    "quarterly_comp_data['shares_dilution_factor_3Y_max1'] = np.where(quarterly_comp_data['shares_dilution_factor_3Y'] > 1, 1,\n",
    "                                                                quarterly_comp_data['shares_dilution_factor_3Y'])\n",
    "\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'] = (((quarterly_comp_data['revenue_12M'] - quarterly_comp_data['acquisition_sales_3Y'].fillna(0))/quarterly_comp_data['revenue_12M_prev_3Y'])*quarterly_comp_data['shares_dilution_factor_3Y_max1'])**0.333 - 1\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'] = pd.to_numeric(quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'])\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'][np.isinf(quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'])] = None\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'] = quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1']*100\n",
    "\n",
    "# As discussed earlier, not calculating revenue CAGR for rows which involve years with possible acq but no exact acq sale amt.\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'] = np.where(quarterly_comp_data['possible_acq_but_no_data_prev_3Y'] > 0, None, \n",
    "                                                                    quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'])\n",
    "\n",
    "# Also, erasing revenue_1Y_CAGR calculation for rows where a major acquisition happened during quarters which are in numerator/denominator\n",
    "quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'] = np.where(((quarterly_comp_data['intan_big_bump_last_4Q_3Y'] == 1) | \n",
    "                                                                     (quarterly_comp_data['intan_big_bump_last_4Q'] == 1)), None, \n",
    "                                                                      quarterly_comp_data['revenue_3Y_CAGR_adj_dilution_max1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_comp_data[['revenue_12M', 'acquisition_sales_1Y', \n",
    "                     'revenue_12M_prev_1Y']] = quarterly_comp_data[['revenue_12M', 'acquisition_sales_1Y', \n",
    "                                                                    'revenue_12M_prev_1Y']].astype('float64')\n",
    "\n",
    "# I intend to calculate 3 versions of revenue 1Y CAGR\n",
    "# 1) Revenue 1Y CAGR (actual without any dilution/buy-back adjustments)\n",
    "# 2) Revenue 1Y CAGR adjusted for dilutions/buy-back (if a company has diluted their equity to grow revenue, I want to adjust for that)\n",
    "# 3) Revenue 1Y CAGR adjusted for dilutions but not buy-backs\n",
    "\n",
    "quarterly_comp_data['revenue_1Y_CAGR_no_dilution'] = ((quarterly_comp_data['revenue_12M'] - quarterly_comp_data['acquisition_sales_1Y'].fillna(0))/quarterly_comp_data['revenue_12M_prev_1Y'])**1 - 1\n",
    "quarterly_comp_data['revenue_1Y_CAGR_no_dilution'] = pd.to_numeric(quarterly_comp_data['revenue_1Y_CAGR_no_dilution'])\n",
    "quarterly_comp_data['revenue_1Y_CAGR_no_dilution'][np.isinf(quarterly_comp_data['revenue_1Y_CAGR_no_dilution'])] = None\n",
    "quarterly_comp_data['revenue_1Y_CAGR_no_dilution'] = quarterly_comp_data['revenue_1Y_CAGR_no_dilution']*100\n",
    "\n",
    "# As discussed earlier, not calculating revenue CAGR for rows which involve years with possible acq but no exact acq sale amt.\n",
    "quarterly_comp_data['revenue_1Y_CAGR_no_dilution'] = np.where(quarterly_comp_data['possible_acq_but_no_data_prev_1Y'] > 0, None, \n",
    "                                                              quarterly_comp_data['revenue_1Y_CAGR_no_dilution'])\n",
    "\n",
    "# Also, erasing revenue_1Y_CAGR calculation for rows where a major acquisition happened during quarters which are in numerator/denominator\n",
    "quarterly_comp_data['revenue_1Y_CAGR_no_dilution'] = np.where(((quarterly_comp_data['intan_big_bump_last_4Q_1Y'] == 1) | \n",
    "                                                               (quarterly_comp_data['intan_big_bump_last_4Q'] == 1)), None, \n",
    "                                                               quarterly_comp_data['revenue_1Y_CAGR_no_dilution'])\n",
    "\n",
    "\n",
    "# Second version of 1Y CAGR\n",
    "quarterly_comp_data['num_shares_eps_split_adj_12M_avg_1Y'] = pd.to_numeric(quarterly_comp_data['num_shares_eps_split_adj_12M_avg_1Y']).astype('float64')\n",
    "quarterly_comp_data['num_shares_eps_split_adj_12M_avg'] = pd.to_numeric(quarterly_comp_data['num_shares_eps_split_adj_12M_avg']).astype('float64')\n",
    "quarterly_comp_data['shares_dilution_factor_1Y'] = quarterly_comp_data['num_shares_eps_split_adj_12M_avg_1Y']/quarterly_comp_data['num_shares_eps_split_adj_12M_avg']\n",
    "quarterly_comp_data['shares_dilution_factor_1Y'][np.isinf(quarterly_comp_data['shares_dilution_factor_1Y'])] = None\n",
    "\n",
    "\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'] = (((quarterly_comp_data['revenue_12M'] - quarterly_comp_data['acquisition_sales_1Y'].fillna(0))/quarterly_comp_data['revenue_12M_prev_1Y'])*quarterly_comp_data['shares_dilution_factor_1Y'])**1 - 1\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'] = pd.to_numeric(quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'])\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'][np.isinf(quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'])] = None\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'] = quarterly_comp_data['revenue_1Y_CAGR_adj_dilution']*100\n",
    "\n",
    "# As discussed earlier, not calculating revenue CAGR for rows which involve years with possible acq but no exact acq sale amt.\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'] = np.where(quarterly_comp_data['possible_acq_but_no_data_prev_1Y'] > 0, None, \n",
    "                                                               quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'])\n",
    "\n",
    "# Also, erasing revenue_1Y_CAGR calculation for rows where a major acquisition happened during quarters which are in numerator/denominator\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'] = np.where(((quarterly_comp_data['intan_big_bump_last_4Q_1Y'] == 1) | \n",
    "                                                                (quarterly_comp_data['intan_big_bump_last_4Q'] == 1)), None, \n",
    "                                                                 quarterly_comp_data['revenue_1Y_CAGR_adj_dilution'])\n",
    "\n",
    "\n",
    "# Third version of 1Y CAGR\n",
    "quarterly_comp_data['shares_dilution_factor_1Y_max1'] = np.where(quarterly_comp_data['shares_dilution_factor_1Y'] > 1, 1,\n",
    "                                                                quarterly_comp_data['shares_dilution_factor_1Y'])\n",
    "\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'] = (((quarterly_comp_data['revenue_12M'] - quarterly_comp_data['acquisition_sales_1Y'].fillna(0))/quarterly_comp_data['revenue_12M_prev_1Y'])*quarterly_comp_data['shares_dilution_factor_1Y_max1'])**1 - 1\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'] = pd.to_numeric(quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'])\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'][np.isinf(quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'])] = None\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'] = quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1']*100\n",
    "\n",
    "# As discussed earlier, not calculating revenue CAGR for rows which involve years with possible acq but no exact acq sale amt.\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'] = np.where(quarterly_comp_data['possible_acq_but_no_data_prev_1Y'] > 0, None, \n",
    "                                                                    quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'])\n",
    "\n",
    "# Also, erasing revenue_1Y_CAGR calculation for rows where a major acquisition happened during quarters which are in numerator/denominator\n",
    "quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'] = np.where(((quarterly_comp_data['intan_big_bump_last_4Q_1Y'] == 1) | \n",
    "                                                                     (quarterly_comp_data['intan_big_bump_last_4Q'] == 1)), None, \n",
    "                                                                      quarterly_comp_data['revenue_1Y_CAGR_adj_dilution_max1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "revenue                               454896\n",
       "revenue_12M                           582529\n",
       "revenue_5Y_CAGR_no_dilution          1212119\n",
       "revenue_3Y_CAGR_no_dilution          1065256\n",
       "revenue_1Y_CAGR_no_dilution           869250\n",
       "revenue_5Y_CAGR_adj_dilution         1244822\n",
       "revenue_3Y_CAGR_adj_dilution         1103734\n",
       "revenue_1Y_CAGR_adj_dilution          915451\n",
       "revenue_5Y_CAGR_adj_dilution_max1    1244822\n",
       "revenue_3Y_CAGR_adj_dilution_max1    1103734\n",
       "revenue_1Y_CAGR_adj_dilution_max1     915451\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarterly_comp_data[['revenue','revenue_12M', 'revenue_5Y_CAGR_no_dilution', 'revenue_3Y_CAGR_no_dilution', \n",
    "                     'revenue_1Y_CAGR_no_dilution', 'revenue_5Y_CAGR_adj_dilution', 'revenue_3Y_CAGR_adj_dilution', \n",
    "                     'revenue_1Y_CAGR_adj_dilution', 'revenue_5Y_CAGR_adj_dilution_max1', 'revenue_3Y_CAGR_adj_dilution_max1', \n",
    "                     'revenue_1Y_CAGR_adj_dilution_max1']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_comp_data[['revenue','revenue_12M', 'revenue_5Y_CAGR_no_dilution', 'revenue_3Y_CAGR_no_dilution', \n",
    "                     'revenue_1Y_CAGR_no_dilution', 'revenue_5Y_CAGR_adj_dilution', 'revenue_3Y_CAGR_adj_dilution', \n",
    "                     'revenue_1Y_CAGR_adj_dilution', 'revenue_5Y_CAGR_adj_dilution_max1', 'revenue_3Y_CAGR_adj_dilution_max1', \n",
    "                     'revenue_1Y_CAGR_adj_dilution_max1']] = quarterly_comp_data[['revenue','revenue_12M', 'revenue_5Y_CAGR_no_dilution', 'revenue_3Y_CAGR_no_dilution', \n",
    "                     'revenue_1Y_CAGR_no_dilution', 'revenue_5Y_CAGR_adj_dilution', 'revenue_3Y_CAGR_adj_dilution', \n",
    "                     'revenue_1Y_CAGR_adj_dilution', 'revenue_5Y_CAGR_adj_dilution_max1', 'revenue_3Y_CAGR_adj_dilution_max1', \n",
    "                     'revenue_1Y_CAGR_adj_dilution_max1']].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_comp_data = quarterly_comp_data.sort_values(by=['gvkey','fiscal_year','fiscal_quarter'],ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quarterly_comp_data[['gvkey','fiscal_year','fiscal_quarter','revenue','revenue_12M','revenue_12M_prev_1Y','revenue_12M_prev_3Y',\n",
    "#                     'revenue_12M_prev_5Y','revenue_5Y_CAGR','revenue_3Y_CAGR','revenue_1Y_CAGR', 'intan_big_bump_last_4Q', \n",
    "#                     'intan_big_bump_last_4Q_1Y', 'intan_big_bump_last_4Q_3Y','intan_big_bump_last_4Q_5Y',\n",
    "#                     'intangible_asset_fill0','intangible_asset_total','asset_total','intan_asset_growth_check1',\n",
    "#                     'intan_asset_growth_check2', 'intan_asset_growth_check3','intan_asset_growth_check4']].to_csv('sdf.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601669"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how well the result date column is filled as I need that column to join fundemental, Monthly data\n",
    "quarterly_comp_data['result_reported_date'].isnull().sum()\n",
    "# Oh WOW! there are nearly 600k nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    313370.000000\n",
      "mean         86.176536\n",
      "std          96.815035\n",
      "min        -353.000000\n",
      "25%          34.000000\n",
      "50%          61.000000\n",
      "75%         106.000000\n",
      "max        3255.000000\n",
      "Name: result_financial_end_date_gap, dtype: float64\n",
      "count    913116.000000\n",
      "mean         30.028176\n",
      "std          87.995544\n",
      "min        -323.000000\n",
      "25%          19.000000\n",
      "50%          28.000000\n",
      "75%          47.000000\n",
      "max        7351.000000\n",
      "Name: result_financial_end_date_gap, dtype: float64\n",
      "915039\n",
      "(1828155, 117)\n",
      "(1828155, 118)\n",
      "418000\n"
     ]
    }
   ],
   "source": [
    "quarterly_comp_data['result_financial_end_date_gap'] = pd.to_datetime(quarterly_comp_data['result_reported_date']) - pd.to_datetime(quarterly_comp_data['quarter_end_date'])\n",
    "quarterly_comp_data['result_financial_end_date_gap'] = quarterly_comp_data['result_financial_end_date_gap'].dt.days\n",
    "print(quarterly_comp_data[quarterly_comp_data['fiscal_quarter'] == 4]['result_financial_end_date_gap'].describe())\n",
    "print(quarterly_comp_data[quarterly_comp_data['fiscal_quarter'] != 4]['result_financial_end_date_gap'].describe())\n",
    "# Realized that the gap btw. quarter end date and result reported date is really high for Q4 because comp data has used annual report release date for Q4 result dates\n",
    "# I need to correct this\n",
    "\n",
    "quarterly_comp_data['result_reported_date_mod'] = np.where(quarterly_comp_data['fiscal_quarter'] == 4, None,\n",
    "                                                           quarterly_comp_data['result_reported_date'])\n",
    "print(quarterly_comp_data['result_reported_date_mod'].isnull().sum())\n",
    "# Now, there are nearly 900k nulls in result date mod column\n",
    "\n",
    "# For the rows where result reported date mod is missing, I am going to impute with quarter_end_date + median of result_financial_end_date_gap (for last year results)\n",
    "gvkey_year_gap_mod = pd.DataFrame(quarterly_comp_data.groupby(['gvkey','fiscal_year'])['result_financial_end_date_gap'].median()).reset_index()\n",
    "gvkey_year_gap_mod.columns = ['gvkey','fiscal_year','median_financial_end_days_gap']\n",
    "gvkey_year_gap_mod['fiscal_year'] = gvkey_year_gap_mod['fiscal_year'] - 1\n",
    "\n",
    "print(quarterly_comp_data.shape)\n",
    "quarterly_comp_data = pd.merge(quarterly_comp_data,\n",
    "                               gvkey_year_gap_mod,\n",
    "                               left_on = ['gvkey','fiscal_year'],\n",
    "                               right_on = ['gvkey','fiscal_year'],\n",
    "                               how = 'left')\n",
    "print(quarterly_comp_data.shape)\n",
    "quarterly_comp_data['result_reported_date_mod'] = quarterly_comp_data['result_reported_date_mod'].fillna(pd.to_datetime(quarterly_comp_data['quarter_end_date']) + pd.to_timedelta(quarterly_comp_data['median_financial_end_days_gap'], unit = 'd'))\n",
    "\n",
    "print(quarterly_comp_data['result_reported_date_mod'].isnull().sum())\n",
    "# Despite my best attempts, I am still unable to reliably estimate result reported date for almost 400k rows\n",
    "\n",
    "# For all rows where result reported date is still missing, I am just going to impute by adding 60 days to quarter end date\n",
    "quarterly_comp_data['result_reported_date_mod'] = quarterly_comp_data['result_reported_date_mod'].fillna(pd.to_datetime(quarterly_comp_data['quarter_end_date']) + \n",
    "                                                                                                         timedelta(days=55))\n",
    "\n",
    "# Removing hours from result reported date mod \n",
    "quarterly_comp_data['result_reported_date_mod'] = pd.to_datetime(quarterly_comp_data['result_reported_date_mod']).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundemental data final grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_comp_data.rename(columns = {'intan_big_bump_last_4Q': 'major_acq_12M'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cusip', 'gvkey', 'ticker', 'company_name', 'quarter_end_date',\n",
       "       'fiscal_quarter', 'fiscal_year', 'reporting_frequency',\n",
       "       'result_reported_date', 'split_adjusting_factor',\n",
       "       ...\n",
       "       'shares_dilution_factor_3Y_max1', 'revenue_3Y_CAGR_adj_dilution_max1',\n",
       "       'revenue_1Y_CAGR_no_dilution', 'shares_dilution_factor_1Y',\n",
       "       'revenue_1Y_CAGR_adj_dilution', 'shares_dilution_factor_1Y_max1',\n",
       "       'revenue_1Y_CAGR_adj_dilution_max1', 'result_financial_end_date_gap',\n",
       "       'result_reported_date_mod', 'median_financial_end_days_gap'],\n",
       "      dtype='object', length=118)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarterly_comp_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Grouping columns\n",
    "id_group_col = ['cusip','gvkey','ticker','company_name']\n",
    "\n",
    "date_group_col = ['quarter_end_date','fiscal_quarter','fiscal_year','reporting_frequency','result_reported_date', \n",
    "                  'result_reported_date_mod']\n",
    "\n",
    "general_group_col = ['split_adjusting_factor','S&P_grade','employees','sic_code', 'NAICS', 'Mcap']\n",
    "\n",
    "balance_sheet_ratios_group = ['percentage_current_asset','percentage_equity','percentage_cash_st','percentage_LT_debt',\n",
    "                              'percentage_current_asset_intan','percentage_equity_intan','percentage_cash_st_intan',\n",
    "                              'percentage_LT_debt_intan','percentage_current_asset_1Y_change','percentage_equity_1Y_change',\n",
    "                              'percentage_current_asset_intan_1Y_change','percentage_equity_intan_1Y_change',\n",
    "                              'percentage_cash_st_intan_1Y_change','percentage_LT_debt_intan_1Y_change',\n",
    "                              'percentage_cash_st_1Y_change','percentage_LT_debt_1Y_change']\n",
    "\n",
    "additional_balance_sheet_group = ['tangible_equity','share_holder_equity', 'intangible_asset_total','asset_total','major_acq_12M']\n",
    "\n",
    "eps_group = ['eps_d_12M','eps_d_core_excl_extr','eps_d','eps_d_core_excl_extr_12M','eps_12M','eps_core_excl_extr_12M','eps',\n",
    "             'eps_core_excl_extr']\n",
    "\n",
    "num_shares_group = ['num_shares_eps_12','num_d_shares_eps_12','num_d_shares_eps','num_shares_eps','num_shares_eps_12_split_adj',\n",
    "                   'num_shares_eps_split_adj']\n",
    "\n",
    "#share_dilution_factor_group = ['shares_dilution_factor_5Y','shares_dilution_factor_5Y_max1','shares_dilution_factor_3Y',\n",
    "#                               'shares_dilution_factor_3Y_max1','shares_dilution_factor_1Y','shares_dilution_factor_1Y_max1']\n",
    "\n",
    "additional_income_group = ['revenue','revenue_12M', 'revenue_5Y_CAGR_no_dilution', 'revenue_3Y_CAGR_no_dilution', \n",
    "                           'revenue_1Y_CAGR_no_dilution','revenue_5Y_CAGR_adj_dilution','revenue_3Y_CAGR_adj_dilution', \n",
    "                           'revenue_1Y_CAGR_adj_dilution','revenue_5Y_CAGR_adj_dilution_max1','revenue_3Y_CAGR_adj_dilution_max1', \n",
    "                           'revenue_1Y_CAGR_adj_dilution_max1']\n",
    "\n",
    "final_cols_quarterly = id_group_col + date_group_col + general_group_col + balance_sheet_ratios_group + additional_balance_sheet_group + eps_group + num_shares_group + additional_income_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After selecting the final columns, 4 tests have to be performed before final write out\n",
    "# 1) Which columns have been excluded in final grouping\n",
    "# 2) Num of nulls in each selected column\n",
    "# 3) Num of unique values in each selected column\n",
    "# 4) Distribution of each selected column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>intangible_asset_fill0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>fiscal_year_mod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>financial_year_end_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>acquisition_sales_5Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>possible_acq_but_no_data_prev_5Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>acquisition_sales_3Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>possible_acq_but_no_data_prev_3Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>acquisition_sales_1Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>possible_acq_but_no_data_prev_1Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>revenue_shift1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>revenue_shift2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>revenue_shift3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>num_shares_eps_split_adj_shift1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>num_shares_eps_split_adj_shift2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>num_shares_eps_split_adj_shift3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>gvkey_shift1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>gvkey_shift2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>gvkey_shift3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>gvkey_shift4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>fiscal_year_shift3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>revenue_12M_nofill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>revenue_12M_fill0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>rev_null_last_12M_count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>num_shares_eps_split_adj_12M_avg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>intan_shift1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>intan_shift2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>intan_shift3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>intan_shift4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>asset_total_shift1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>asset_total_shift2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>asset_total_shift3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>intan_asset_growth_check1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>intan_asset_growth_check2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>intan_asset_growth_check3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>intan_asset_growth_check4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>intan_asset_growth_max_over_4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>fiscal_year_prev_5years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>fiscal_year_prev_3years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>fiscal_year_prev_1year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>revenue_12M_prev_1Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>intan_big_bump_last_4Q_1Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>num_shares_eps_split_adj_12M_avg_1Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>revenue_12M_prev_3Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>intan_big_bump_last_4Q_3Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>num_shares_eps_split_adj_12M_avg_3Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>revenue_12M_prev_5Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>intan_big_bump_last_4Q_5Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>num_shares_eps_split_adj_12M_avg_5Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>shares_dilution_factor_5Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>shares_dilution_factor_5Y_max1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>shares_dilution_factor_3Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>shares_dilution_factor_3Y_max1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>shares_dilution_factor_1Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>shares_dilution_factor_1Y_max1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>result_financial_end_date_gap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>median_financial_end_days_gap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             column_name\n",
       "33                intangible_asset_fill0\n",
       "50                       fiscal_year_mod\n",
       "51               financial_year_end_date\n",
       "53                  acquisition_sales_5Y\n",
       "54      possible_acq_but_no_data_prev_5Y\n",
       "55                  acquisition_sales_3Y\n",
       "56      possible_acq_but_no_data_prev_3Y\n",
       "57                  acquisition_sales_1Y\n",
       "58      possible_acq_but_no_data_prev_1Y\n",
       "59                        revenue_shift1\n",
       "60                        revenue_shift2\n",
       "61                        revenue_shift3\n",
       "62       num_shares_eps_split_adj_shift1\n",
       "63       num_shares_eps_split_adj_shift2\n",
       "64       num_shares_eps_split_adj_shift3\n",
       "65                          gvkey_shift1\n",
       "66                          gvkey_shift2\n",
       "67                          gvkey_shift3\n",
       "68                          gvkey_shift4\n",
       "69                    fiscal_year_shift3\n",
       "70                    revenue_12M_nofill\n",
       "71                     revenue_12M_fill0\n",
       "73               rev_null_last_12M_count\n",
       "74      num_shares_eps_split_adj_12M_avg\n",
       "75                          intan_shift1\n",
       "76                          intan_shift2\n",
       "77                          intan_shift3\n",
       "78                          intan_shift4\n",
       "79                    asset_total_shift1\n",
       "80                    asset_total_shift2\n",
       "81                    asset_total_shift3\n",
       "82             intan_asset_growth_check1\n",
       "83             intan_asset_growth_check2\n",
       "84             intan_asset_growth_check3\n",
       "85             intan_asset_growth_check4\n",
       "86        intan_asset_growth_max_over_4Q\n",
       "88               fiscal_year_prev_5years\n",
       "89               fiscal_year_prev_3years\n",
       "90                fiscal_year_prev_1year\n",
       "91                   revenue_12M_prev_1Y\n",
       "92             intan_big_bump_last_4Q_1Y\n",
       "93   num_shares_eps_split_adj_12M_avg_1Y\n",
       "94                   revenue_12M_prev_3Y\n",
       "95             intan_big_bump_last_4Q_3Y\n",
       "96   num_shares_eps_split_adj_12M_avg_3Y\n",
       "97                   revenue_12M_prev_5Y\n",
       "98             intan_big_bump_last_4Q_5Y\n",
       "99   num_shares_eps_split_adj_12M_avg_5Y\n",
       "101            shares_dilution_factor_5Y\n",
       "103       shares_dilution_factor_5Y_max1\n",
       "106            shares_dilution_factor_3Y\n",
       "108       shares_dilution_factor_3Y_max1\n",
       "111            shares_dilution_factor_1Y\n",
       "113       shares_dilution_factor_1Y_max1\n",
       "115        result_financial_end_date_gap\n",
       "117        median_financial_end_days_gap"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there's any imp columns that are in quarterly_comp_data but didn't make it into final group\n",
    "columns_in_quarterly_data = pd.DataFrame(quarterly_comp_data.columns)\n",
    "columns_in_quarterly_data.columns = ['column_name']\n",
    "columns_in_quarterly_data[~columns_in_quarterly_data['column_name'].isin(final_cols_quarterly)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing all selected variables\n",
    "final_variables_null_variables = pd.DataFrame(quarterly_comp_data[final_cols_quarterly].isnull().sum())\n",
    "final_variables_null_variables = final_variables_null_variables.reset_index()\n",
    "final_variables_null_variables.columns = ['column_name','#_null_values']\n",
    "#final_variables_null_variables.to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\fundamental_data_final_var_null_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving data of unique values into data frame\n",
    "final_variables_uniques = pd.DataFrame(quarterly_comp_data[final_cols_quarterly].apply(lambda x: x.nunique()))\n",
    "final_variables_uniques = final_variables_uniques.reset_index()\n",
    "final_variables_uniques.rename(columns = {'index':'Column_name', 0: 'Num_unique_values'}, inplace = True)\n",
    "#final_variables_uniques.to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\fundamental_data_final_var_unique_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cusip\n",
      "count     1828155\n",
      "unique      39303\n",
      "top     494368103\n",
      "freq          218\n",
      "              gvkey\n",
      "count  1.828155e+06\n",
      "mean   5.280025e+04\n",
      "std    6.364492e+04\n",
      "min    1.000000e+03\n",
      "25%    8.804000e+03\n",
      "50%    2.032300e+04\n",
      "75%    6.628100e+04\n",
      "max    3.459800e+05\n",
      "         ticker\n",
      "count   1828028\n",
      "unique    39297\n",
      "top         SEB\n",
      "freq        218\n",
      "        company_name\n",
      "count        1828155\n",
      "unique         39301\n",
      "top     UNILEVER PLC\n",
      "freq             426\n",
      "           quarter_end_date\n",
      "count               1828155\n",
      "unique                  648\n",
      "top     1999-03-31 00:00:00\n",
      "freq                  11092\n",
      "first   1967-01-31 00:00:00\n",
      "last    2020-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-90-3cead0be3e9f>:3: FutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now.\n",
      "  print(quarterly_comp_data[final_cols_quarterly[i:i+1]].describe())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fiscal_quarter\n",
      "count    1.828155e+06\n",
      "mean     2.481014e+00\n",
      "std      1.116447e+00\n",
      "min      1.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      2.000000e+00\n",
      "75%      3.000000e+00\n",
      "max      4.000000e+00\n",
      "        fiscal_year\n",
      "count  1.828155e+06\n",
      "mean   1.999527e+03\n",
      "std    1.311033e+01\n",
      "min    1.966000e+03\n",
      "25%    1.990000e+03\n",
      "50%    2.000000e+03\n",
      "75%    2.011000e+03\n",
      "max    2.021000e+03\n",
      "       reporting_frequency\n",
      "count              1828155\n",
      "unique                   2\n",
      "top                      Q\n",
      "freq               1802133\n",
      "       result_reported_date\n",
      "count               1226486\n",
      "unique                14607\n",
      "top              14/11/2001\n",
      "freq                   1511\n",
      "       result_reported_date_mod\n",
      "count                   1828155\n",
      "unique                    18772\n",
      "top                  2021-02-24\n",
      "freq                       8879\n"
     ]
    }
   ],
   "source": [
    "# Checking distributions of final columns\n",
    "for i in range(0,10):\n",
    "    print(quarterly_comp_data[final_cols_quarterly[i:i+1]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to multiply employees with 1000\n",
    "quarterly_comp_data['employees'] = quarterly_comp_data['employees']*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final write out\n",
    "quarterly_comp_data[final_cols_quarterly].to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\fundemental_data_prepared.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I ever need to add just one columns at a later stage,will use the below template\n",
    "\n",
    "#original_data_reload = pd.read_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\Quarterly compustat data.csv',\n",
    "#                                   usecols = ['datadate','cusip','tic','gvkey','fyearq','fqtr','conm'])\n",
    "#original_data_reload.columns = ['gvkey','quarter_end_date','fiscal_year','fiscal_quarter','ticker','cusip','company_name']\n",
    "#original_data_reload = original_data_reload.drop_duplicates(subset=['gvkey','fiscal_year','fiscal_quarter'], keep=\"first\")\n",
    "\n",
    "#print(quarterly_comp_data.shape)\n",
    "#quarterly_comp_data = pd.merge(quarterly_comp_data,\n",
    "#                               original_data_reload[['ticker','company_name','gvkey','fiscal_year','fiscal_quarter']],\n",
    "#                               left_on = ['gvkey','fiscal_year','fiscal_quarter'],\n",
    "#                               right_on = ['gvkey','fiscal_year','fiscal_quarter'],\n",
    "#                               how = 'left')\n",
    "#print(quarterly_comp_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Historic segment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshn\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (7,16,19,48,60,78,89) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "seg_data = pd.read_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\Historical segment data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not taking any currency variables because compustat data has already converted everything to USD (Checked few companies)\n",
    "\n",
    "# Renaming columns based on their business description\n",
    "seg_data.rename(columns = {'stype' : 'segment_type',\n",
    "                           'conm' : 'company_name',\n",
    "                           'datadate' : 'financial_end_date',\n",
    "                           'cusip': 'cusip',\n",
    "                           'tic' : 'ticker',\n",
    "                           'naics' : 'naics_company',\n",
    "                           'sic' : 'sic_company',\n",
    "                           'snms' : 'segment_name',\n",
    "                           'NAICSS1' : 'naics_segment',\n",
    "                           'SICS1' : 'sic_segment',\n",
    "                           'sales' : 'sales',\n",
    "                           'revts' : 'revenue',\n",
    "                           'srcdate': 'data_updated_date'},\n",
    "                           inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GEOSEG    1131728\n",
       "BUSSEG    1031500\n",
       "OPSEG      134811\n",
       "STSEG        4290\n",
       "Name: segment_type, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see common segment types\n",
    "seg_data['segment_type'].value_counts()\n",
    "# BUSSEG is always product segmentation\n",
    "# GEOSEG is always geographic segmentation\n",
    "# OPSEG is a catch all segment with very unique values which can't be easily classified\n",
    "# I am going to ignore OPSEG and STSEG in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307165\n",
      "GEOSEG    306839\n",
      "BUSSEG       326\n",
      "Name: segment_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(seg_data['segment_name'].isnull().sum())\n",
    "# Why are there nulls in segment names? Why would compustat create a row if they din't have segment info?\n",
    "\n",
    "print(seg_data[seg_data['segment_name'].isnull()]['segment_type'].value_counts())\n",
    "# And also when segment name is null, segment type is almost always GEOSEG\n",
    "\n",
    "# I think there's some geographic column using which I can impute these missing segment names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0    707513\n",
      "2.0    492412\n",
      "1.0        32\n",
      "Name: geotp, dtype: int64\n",
      "United States    214396\n",
      "Domestic          15434\n",
      "North America     14490\n",
      "Canada            12952\n",
      "Americas           5290\n",
      "Name: segment_name, dtype: int64\n",
      "Europe            45012\n",
      "Other             28201\n",
      "Canada            28194\n",
      "United Kingdom    19586\n",
      "United States     19242\n",
      "Name: segment_name, dtype: int64\n",
      "GERDAU SA                       93\n",
      "TIM S.A.                        63\n",
      "TELEFONICA BRASIL SA            62\n",
      "COPEL-CIA PARANAENSE ENERGIA    62\n",
      "COMPANHIA SIDERURGICA NACION    61\n",
      "Name: company_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's check out a geographic variable named geotp\n",
    "print(seg_data['geotp'].value_counts())\n",
    "\n",
    "print(seg_data[seg_data['geotp'] == 2]['segment_name'].value_counts()[0:5])\n",
    "# When geotp = 2, a lot of segment names are US\n",
    "\n",
    "print(seg_data[seg_data['geotp'] == 3]['segment_name'].value_counts()[0:5])\n",
    "# When geotp = 3, a lot of segment names are foreign countries\n",
    "\n",
    "# Discovered that 2 indicates home country for every company & 3 indicates international business\n",
    "# For example, for Brazilian ADR's, when geotp is 2, segment_name is Brazil\n",
    "print(seg_data[(seg_data['geotp'] == 2) & (seg_data['segment_name'] == 'Brazil')]['company_name'].value_counts()[0:5])\n",
    "\n",
    "# So, let's impute segment name based on geotp\n",
    "seg_data['segment_name'] = np.where((seg_data['segment_name'].isnull()) & (seg_data['geotp'] == 2),'Domestic',seg_data['segment_name'])\n",
    "seg_data['segment_name'] = np.where((seg_data['segment_name'].isnull()) & (seg_data['geotp'] == 3),'Foreign country',seg_data['segment_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-acd93b088316>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  seg_data['segment_name_mod'] =  seg_data['segment_name'].str.replace('[^a-zA-Z]', '')\n"
     ]
    }
   ],
   "source": [
    "# Cleaning segment name text column\n",
    "\n",
    "# Taking only alphabets in companies name\n",
    "seg_data['segment_name_mod'] =  seg_data['segment_name'].str.replace('[^a-zA-Z]', '')\n",
    "# Removing all white spaces (even between company name)\n",
    "seg_data['segment_name_mod'] = seg_data['segment_name_mod'].str.replace(' ', '')\n",
    "# Converting all characters to upper\n",
    "seg_data['segment_name_mod'] = seg_data['segment_name_mod'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2302329, 98)\n",
      "74807\n",
      "64697\n",
      "1    56379\n",
      "2     7115\n",
      "3      913\n",
      "Name: Num_of_unique_segment_name, dtype: int64\n",
      "Asia/Australia      116\n",
      "Asia, Australia      65\n",
      "Asia & Australia     62\n",
      "Asia,Australia       54\n",
      "Asia-Australia       18\n",
      "Name: segment_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(seg_data.shape)\n",
    "print(len(seg_data['segment_name'].unique()))\n",
    "# Let's see how many less uniques segment_name_mod has\n",
    "print(len(seg_data['segment_name_mod'].unique()))\n",
    "# Wow! almost 10,000 less uniques\n",
    "\n",
    "# Wondering what segment_name_mod maps to multiple segment_names\n",
    "segment_name_mod_mapping = pd.DataFrame(seg_data.groupby('segment_name_mod')['segment_name'].nunique()).reset_index()\n",
    "segment_name_mod_mapping.columns = ['segment_name_mod','Num_of_unique_segment_name']\n",
    "print(segment_name_mod_mapping['Num_of_unique_segment_name'].value_counts()[0:3])\n",
    "\n",
    "# Yeah there's genuine cases where companies have written the same segment slightly differently each year\n",
    "print(seg_data[seg_data['segment_name_mod'] == 'ASIAAUSTRALIA']['segment_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales       64244\n",
      "revenue    686846\n",
      "dtype: int64\n",
      "count    1.453520e+06\n",
      "mean    -1.171705e+01\n",
      "std      7.690944e+04\n",
      "min     -4.743250e+07\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      3.317477e+07\n",
      "Name: percent_diff_btw_sales_revenue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(seg_data[['sales','revenue']].isnull().sum())\n",
    "# Revenue includes few additional items beyond sales \n",
    "\n",
    "seg_data['percent_diff_btw_sales_revenue'] = ((seg_data['revenue'] - seg_data['sales'])/seg_data['sales'])*100\n",
    "seg_data['percent_diff_btw_sales_revenue'][np.isinf(seg_data['percent_diff_btw_sales_revenue'])] = None\n",
    "print(seg_data['percent_diff_btw_sales_revenue'].describe())\n",
    "# Revenue and sales are almost always equal\n",
    "\n",
    "# But revenue data exists only from 1990's\n",
    "# Hence, imputing revenue column with sales info \n",
    "seg_data['revenue'] = seg_data['revenue'].fillna(seg_data['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing level and cleaning segment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    311656\n",
       "2       951\n",
       "Name: # unique_months_for_every_gvkey_year, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing level of data\n",
    "\n",
    "# Checking if the data is yearly or quarterly \n",
    "seg_data['financial_end_date'] = pd.to_datetime(seg_data['financial_end_date']) \n",
    "seg_data['Year'] = seg_data['financial_end_date'].dt.year\n",
    "seg_data['Month'] = seg_data['financial_end_date'].dt.month\n",
    "\n",
    "seg_data['gvkey_year'] = seg_data['gvkey'].astype(str) + seg_data['Year'].astype(str)\n",
    "\n",
    "testing_yearly_or_quarterly = pd.DataFrame(seg_data.groupby('gvkey_year')['Month'].nunique()).reset_index()\n",
    "testing_yearly_or_quarterly.columns = ['gvkey_year','# unique_months_for_every_gvkey_year']\n",
    "testing_yearly_or_quarterly['# unique_months_for_every_gvkey_year'].value_counts()\n",
    "# Clearly, most companies have only row for every year\n",
    "# There's possibly few restatements/changes in financial year end\n",
    "\n",
    "# Broadly, data seems to be at gvkey, yearly, segment level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322721\n",
      "1325419\n",
      "(2302329,)\n"
     ]
    }
   ],
   "source": [
    "# Is the level of data gvkey_year,segment_type & segment_name?\n",
    "# Let's try with both date & year combinations\n",
    "\n",
    "seg_data['gvkey_year_seg_type_seg_name_mod'] = seg_data['gvkey_year'].astype(str) + seg_data['segment_name_mod'].astype(str) + seg_data['segment_type'].astype(str)\n",
    "seg_data['gvkey_date'] = seg_data['gvkey'].astype(str) + seg_data['financial_end_date'].astype(str)\n",
    "seg_data['gvkey_date_seg_type_seg_name_mod'] = seg_data['gvkey'].astype(str) + seg_data['financial_end_date'].astype(str) + seg_data['segment_name_mod'].astype(str) + seg_data['segment_type'].astype(str)\n",
    "\n",
    "print(len(seg_data['gvkey_year_seg_type_seg_name_mod'].unique()))\n",
    "print(len(seg_data['gvkey_date_seg_type_seg_name_mod'].unique()))\n",
    "print(seg_data['gvkey_year_seg_type_seg_name_mod'].shape)\n",
    "\n",
    "#Oh wow! Even the main 4 columns are not enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1222985\n",
       "2      55647\n",
       "0      37536\n",
       "3       9234\n",
       "4          8\n",
       "6          6\n",
       "5          2\n",
       "7          1\n",
       "Name: Num_unique_revenue, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's really shocking is this: it's possible for a combination of gvkey, year (or date), segment_name & segment_type to have more than 1 unique revenue \n",
    "\n",
    "testing_level_of_data = pd.DataFrame(seg_data.groupby('gvkey_date_seg_type_seg_name_mod')['revenue'].nunique()).reset_index()\n",
    "testing_level_of_data.columns = ['gvkey_date_seg_type_seg_name_mod','Num_unique_revenue']\n",
    "testing_level_of_data.sort_values(by = 'Num_unique_revenue',ascending = False)\n",
    "testing_level_of_data['Num_unique_revenue'].value_counts()\n",
    "\n",
    "# Realized this is possible because there are lot of restatements\n",
    "# So, earliest data updated date will give the 1st result that company announced for that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2279865\n",
      "1    2200117\n",
      "0      63855\n",
      "2       9785\n",
      "3       6098\n",
      "4          7\n",
      "6          2\n",
      "7          1\n",
      "Name: Num_unique_revenue, dtype: int64\n",
      "15843\n"
     ]
    }
   ],
   "source": [
    "# Let's test the level of data again by included data_updated_date\n",
    "seg_data['gvkey_date_updated_date_seg_type_seg_name_mod'] = seg_data['gvkey'].astype(str) + seg_data['financial_end_date'].astype(str) + seg_data['data_updated_date'].astype(str) + seg_data['segment_name_mod'].astype(str) + seg_data['segment_type'].astype(str)\n",
    "print(len(seg_data['gvkey_date_updated_date_seg_type_seg_name_mod'].unique()))\n",
    "# There are still nearly 100k rows which are not unique even at this level\n",
    "\n",
    "# However, atleast revenue (the only column I need) is almost unique at this level\n",
    "testing_level_of_data = pd.DataFrame(seg_data.groupby('gvkey_date_updated_date_seg_type_seg_name_mod')['revenue'].nunique()).reset_index()\n",
    "testing_level_of_data.columns = ['gvkey_date_updated_date_seg_type_seg_name_mod','Num_unique_revenue']\n",
    "testing_level_of_data.sort_values(by = 'Num_unique_revenue',ascending = False)\n",
    "print(testing_level_of_data['Num_unique_revenue'].value_counts())\n",
    "# Why are there multiple revenues for the same gvkey_date_updated_date_seg_type_seg_name_mod?\n",
    "\n",
    "testing_level_of_data[testing_level_of_data['Num_unique_revenue'] == 2][0:2]\n",
    "seg_data[seg_data['gvkey_date'] == '99881996-03-31'][['segment_type','company_name','financial_end_date','gvkey','cusip',\n",
    "                                                      'ticker','naics_company','sic_company','segment_name_mod','naics_segment',\n",
    "                                                      'sic_segment','revenue','data_updated_date','Year','Month','gvkey_date',\n",
    "                                                      'gvkey_year']]\n",
    "# The reason is some companies have multiple foreign company segments\n",
    "# These multiple foreign countries have different revenues\n",
    "# However, since geotp is 3 for all foreign rows, imputed segment name is the same \"foreign country\"\n",
    "\n",
    "# Gathering all such problematic rows\n",
    "problematic_ids = testing_level_of_data[testing_level_of_data['Num_unique_revenue'] > 1]['gvkey_date_updated_date_seg_type_seg_name_mod']\n",
    "# I am going to remove gvkey, year & segment_type of all such problematic rows\n",
    "seg_data['gvkey_year_seg_type'] = seg_data['gvkey'].astype(str) + seg_data['Year'].astype(str) + seg_data['segment_type'].astype(str)\n",
    "problamatic_gvkey_year_seg_type = seg_data[seg_data['gvkey_date_updated_date_seg_type_seg_name_mod'].isin(problematic_ids)]['gvkey_year_seg_type'].unique()\n",
    "print(len(problamatic_gvkey_year_seg_type))\n",
    "\n",
    "# Removing all problematic rows\n",
    "seg_data = seg_data[~seg_data['gvkey_year_seg_type'].isin(problamatic_gvkey_year_seg_type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2247938, 107)\n",
      "(1290791, 107)\n"
     ]
    }
   ],
   "source": [
    "# Let's correct the level of data & Select only important columns\n",
    "\n",
    "seg_data = seg_data.sort_values(by = ['gvkey','financial_end_date','data_updated_date','segment_name_mod',\n",
    "                                      'segment_type'],ascending = True)\n",
    "print(seg_data.shape)\n",
    "# For companies which have restated earnings, will take the 1st earnings declared only \n",
    "# Taking earliest data updated date for every financial_end_date\n",
    "\n",
    "seg_data = seg_data.drop_duplicates(subset = ['gvkey','Year','segment_type', 'segment_name_mod'],\n",
    "                                    keep = \"first\")\n",
    "print(seg_data.shape)\n",
    "\n",
    "segment_imp_cols = ['segment_type','company_name','financial_end_date','gvkey','cusip','ticker','naics_company','sic_company',\n",
    "                    'segment_name','segment_name_mod','naics_segment','sic_segment','revenue','Year','Month','gvkey_date','gvkey_year',\n",
    "                    'data_updated_date']\n",
    "seg_data = seg_data[segment_imp_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1290791, 18)\n",
      "(312601, 3)\n",
      "(1290791, 19)\n",
      "(1177331, 19)\n"
     ]
    }
   ],
   "source": [
    "# Need to think about cases where company has created a new segment at the time of restatment (segments change btw. original & restatement at a later date)\n",
    "# For every for gvkey, year I am going to consider only results that appeared for earliest\n",
    "\n",
    "min_updated_date_for_every_gvkey_year = seg_data.groupby(['gvkey', 'Year'])['data_updated_date'].min().reset_index()\n",
    "min_updated_date_for_every_gvkey_year.columns = ['gvkey','Year','earliest_updated_date']\n",
    "\n",
    "print(seg_data.shape)\n",
    "print(min_updated_date_for_every_gvkey_year.shape)\n",
    "seg_data = pd.merge(seg_data,\n",
    "                    min_updated_date_for_every_gvkey_year,\n",
    "                    left_on = ['gvkey','Year'],\n",
    "                    right_on = ['gvkey','Year'],\n",
    "                    how = 'left')\n",
    "print(seg_data.shape)\n",
    "\n",
    "seg_data = seg_data[seg_data['data_updated_date'] == seg_data['earliest_updated_date']]\n",
    "print(seg_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, As discussed earlier I am going to consider only busseg and geoseg segments\n",
    "seg_data['segment_type'].value_counts()\n",
    "seg_data = seg_data[(seg_data['segment_type'] == 'GEOSEG') | (seg_data['segment_type'] == 'BUSSEG')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting usful info from seg data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3 ways in which I intend to use seg data\n",
    "# 1) I can more accurately estimate rev cagr calculation if i have produt segment wise revenue\n",
    "#    Earlier, I calculated revenue cagr calculation. But 1 seg growing 10% and 2 segments growing at 0% & 20% are not the same\n",
    "#    It's always better for companies to have few high growth segments rather than multiple low growth segments\n",
    "# 2) Similarly, I can also more accuratly estimate rev cagr using country seg rev info\n",
    "# 3) I can accurately estimate a company market share by using product segment revenue\n",
    "#    For example, Let's say companyA has $100Mn rev in one segment with NAICS '111' & (contd.)\n",
    "#    companyB has $100Mn rev but in 2 segments with NAICS '111' & '123', then companyB has less market share relative to companyA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the 1st objective described above \n",
    "# Accurately estimating rev cagr taking segment wise rev into consideration\n",
    "\n",
    "# Wondering if I should attempt 1Y/5Y/3Y rev cagr \n",
    "seg_years_join_null_values = pd.DataFrame(None)\n",
    "\n",
    "for i in [1,5,3]:\n",
    "    geoseg_rev_prev_iY_df  = seg_data[seg_data['segment_type'] == 'GEOSEG'][['revenue','gvkey','Year','segment_type','segment_name_mod']]\n",
    "    geoseg_rev_prev_iY_df['Year'] = geoseg_rev_prev_iY_df['Year'] + i\n",
    "    geoseg_rev_prev_iY_df.rename(columns = {'revenue': 'revenue_prev_iY'}, inplace = True)\n",
    "    geoseg_rev_prev_iY_df['always1'] = 1\n",
    "\n",
    "    geoseg_data_prior_years_rev = pd.merge(seg_data[seg_data['segment_type'] == 'GEOSEG'],\n",
    "                                           geoseg_rev_prev_iY_df,\n",
    "                                           left_on = ['gvkey','Year','segment_type','segment_name_mod'],\n",
    "                                           right_on = ['gvkey','Year','segment_type','segment_name_mod'],\n",
    "                                           how = 'left')\n",
    "    geoseg_non_matching = geoseg_data_prior_years_rev['always1'].isnull().sum()\n",
    "    geoseg_data_prior_years_rev['gvkey_year'] = geoseg_data_prior_years_rev['gvkey'].astype(str) + geoseg_data_prior_years_rev['Year'].astype(str)\n",
    "    \n",
    "    geoseg_problematic_gvkey_years = geoseg_data_prior_years_rev[geoseg_data_prior_years_rev['always1'].isnull()]['gvkey_year']\n",
    "    geoseg_non_matching_gvkey_years = len(geoseg_data_prior_years_rev[geoseg_data_prior_years_rev['gvkey_year'].isin(geoseg_problematic_gvkey_years)])\n",
    "\n",
    "\n",
    "    busseg_rev_prev_iY_df  = seg_data[seg_data['segment_type'] == 'BUSSEG'][['revenue','gvkey','Year','segment_type','segment_name_mod']]\n",
    "    busseg_rev_prev_iY_df['Year'] = busseg_rev_prev_iY_df['Year'] + i\n",
    "    busseg_rev_prev_iY_df.rename(columns = {'revenue': 'revenue_prev_iY'}, inplace = True)\n",
    "    busseg_rev_prev_iY_df['always1'] = 1\n",
    "\n",
    "    busseg_data_prior_years_rev = pd.merge(seg_data[seg_data['segment_type'] == 'BUSSEG'],\n",
    "                                           busseg_rev_prev_iY_df,\n",
    "                                           left_on = ['gvkey','Year','segment_type','segment_name_mod'],\n",
    "                                           right_on = ['gvkey','Year','segment_type','segment_name_mod'],\n",
    "                                           how = 'left')\n",
    "    busseg_non_matching = busseg_data_prior_years_rev['always1'].isnull().sum()\n",
    "    busseg_data_prior_years_rev['gvkey_year'] = busseg_data_prior_years_rev['gvkey'].astype(str) + busseg_data_prior_years_rev['Year'].astype(str)\n",
    "    \n",
    "    busseg_problematic_gvkey_years = busseg_data_prior_years_rev[busseg_data_prior_years_rev['always1'].isnull()]['gvkey_year']\n",
    "    busseg_non_matching_gvkey_years = len(busseg_data_prior_years_rev[busseg_data_prior_years_rev['gvkey_year'].isin(busseg_problematic_gvkey_years)])\n",
    "    \n",
    "    seg_years_join_null_values_iter = pd.DataFrame([[i,geoseg_non_matching,busseg_non_matching, geoseg_non_matching_gvkey_years,\n",
    "                                                     busseg_non_matching_gvkey_years]],\n",
    "                                                   columns = ['prev_year','geoseg_non_matching','busseg_non_matching',\n",
    "                                                              'geoseg_non_matching_gvkey_years', \n",
    "                                                              'busseg_non_matching_gvkey_years'])\n",
    "    seg_years_join_null_values = pd.concat([seg_years_join_null_values,seg_years_join_null_values_iter],axis = 0)\n",
    "\n",
    "\n",
    "seg_years_join_null_values['total_non_matching'] = seg_years_join_null_values['geoseg_non_matching'] + seg_years_join_null_values['busseg_non_matching']\n",
    "seg_years_join_null_values['total_non_matching_gvkey_years'] = seg_years_join_null_values['geoseg_non_matching_gvkey_years'] + seg_years_join_null_values['busseg_non_matching_gvkey_years']\n",
    "seg_years_join_null_values['num_of_rows_in_data'] = len(seg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_year</th>\n",
       "      <th>geoseg_non_matching</th>\n",
       "      <th>busseg_non_matching</th>\n",
       "      <th>geoseg_non_matching_gvkey_years</th>\n",
       "      <th>busseg_non_matching_gvkey_years</th>\n",
       "      <th>total_non_matching</th>\n",
       "      <th>total_non_matching_gvkey_years</th>\n",
       "      <th>num_of_rows_in_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>97683</td>\n",
       "      <td>112709</td>\n",
       "      <td>150033</td>\n",
       "      <td>164530</td>\n",
       "      <td>210392</td>\n",
       "      <td>314563</td>\n",
       "      <td>1126544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>328528</td>\n",
       "      <td>359082</td>\n",
       "      <td>392838</td>\n",
       "      <td>418399</td>\n",
       "      <td>687610</td>\n",
       "      <td>811237</td>\n",
       "      <td>1126544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>233881</td>\n",
       "      <td>264073</td>\n",
       "      <td>304792</td>\n",
       "      <td>333503</td>\n",
       "      <td>497954</td>\n",
       "      <td>638295</td>\n",
       "      <td>1126544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prev_year  geoseg_non_matching  busseg_non_matching  \\\n",
       "0          1                97683               112709   \n",
       "0          5               328528               359082   \n",
       "0          3               233881               264073   \n",
       "\n",
       "   geoseg_non_matching_gvkey_years  busseg_non_matching_gvkey_years  \\\n",
       "0                           150033                           164530   \n",
       "0                           392838                           418399   \n",
       "0                           304792                           333503   \n",
       "\n",
       "   total_non_matching  total_non_matching_gvkey_years  num_of_rows_in_data  \n",
       "0              210392                          314563              1126544  \n",
       "0              687610                          811237              1126544  \n",
       "0              497954                          638295              1126544  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_years_join_null_values\n",
    "# 5Y_prev_rev has too many nulls\n",
    "#  It's always better to study revenue CAGR over long period of time (the variable will be more stable)\n",
    "# But because companies change segments very frequently, 5Y_prev_rev has too many nulls\n",
    "# So, I am going to focus primarily on 3 year segment growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating 3Y factor (revenue multiplier)\n",
    "geoseg_data_prior_years_rev['3Y_factor'] = geoseg_data_prior_years_rev['revenue']/geoseg_data_prior_years_rev['revenue_prev_iY']\n",
    "\n",
    "# capping 3y factor because in general a companies revenue cant grow by more than 3X within 3 years\n",
    "geoseg_data_prior_years_rev['3Y_factor_mod'] = np.where(geoseg_data_prior_years_rev['3Y_factor'] > 4, 4, geoseg_data_prior_years_rev['3Y_factor'])\n",
    "geoseg_data_prior_years_rev['3Y_factor_mod'] = np.where(geoseg_data_prior_years_rev['3Y_factor'] < 0, None, geoseg_data_prior_years_rev['3Y_factor_mod'])\n",
    "geoseg_data_prior_years_rev['3Y_factor_mod'] = pd.to_numeric(geoseg_data_prior_years_rev['3Y_factor_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251803, 2)\n",
      "(189557, 2)\n",
      "(189557, 3)\n"
     ]
    }
   ],
   "source": [
    "# Just because a new segment appears (relative to 3Y back), I don't want to exclude factor calculation for that entire gvkey_year\n",
    "# First, I want to look at %revenue (within that gvkey_year) for which I don't have 3Y factor calculated\n",
    "\n",
    "geoseg_gvkey_year_rev_sum_3Y_factor_null = pd.DataFrame(geoseg_data_prior_years_rev[geoseg_data_prior_years_rev['3Y_factor_mod'].isnull()].groupby('gvkey_year')['revenue'].sum()).reset_index()\n",
    "geoseg_gvkey_year_rev_sum_3Y_factor_null.columns = ['gvkey_year','sum_rev_3Y_factor_null']\n",
    "\n",
    "geoseg_gvkey_year_rev_sum = pd.DataFrame(geoseg_data_prior_years_rev.groupby('gvkey_year')['revenue'].sum()).reset_index()\n",
    "geoseg_gvkey_year_rev_sum.columns = ['gvkey_year','sum_rev']\n",
    "\n",
    "print(geoseg_gvkey_year_rev_sum.shape)\n",
    "print(geoseg_gvkey_year_rev_sum_3Y_factor_null.shape)\n",
    "rev_sum_3Y_factor = pd.merge(geoseg_gvkey_year_rev_sum_3Y_factor_null,\n",
    "                             geoseg_gvkey_year_rev_sum,\n",
    "                             left_on = ['gvkey_year'],\n",
    "                             right_on = ['gvkey_year'],\n",
    "                             how = 'inner')\n",
    "print(rev_sum_3Y_factor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96527\n",
      "251803\n"
     ]
    }
   ],
   "source": [
    "rev_sum_3Y_factor['prcnt_rev_without_3Y_factor'] = (rev_sum_3Y_factor['sum_rev_3Y_factor_null'] / rev_sum_3Y_factor['sum_rev'])*100\n",
    "\n",
    "rev_sum_3Y_factor['prcnt_rev_without_3Y_factor'].describe()\n",
    "# Clearly, there's lot of gvkey_years where segments that don't have 3Y factors have sizable revenue\n",
    "# How is it possible that 3Y factor revenue sum / total rev sum > 100?\n",
    "\n",
    "rev_sum_3Y_factor['prcnt_rev_without_3Y_factor'].quantile(0.99)\n",
    "# There are very few cases where some segments have negative revenue\n",
    "# So, total rev sum < 3Y factor rev sum\n",
    "\n",
    "geoseg_problematic_gvkey_years = rev_sum_3Y_factor[rev_sum_3Y_factor['prcnt_rev_without_3Y_factor'] > 15]['gvkey_year']\n",
    "\n",
    "print(len(geoseg_problematic_gvkey_years))\n",
    "print(len(geoseg_data_prior_years_rev['gvkey_year'].unique()))\n",
    "# As expected 40 - 50% of gvkey_years are problematic from a geosegment perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a gvkey year has been found in the problamatic list, excluding 3Y factor for such gvkey years\n",
    "geoseg_data_prior_years_rev['3Y_factor_mod'] = np.where(geoseg_data_prior_years_rev['gvkey_year'].isin(geoseg_problematic_gvkey_years),\n",
    "                                                    None, geoseg_data_prior_years_rev['3Y_factor_mod'])\n",
    "\n",
    "# Calculating future segment revenue based on past 3Y rev growth factors\n",
    "geoseg_data_prior_years_rev['3Y_factor_mod'] = pd.to_numeric(geoseg_data_prior_years_rev['3Y_factor_mod'])\n",
    "geoseg_data_prior_years_rev['segment_future_3Y_rev_mod_est'] = geoseg_data_prior_years_rev['revenue'] * geoseg_data_prior_years_rev['3Y_factor_mod']\n",
    "\n",
    "geoseg_future_rev_sum = pd.DataFrame(geoseg_data_prior_years_rev.groupby('gvkey_year')['segment_future_3Y_rev_mod_est'].sum()).reset_index()\n",
    "geoseg_future_rev_sum.columns = ['gvkey_year','future_3Y_rev']\n",
    "\n",
    "geoseg_cur_rev_excluding_3Y_factor_null_seg = pd.DataFrame(geoseg_data_prior_years_rev[~geoseg_data_prior_years_rev['3Y_factor'].isnull()].groupby('gvkey_year')['revenue'].sum()).reset_index()\n",
    "geoseg_cur_rev_excluding_3Y_factor_null_seg.columns = ['gvkey_year','sum_rev_3Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161907, 2)\n",
      "(251803, 2)\n",
      "(149307, 3)\n"
     ]
    }
   ],
   "source": [
    "print(geoseg_cur_rev_excluding_3Y_factor_null_seg.shape)\n",
    "print(geoseg_future_rev_sum.shape)\n",
    "geoseg_current_future_rev_sum = pd.merge(geoseg_cur_rev_excluding_3Y_factor_null_seg[~geoseg_cur_rev_excluding_3Y_factor_null_seg['gvkey_year'].isin(geoseg_problematic_gvkey_years)],\n",
    "                                         geoseg_future_rev_sum,\n",
    "                                         left_on = 'gvkey_year',\n",
    "                                         right_on = 'gvkey_year',\n",
    "                                         how = 'inner')\n",
    "print(geoseg_current_future_rev_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing to join seg data into quarterly data\n",
    "quarterly_comp_data['gvkey_fiscal_year_mod'] = quarterly_comp_data['gvkey'].astype(str) + quarterly_comp_data['fiscal_year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Q1, Q2, Q3 I need to merge last year annual characteristics (2018Q2 should be merged with 2017 annual)\n",
    "# For Q4, I need to merge current year annual characteristics (2018Q4 should be merged with 2018 annual)\n",
    "\n",
    "#quarterly_comp_data['fiscal_year_mod'] = np.where(quarterly_comp_data['fiscal_quarter'] == 4.0, quarterly_comp_data['fiscal_year'], quarterly_comp_data['fiscal_year']-1)\n",
    "\n",
    "#print(quarterly_comp_data.shape)\n",
    "#print(geoseg_current_future_rev_sum.shape)\n",
    "#quarterly_comp_data = pd.merge(quarterly_comp_data,\n",
    "#                               geoseg_current_future_rev_sum,\n",
    "#                               left_on = 'gvkey_fiscal_year_mod',\n",
    "#                               right_on = 'gvkey_year',\n",
    "#                               how = 'left')\n",
    "#print(quarterly_comp_data.shape)\n",
    "\n",
    "# Never join seg data to quarterly comp data\n",
    "# The reason is \n",
    "# 1) First od all, I was able to compute 'future_3Y_rev' information for only 40% of data (So, able to join only 40%)\n",
    "# 2) On top of that, computing 3Y CAGR is very hard if there's an acquisition \n",
    "#    I need to know in which segment acquisiton happened along with whether acquisition happened or not\n",
    "#    Earlier, if I had a certain acquisition_sales_amt, I just subtracted that amt. from revenue\n",
    "#    But, for segment 3Y segment CAGR calculation, if any acquisition has happened, I need make calculation 'None'\n",
    "# Overall, I will be able to get 3Y segment CAGR for barely 30% of data\n",
    "\n",
    "# So, never completed the first objective of estimating a segment based CAGR\n",
    "\n",
    "# Similarly, not going to complete the 2nd objective with segment data either (Not worth my time for 30% fill rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the 3rd objective, I am planning to merge seg data into price data (monthly/daily)\n",
    "# Lets create a copy of seg data frame by selecting only useful columns\n",
    "seg_data_cut = seg_data[seg_data['segment_type'] == 'BUSSEG'][['Year','gvkey','segment_name','naics_segment','sic_segment',\n",
    "                                                              'revenue','financial_end_date','cusip']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_data_numbers\n",
      "count    547798.000000\n",
      "mean       1998.309156\n",
      "std          11.985352\n",
      "min        1976.000000\n",
      "25%        1988.000000\n",
      "50%        1999.000000\n",
      "75%        2008.000000\n",
      "max        2020.000000\n",
      "Name: Year, dtype: float64\n",
      "\n",
      "\n",
      "sic_segment_numbers\n",
      "46031\n",
      "count    46031.000000\n",
      "mean      2005.332189\n",
      "std         10.022137\n",
      "min       1976.000000\n",
      "25%       2001.000000\n",
      "50%       2006.000000\n",
      "75%       2012.000000\n",
      "max       2020.000000\n",
      "Name: Year, dtype: float64\n",
      "Corporate                     12712\n",
      "Eliminations                   7561\n",
      "Other                          3976\n",
      "Corporate and Other            2213\n",
      "OTHER                          2024\n",
      "                              ...  \n",
      "STATIONERY-CANDY                  1\n",
      "TRUCK COMPONENTS-FASTENERS        1\n",
      "OIL-COAL AND GAS                  1\n",
      "F&A                               1\n",
      "DIVESTED ACTIVITIES               1\n",
      "Name: segment_name, Length: 2488, dtype: int64\n",
      "\n",
      "\n",
      "naics_segment_numbers\n",
      "193840\n",
      "count    193840.000000\n",
      "mean       1988.262082\n",
      "std          11.229453\n",
      "min        1976.000000\n",
      "25%        1980.000000\n",
      "50%        1985.000000\n",
      "75%        1989.000000\n",
      "max        2020.000000\n",
      "Name: Year, dtype: float64\n",
      "Corporate                       12712\n",
      "Eliminations                     7560\n",
      "OTHER                            6245\n",
      "Other                            3976\n",
      "Corporate and Other              2213\n",
      "                                ...  \n",
      "COPPR/ALUM STEEL ROD-WIRE PD        1\n",
      "BEER DISTRIBUTORSHIP                1\n",
      "VIDEO TAPE DISTRIBUTION             1\n",
      "Unallocated Corporate Office        1\n",
      "CONSTRUCTION RELATED                1\n",
      "Name: segment_name, Length: 18328, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Testing whether it's better to use sic segment or naics segment \n",
    "print('overall_data_numbers')\n",
    "print(seg_data_cut['Year'].describe())\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"sic_segment_numbers\")\n",
    "print(seg_data_cut['sic_segment'].isnull().sum())\n",
    "print(seg_data_cut[seg_data_cut['sic_segment'].isnull()]['Year'].describe())\n",
    "print(seg_data_cut[seg_data_cut['sic_segment'].isnull()]['segment_name'].value_counts())\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"naics_segment_numbers\")\n",
    "print(seg_data_cut['naics_segment'].isnull().sum())\n",
    "print(seg_data_cut[seg_data_cut['naics_segment'].isnull()]['Year'].describe())\n",
    "print(seg_data_cut[seg_data_cut['naics_segment'].isnull()]['segment_name'].value_counts())\n",
    "\n",
    "# Realized sic segment is some times null when seg name is weired (corp/others/elimination)\n",
    "# When naics seg is null it is because seg name is weired / data is before 1997 (govt. introduced naics in 1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(547798, 8)\n",
      "(547798, 10)\n"
     ]
    }
   ],
   "source": [
    "# Before I write out final segment data, I need to add result date into seg_data_cut\n",
    "print(seg_data_cut.shape)\n",
    "\n",
    "seg_data_cut = pd.merge(seg_data_cut,\n",
    "                        quarterly_comp_data[quarterly_comp_data['fiscal_quarter'] == 4][['gvkey','fiscal_year','result_reported_date_mod']],\n",
    "                        left_on = ['gvkey','Year'],\n",
    "                        right_on = ['gvkey','fiscal_year'],\n",
    "                        how = 'left')\n",
    "print(seg_data_cut.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_data_cut.to_csv('final_segment_data.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
