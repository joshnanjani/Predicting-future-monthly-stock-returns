{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from os import path\n",
    "from functools import reduce\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic understanding of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading monthly price information from 1967 to 2020\n",
    "# CUSIP number (one of the most important columns to join fundamental & price data) was introduced in 1967 only\n",
    "\n",
    "monthly_stock_data = pd.read_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\Monthly stock data.csv')\n",
    "monthly_stock_data['date'] = pd.to_datetime(monthly_stock_data['date'])\n",
    "monthly_stock_data['Year'] = monthly_stock_data['date'].dt.year\n",
    "monthly_stock_data['Month'] = monthly_stock_data['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4200298, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are 64 columns in the data\n",
    "monthly_stock_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33534"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(monthly_stock_data['CUSIP'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am not going to impute. First need to study each column\n",
    "# monthly_stock_data = monthly_stock_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving data of unique values into data frame\n",
    "Num_unique_eachcol = pd.DataFrame(monthly_stock_data.apply(lambda x: x.nunique()))\n",
    "Num_unique_eachcol = Num_unique_eachcol.reset_index()\n",
    "#Num_unique_eachcol.columns = ['Column_name', 'Num_unique_values']\n",
    "Num_unique_eachcol.rename(columns = {'index':'Column_name', 0: 'Num_unique_values'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating fill rate\n",
    "null_df = pd.DataFrame(monthly_stock_data.isnull().sum()).reset_index()\n",
    "null_df.rename(columns = {'index':'Column_name', 0:'Number_of_Null_values'}, inplace = True)\n",
    "null_df['Fill_rate_percentage'] = ((94524 - null_df['Number_of_Null_values'])/94524)*100\n",
    "# Just an alternate way of creating fill rate column\n",
    "#for i in df2.index:\n",
    "#    df2['Fill_rate_percentage'][i] = ((94524 - df2['Number_of_Null_values'][i])/94524)*100\n",
    "\n",
    "# Creating a fill rate category\n",
    "#if else statement for creating a new column condition based.np.where() is easier to use \n",
    "#null_df['Fill_rate_category'] = None0\n",
    "#for i in null_df.index:\n",
    "    #if null_df['Fill_rate_percentage'][i]<10:\n",
    "        #null_df['Fill_rate_category'][i]='less_than_10'\n",
    "    #else:\n",
    "        #null_df['Fill_rate_category'][i]='btw_10_80'\n",
    "    #if null_df['Fill_rate_percentage'][i] > 80:\n",
    "        #null_df['Fill_rate_category'][i]='grt_than_80'\n",
    "    #else:\n",
    "        #null_df['Fill_rate_category']\n",
    "\n",
    "#creating a new column using np.where(). Edditing the same code as above using np.where()\n",
    "null_df['Fill_rate_category'] = np.where(null_df['Fill_rate_percentage']>10, 'btw_10_80', 'less_than_10')\n",
    "null_df['Fill_rate_category'] = np.where(null_df['Fill_rate_percentage']>80, 'grt_than_80', null_df['Fill_rate_category'])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging columns, inner join\n",
    "merge_data = pd.merge(Num_unique_eachcol,\n",
    "                      null_df[['Column_name','Fill_rate_percentage','Fill_rate_category']],\n",
    "                      left_on = 'Column_name',\n",
    "                      right_on = 'Column_name',\n",
    "                      how = 'inner')\n",
    "\n",
    "# A new csv file has been created by unique_values & fillrate.\n",
    "merge_data.to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\unique_values_fillrate.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200298, 21)\n",
      "(4200298, 22)\n"
     ]
    }
   ],
   "source": [
    "# Joining share code (type of stock/index/foreign company) into main data\n",
    "share_code_data = pd.read_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\share code description.csv')\n",
    "\n",
    "print(monthly_stock_data.shape)\n",
    "monthly_stock_data = pd.merge(monthly_stock_data,\n",
    "                              share_code_data,\n",
    "                              left_on = 'SHRCD',\n",
    "                              right_on = 'SHRCD',\n",
    "                              how = 'left')\n",
    "print(monthly_stock_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30089\n",
      "30089\n"
     ]
    }
   ],
   "source": [
    "print(monthly_stock_data['SHRCD_Description'].isnull().sum())\n",
    "# Realized that SHRCD_Description has few nulls\n",
    "\n",
    "# Based on testing few CUSIP's, figured out SHRCD_Description is null for every first row of CUSIP\n",
    "# If a stock lists on Nov 2019, noticed Nov row SHRCD_Description is blank. \n",
    "monthly_stock_data = monthly_stock_data.sort_values(by=['date','CUSIP'],ascending = True)\n",
    "monthly_stock_data['CUSIP_shift'] = monthly_stock_data['CUSIP'].shift(1)\n",
    "print(monthly_stock_data[((monthly_stock_data['CUSIP'] != monthly_stock_data['CUSIP_shift']) &\n",
    "                          (monthly_stock_data['SHRCD_Description'].isnull()))]['SHRCD_Description'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal Stock           3226976\n",
       "Foreign Companies       346737\n",
       "ETF                     247507\n",
       "Close end funds         225669\n",
       "REIT                     95782\n",
       "Open end funds           39151\n",
       "Trust                    14237\n",
       "Foreign Trust             3175\n",
       "Foreign mutual fund       1064\n",
       "Name: SHRCD_Description, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, I am back filling SHRCD_Description. The data is already in order\n",
    "# monthly_stock_data['SHRCD_Description'] = monthly_stock_data['SHRCD_Description'].bfill()\n",
    "# But I need to ensure the above code works correct under all circumstances\n",
    "# As I realised if any company left market in a month then the 1st month data of the company will be null and it will be filled by next company SHRCD_Description\n",
    "# So, to avoid such errors I decided to groupby SHRCD_Description with CUSIP and bfill the null values\n",
    "monthly_stock_data['SHRCD_Description'] = monthly_stock_data.groupby('CUSIP')['SHRCD_Description'].transform(lambda v: v.bfill())\n",
    "# Let's check distribution of SHRCD_Description now\n",
    "monthly_stock_data['SHRCD_Description'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    33173\n",
       "2      347\n",
       "3       14\n",
       "Name: # Unique SHRCD_Descriptions, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_stock_data = monthly_stock_data.sort_values(by=['date','CUSIP'],ascending = True)\n",
    "cusip_shrcd_mapping = pd.DataFrame(monthly_stock_data.groupby('CUSIP')['SHRCD_Description'].nunique()).reset_index()\n",
    "cusip_shrcd_mapping.columns = ['CUSIP', '# Unique SHRCD_Descriptions']\n",
    "cusip_shrcd_mapping.sort_values(by = ['# Unique SHRCD_Descriptions'], ascending = False)\n",
    "cusip_shrcd_mapping['# Unique SHRCD_Descriptions'].value_counts()\n",
    "# Almost all companies have only 1 SHRCD description. There are few that were one REIT/Trust and later transformed to Normal stock\n",
    "# Let's just ignore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrcd_to_consider = ['Normal Stock','Foreign Companies','Probably_Normal_stock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate M.Cap of each stock\n",
    "monthly_stock_data['Mcap'] = monthly_stock_data['PRC'].abs()*monthly_stock_data['SHROUT']*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out level of data, primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4200298"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figuring out level of data\n",
    "# monthly_stock_data['PERMNO_str'] = df['PERMNO'].astype(str) - No need to create a new column\n",
    "monthly_stock_data['possible_level_of_data'] = monthly_stock_data['PERMNO'].astype(str) + monthly_stock_data['date'].astype(str)\n",
    "len(monthly_stock_data['possible_level_of_data'].unique()) #unfortunately permno and date are not matching so i am creating a new dataframe\n",
    "\n",
    "#important tips for variable naming\n",
    "#A variable name should be as small as possible\n",
    "#only 1st letter should be capital\n",
    "#the most imp characteristics of a variable name should be within the 1st 7 to 8 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column_name</th>\n",
       "      <th>unique_values_combined_date</th>\n",
       "      <th>rows_in_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCUSIP</td>\n",
       "      <td>4142382</td>\n",
       "      <td>4200298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKER</td>\n",
       "      <td>4075832</td>\n",
       "      <td>4200298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PERMCO</td>\n",
       "      <td>3930704</td>\n",
       "      <td>4200298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUSIP</td>\n",
       "      <td>4200298</td>\n",
       "      <td>4200298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PERMNO</td>\n",
       "      <td>4200298</td>\n",
       "      <td>4200298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Column_name  unique_values_combined_date  rows_in_data\n",
       "0      NCUSIP                      4142382       4200298\n",
       "0      TICKER                      4075832       4200298\n",
       "0      PERMCO                      3930704       4200298\n",
       "0       CUSIP                      4200298       4200298\n",
       "0      PERMNO                      4200298       4200298"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Writting a for loop for the best 5 possibilities to identify as level of data. running iteration for a null data frame and adding new rows accordingly.\n",
    "level_of_data_exploration = pd.DataFrame(None) # better to have a null df, It will be easy to use during iterations.\n",
    "for i in ['NCUSIP','TICKER','PERMCO','CUSIP','PERMNO']: # i should be used wherever the change of data is required.\n",
    "    monthly_stock_data['possible_level_of_data'] = monthly_stock_data[i].astype(str) + monthly_stock_data['date'].astype(str)\n",
    "    x1 = i\n",
    "    x2 = len(monthly_stock_data['possible_level_of_data'].unique())\n",
    "    x3 = len(monthly_stock_data)\n",
    "#    print(x1)\n",
    "#    print(x2)\n",
    "#    print(x3)\n",
    "    level_of_data_exploration_iter = pd.DataFrame([[x1,x2,x3]], columns=['Column_name', 'unique_values_combined_date', 'rows_in_data'])\n",
    "    level_of_data_exploration = pd.concat([level_of_data_exploration, level_of_data_exploration_iter], axis = 0)\n",
    "    \n",
    "# 3 things to remember in a for loop\n",
    "# 1) Using iterator (i) in the right places\n",
    "# 2) Use print statements to debug\n",
    "# 3) Sometimes, i may need to initialize a series or dataframe before the for loop\n",
    "\n",
    "level_of_data_exploration\n",
    "# The data is unique at both PERMNO, date & CUSIP, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33534\n",
      "46267\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSIP</th>\n",
       "      <th>Num_unique_NCUSIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>68235H30</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33529</th>\n",
       "      <td>Y8897Y18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15368</th>\n",
       "      <td>45857P80</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18528</th>\n",
       "      <td>55357010</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24209</th>\n",
       "      <td>74347W14</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CUSIP  Num_unique_NCUSIP\n",
       "21998  68235H30                 13\n",
       "33529  Y8897Y18                 12\n",
       "15368  45857P80                 10\n",
       "18528  55357010                 10\n",
       "24209  74347W14                 10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(monthly_stock_data['CUSIP'].unique()))\n",
    "print(len(monthly_stock_data['NCUSIP'].unique()))\n",
    "\n",
    "cusip_ncusip_map = pd.DataFrame(monthly_stock_data.groupby('CUSIP')['NCUSIP'].nunique()).reset_index()\n",
    "cusip_ncusip_map.columns = ['CUSIP', 'Num_unique_NCUSIP']\n",
    "cusip_ncusip_map = cusip_ncusip_map.sort_values(by=['Num_unique_NCUSIP'],ascending = False)\n",
    "cusip_ncusip_map[0:5]\n",
    "# monthly_stock_data[monthly_stock_data['CUSIP'] == '74347W14'].to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\cusip_data.csv')\n",
    "# Realised that NCUSIP can change with time. CUSIP is actually the most recent NCUSIP for every stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating 6 digit CUSIP. \n",
    "# Ideally, CRSP data should be unique at CUSIP_6digit & date\n",
    "# The first 6 digits of a CUSIP uniquely identify a company\n",
    "# The last 2 digits of CUSIP indicate Share class or some name change, ticker change, restructuring\n",
    "\n",
    "monthly_stock_data['cusip_6digit'] = monthly_stock_data['CUSIP'].str[0:6]\n",
    "\n",
    "# I am also gonna create a columns called cusip_8digit that's literally the original CUSIP\n",
    "monthly_stock_data['cusip_8digit'] = monthly_stock_data['CUSIP']\n",
    "# CRSP actaully has 8 digit CUSIP by default (unlike compusat which provides 9 digit cusip by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, before I work with CUSIP_6 digit, let's understand if there's instances of 1 CUSIP_6digit mapping to multiple CUSIP_8digits\n",
    "\n",
    "# A Cusip_6digit can actaully map to multiple cusip_8digits indeed\n",
    "# Expected reasons are\n",
    "# 1) Index funds - because components of an index change continuously, Indices get multiple CUSIP's\n",
    "# Solution: not ideal, but analysis is not focused on index funds\n",
    "# 2) Some companies have multiple share classes. ClassA and ClassB shares will have same CUSIP_6digit but different CUSIP_8digit\n",
    "# Solution: need to pick just one class share for every CUSIP_6digit. Have to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    163783\n",
       "B     44410\n",
       "C      1507\n",
       "D       908\n",
       "L       766\n",
       "H       689\n",
       "E       481\n",
       "N       277\n",
       "V       260\n",
       "1       136\n",
       "P       119\n",
       "U        92\n",
       "G        52\n",
       "Z        34\n",
       "T        10\n",
       "Name: SHRCLS, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data indicates there's actually quite a few companies with multiple classes\n",
    "monthly_stock_data['SHRCLS'].value_counts()\n",
    "# Need to account for this if I want to use cusip_6digit & date as level of data\n",
    "# Moreover, need to make sure one class can appear only once in data. \n",
    "# Can't have 3 rows for Google just because google happens to have classA, classB, classC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    26703\n",
      "2      266\n",
      "3        4\n",
      "4        1\n",
      "5        1\n",
      "Name: Num_unique_cusip_8digit, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip_6digit</th>\n",
       "      <th>Num_unique_cusip_8digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>003881</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19407</th>\n",
       "      <td>747906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13578</th>\n",
       "      <td>500472</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16530</th>\n",
       "      <td>640671</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>002896</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cusip_6digit  Num_unique_cusip_8digit\n",
       "270         003881                        3\n",
       "19407       747906                        3\n",
       "13578       500472                        3\n",
       "16530       640671                        2\n",
       "237         002896                        2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if there's any other reasons for multiple mapping (apart from the 2 reasons already mentioned)\n",
    "\n",
    "cusip_shrcls_unique = pd.DataFrame(monthly_stock_data.groupby('cusip_6digit')['SHRCLS'].nunique()).reset_index()\n",
    "cusip_shrcls_unique.columns = ['cusip_6digit' , 'Num_unique_shrcls']\n",
    "cusip_shrcls_unique['Num_unique_shrcls'].value_counts()\n",
    "# Getting CUSIP_6digits of all the people with more than 1 share class\n",
    "cusip_6digit_multiple_shrcls = cusip_shrcls_unique[cusip_shrcls_unique['Num_unique_shrcls'] > 1]['cusip_6digit']\n",
    "\n",
    "# Let's exclude indices & stocks with more than 1 shareclass (known 2 reasons)\n",
    "comp_btw_cusip6_cusip8_multiple_values = pd.DataFrame(monthly_stock_data[(monthly_stock_data['SHRCD_Description'].isin(shrcd_to_consider)) & \n",
    "                                                                        (~monthly_stock_data['cusip_6digit'].isin(cusip_6digit_multiple_shrcls))]\n",
    "                                                      .groupby('cusip_6digit')['cusip_8digit'].nunique()).reset_index()\n",
    "comp_btw_cusip6_cusip8_multiple_values.columns = ['cusip_6digit', 'Num_unique_cusip_8digit']\n",
    "comp_btw_cusip6_cusip8_multiple_values = comp_btw_cusip6_cusip8_multiple_values.sort_values(by=['Num_unique_cusip_8digit'],\n",
    "                                                                                            ascending = False)\n",
    "print(comp_btw_cusip6_cusip8_multiple_values['Num_unique_cusip_8digit'].value_counts())\n",
    "# Realized that when a cusip_6digit is a normal stock & has just 1 unique share class associated with it, (continued below)\n",
    "# there's 1-1 mapping in most of cases between CUSIP (6 digit) & CUSIP (8 digit)\n",
    "\n",
    "# For investigation of some of the weird cases, let's pick few companies that are still active in 2020\n",
    "cusip_6digit_2020_active = monthly_stock_data[monthly_stock_data['Year'] == 2020]['cusip_6digit']\n",
    "comp_btw_cusip6_cusip8_multiple_values[comp_btw_cusip6_cusip8_multiple_values['cusip_6digit'].isin(cusip_6digit_2020_active)][0:5]\n",
    "\n",
    "# Upon further investigation (code below), last 2 digits of CUSIP can also change during 2 additional scenarios\n",
    "# 3) Restructuring. If a company went through restructruing & emerged, I think CUSIP bureau automatically assigns different CUSIP number with same first 6 digits\n",
    "# Solution: Don't need to think of this a major problem. See comments below\n",
    "# 4) There are some companies like Google that change name to Alphabet (and manually apply for a new CUSIP)\n",
    "# Solution: Don't need to think of this a major problem. See comments below\n",
    "\n",
    "# Realized both (3) and (4) are weird folks who manually applied for a new CUSIP after a name, ticker change\n",
    "# Don't have to worry about them\n",
    "# If a company used CUSIP g5433m34 from 1980-2000 & suddenly changed to g5433m67 (same first 6 digits), it's still same company\n",
    "# Just because google changed last 2 digits of cusip after becoming alphabet, doesn't mean I can't calculate last_5Y % change\n",
    "\n",
    "# To conclude, there are 4 reasons why 1 CUSIP_6digit can map to multiple CUSIP_8digits\n",
    "# But there's only type of instance that I need to specifically correct for: a company having multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.330000e+04\n",
      "mean              inf\n",
      "std               NaN\n",
      "min      0.000000e+00\n",
      "25%      5.241846e-01\n",
      "50%      3.572992e+00\n",
      "75%      4.605866e+01\n",
      "max               inf\n",
      "Name: volA_div_volB, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# In cases when both classA & classB are present, let's see which class generally has better volumes\n",
    "\n",
    "monthly_stock_data['volume_price'] = monthly_stock_data['VOL'] * abs(monthly_stock_data['PRC'])\n",
    "monthly_stock_data['cusip_6digit_date'] = monthly_stock_data['cusip_6digit'].astype(str) + monthly_stock_data['date'].astype(str)\n",
    "\n",
    "cusip_6digit_shrA_vol = monthly_stock_data[monthly_stock_data['SHRCLS'] == 'A'][['cusip_6digit','date','cusip_6digit_date', \n",
    "                                                                                 'volume_price']]\n",
    "cusip_6digit_shrA_vol.rename(columns = {'volume_price' : 'volume_price_shrA'},inplace = True)\n",
    "\n",
    "cusip_6digit_shrB_vol = monthly_stock_data[monthly_stock_data['SHRCLS'] == 'B'][['cusip_6digit','date','cusip_6digit_date','volume_price']]\n",
    "cusip_6digit_shrB_vol.rename(columns = {'volume_price' : 'volume_price_shrB'},inplace = True)\n",
    "\n",
    "#print(cusip_6digit_shrA_vol.shape)\n",
    "#print(cusip_6digit_shrB_vol.shape)\n",
    "cusip_6digit_shrAB_vol = pd.merge(cusip_6digit_shrA_vol[['cusip_6digit','date','volume_price_shrA']],\n",
    "                                  cusip_6digit_shrB_vol[['cusip_6digit','date','volume_price_shrB']],\n",
    "                                  left_on = ['cusip_6digit','date'],\n",
    "                                  right_on = ['cusip_6digit','date'],\n",
    "                                  how = 'inner')\n",
    "#print(cusip_6digit_shrAB_vol.shape)\n",
    "\n",
    "cusip_6digit_shrAB_vol['volA_div_volB'] = cusip_6digit_shrAB_vol['volume_price_shrA']/cusip_6digit_shrAB_vol['volume_price_shrB']\n",
    "print(cusip_6digit_shrAB_vol['volA_div_volB'].describe())\n",
    "# Realized that when both classA & classB are available, classA dollar volume is generally larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    44410\n",
       "A    23744\n",
       "C      647\n",
       "L       86\n",
       "Name: SHRCLS, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that there are quite a few stocks where only classB is present in a monthly row (no class A)\n",
    "# Can't just delete all rows with SHRCD != A\n",
    "monthly_stock_data[monthly_stock_data['cusip_6digit_date'].isin(cusip_6digit_shrB_vol['cusip_6digit_date'])]['SHRCLS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    1728\n",
       "B     183\n",
       "D       8\n",
       "Name: SHRCLS, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_shrcls_that_appears = monthly_stock_data[~monthly_stock_data['SHRCLS'].isnull()][['cusip_6digit', \n",
    "                            'date', 'SHRCLS']].sort_values(by = ['cusip_6digit', 'date', \n",
    "                                                                 'SHRCLS'], \n",
    "                                                           ascending = True).drop_duplicates(subset = ['cusip_6digit'])\n",
    "first_shrcls_that_appears['SHRCLS'].value_counts()[0:3]\n",
    "# Realized that 'A' is often the first class that appears for every stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4535.000000\n",
      "mean             inf\n",
      "std              NaN\n",
      "min         0.000000\n",
      "25%         1.219236\n",
      "50%         8.470588\n",
      "75%        58.974251\n",
      "max              inf\n",
      "Name: volA_div_volB, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Interestingly, noticed that even when B is the first listed stock, volume of A is larger\n",
    "print(cusip_6digit_shrAB_vol[cusip_6digit_shrAB_vol['cusip_6digit'].isin(first_shrcls_that_appears\n",
    "                                                                         [first_shrcls_that_appears['SHRCLS'] == 'B']\n",
    "                                                                         ['cusip_6digit'])]['volA_div_volB'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       29.000000\n",
       "mean       927.862069\n",
       "std       2158.298151\n",
       "min         28.000000\n",
       "25%         31.000000\n",
       "50%         60.000000\n",
       "75%        700.000000\n",
       "max      10836.000000\n",
       "Name: gap_btw_shrclsA_shrclsB, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When a company has listed classB first & classA later, let's see how late classA appears on market\n",
    "\n",
    "first_date_of_shrclsA = monthly_stock_data[monthly_stock_data['SHRCLS'] == 'A'][['cusip_6digit','date']].sort_values(by = ['cusip_6digit','date'],\n",
    "                                                                    ascending = True).drop_duplicates(subset = ['cusip_6digit'])\n",
    "first_date_of_shrclsA.columns = ['cusip_6digit','first_date_of_shrclsA']\n",
    "\n",
    "first_date_of_shrclsB = monthly_stock_data[monthly_stock_data['SHRCLS'] == 'B'][['cusip_6digit','date']].sort_values(by = ['cusip_6digit','date'],\n",
    "                                                                    ascending = True).drop_duplicates(subset = ['cusip_6digit'])\n",
    "first_date_of_shrclsB.columns = ['cusip_6digit','first_date_of_shrclsB']\n",
    "\n",
    "first_date_of_shrclsAB = pd.merge(first_date_of_shrclsA,\n",
    "                                  first_date_of_shrclsB,\n",
    "                                  left_on = ['cusip_6digit'],\n",
    "                                  right_on = ['cusip_6digit'],\n",
    "                                  how = 'inner')\n",
    "\n",
    "first_date_of_shrclsAB['gap_btw_shrclsA_shrclsB'] = pd.to_datetime(first_date_of_shrclsAB['first_date_of_shrclsA']) - pd.to_datetime(first_date_of_shrclsAB['first_date_of_shrclsB'])\n",
    "first_date_of_shrclsAB['gap_btw_shrclsA_shrclsB'] = first_date_of_shrclsAB['gap_btw_shrclsA_shrclsB'].dt.days\n",
    "\n",
    "first_date_of_shrclsAB[first_date_of_shrclsAB['cusip_6digit'].isin(first_shrcls_that_appears\n",
    "                                                                   [first_shrcls_that_appears['SHRCLS'] == 'B']['cusip_6digit'])]['gap_btw_shrclsA_shrclsB'].describe()\n",
    "# Very interesting to note that even when classB is first listed, in almost half cases classA stock gets listed in 60 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing our research on different classes\n",
    "# 1) when both stockA & stockB are listed, volume of stockA is higher\n",
    "# 2) StockA also is the class that often appears first on market\n",
    "# 3) Even for companies where stockB appears first, volume of classA (which is listed later) is larger\n",
    "# 4) Further, in cases where classB is listed first, classA often appears very soon\n",
    "\n",
    "# Based on the learnings above, if a company (CUSIP_6digit) has classA in data, let's just take only classA rows \n",
    "# (irrespective of whether the company also has classB/classC and listed them before/after etc.)\n",
    "\n",
    "# While the above insights are obtained focusing on classA & classB only, let's apply the same logic to other classes like C,D also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200298, 29)\n",
      "(4200298, 30)\n",
      "(4200298, 30)\n",
      "(4172195, 30)\n"
     ]
    }
   ],
   "source": [
    "best_shrclas_every_cusip6digit = monthly_stock_data[~monthly_stock_data['SHRCLS'].\n",
    "                                                    isnull()][['cusip_6digit', \n",
    "                                                               'SHRCLS']].sort_values(by = ['cusip_6digit', 'SHRCLS'], \n",
    "                                                                                      ascending = True).drop_duplicates(subset = ['cusip_6digit'])\n",
    "# Getting first alphabet share class for every cusip 6digit\n",
    "best_shrclas_every_cusip6digit.columns = ['cusip_6digit','first_alphabet_shrcls']\n",
    "\n",
    "\n",
    "print(monthly_stock_data.shape)\n",
    "monthly_stock_data = pd.merge(monthly_stock_data,\n",
    "                              best_shrclas_every_cusip6digit,\n",
    "                              left_on = ['cusip_6digit'],\n",
    "                              right_on = ['cusip_6digit'],\n",
    "                              how = 'left')\n",
    "print(monthly_stock_data.shape)\n",
    "\n",
    "\n",
    "print(monthly_stock_data.shape)\n",
    "# Subsetting only for rows where shareclass is matching with first alphabet shareclass\n",
    "monthly_stock_data = monthly_stock_data[(monthly_stock_data['SHRCLS'].isnull()) |\n",
    "                                        (monthly_stock_data['SHRCLS'] == monthly_stock_data['first_alphabet_shrcls'])]\n",
    "print(monthly_stock_data.shape)\n",
    "# Finally, figured out how to deal with stocks that have multiple classes (same CUSIP_6digit appearing twice in a month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3753645\n",
      "1     184976\n",
      "Name: SHRCLS, dtype: int64\n",
      "5312292020-08-31    3\n",
      "5312292017-01-31    3\n",
      "87924V1998-09-30    3\n",
      "5312292019-11-29    3\n",
      "87924V1999-01-29    3\n",
      "                   ..\n",
      "8788952006-09-29    1\n",
      "7059042005-04-29    1\n",
      "92645B2020-08-31    1\n",
      "4835481977-03-31    1\n",
      "3131482006-04-28    1\n",
      "Name: cusip_6digit_date, Length: 163338, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cusip_6digit</th>\n",
       "      <th>cusip_8digit</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>COMNAM</th>\n",
       "      <th>SHRCLS</th>\n",
       "      <th>RET</th>\n",
       "      <th>Mcap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3902241</th>\n",
       "      <td>2017-09-29</td>\n",
       "      <td>G5480U</td>\n",
       "      <td>G5480U10</td>\n",
       "      <td>LBTYA</td>\n",
       "      <td>LIBERTY GLOBAL PLC</td>\n",
       "      <td>A</td>\n",
       "      <td>-0.002647</td>\n",
       "      <td>7.524426e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902244</th>\n",
       "      <td>2017-09-29</td>\n",
       "      <td>G5480U</td>\n",
       "      <td>G5480U13</td>\n",
       "      <td>LILA</td>\n",
       "      <td>LIBERTY GLOBAL PLC</td>\n",
       "      <td>A</td>\n",
       "      <td>-0.086505</td>\n",
       "      <td>1.149936e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date cusip_6digit cusip_8digit TICKER              COMNAM  \\\n",
       "3902241 2017-09-29       G5480U     G5480U10  LBTYA  LIBERTY GLOBAL PLC   \n",
       "3902244 2017-09-29       G5480U     G5480U13   LILA  LIBERTY GLOBAL PLC   \n",
       "\n",
       "        SHRCLS        RET          Mcap  \n",
       "3902241      A  -0.002647  7.524426e+09  \n",
       "3902244      A  -0.086505  1.149936e+09  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(monthly_stock_data.groupby('cusip_6digit_date')['SHRCLS'].nunique().value_counts())\n",
    "# Based on our effort so far, 1 cusip_6digit & date can have only 1 shareclass\n",
    "\n",
    "# But realized there are even weirder companies that have 3 share class 'A' in the same month\n",
    "print(monthly_stock_data[monthly_stock_data['SHRCLS'] == 'A']['cusip_6digit_date'].value_counts())\n",
    "\n",
    "# Investigated them and realized they are just 'weird'!\n",
    "imp_cols_for_investigation = ['date','cusip_6digit','cusip_8digit','TICKER','COMNAM','SHRCLS','RET','Mcap']\n",
    "monthly_stock_data[monthly_stock_data['cusip_6digit_date'] == 'G5480U2017-09-29'][imp_cols_for_investigation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    3527764\n",
      "2       8880\n",
      "3        145\n",
      "4         20\n",
      "5          2\n",
      "6          1\n",
      "Name: Num_rows, dtype: int64\n",
      "286\n",
      "(4172195, 30)\n",
      "(3938621, 30)\n"
     ]
    }
   ],
   "source": [
    "# But, is it possible for CUSIP (6digit) to appear twice in same month for normal stocks that don't have any SHRCLS filled in?\n",
    "\n",
    "# when share class is not available at all, there are very few instances of one CUSIP_6digit having 2 rows in the same month\n",
    "# There are 3 possible reasons for this\n",
    "# 1) There are some 'great' companies like liberty media which have 3 classA stocks within a month\n",
    "# 2) CRSP didn't properly SHRCLS info for such rows. The last 2 digits are different because that stock had classA, classB in same month\n",
    "# 3) Some folks changed their name (& obtained different CUSIP with different last 2 digits) in the middle of month\n",
    "#    So their CUSIP_6digit appears twice in the same month\n",
    "\n",
    "same_cusip_date_but_multiplerows = pd.DataFrame(monthly_stock_data[monthly_stock_data['SHRCD_Description'].isin(shrcd_to_consider)]['cusip_6digit_date'].value_counts()).reset_index()\n",
    "same_cusip_date_but_multiplerows.columns = ['cusip_6digit_date','Num_rows']\n",
    "print(same_cusip_date_but_multiplerows['Num_rows'].value_counts())\n",
    "# In the below value counts noticed there are nearly 9000 rows where data is not unique at cusip_6digit_date level\n",
    "print(len(same_cusip_date_but_multiplerows[same_cusip_date_but_multiplerows['Num_rows'] > 1]['cusip_6digit_date'].astype(str).str[0:6].unique()))\n",
    "# Fortunately, this problem is only for 286 companies\n",
    "\n",
    "# Let's just sort the data for cusip_6digit, cusip_8digit, date and drop duplicates \n",
    "print(monthly_stock_data.shape)\n",
    "monthly_stock_data = monthly_stock_data.sort_values(by = ['cusip_6digit', 'cusip_8digit', \n",
    "                                                          'date']).drop_duplicates(subset = ['cusip_6digit', 'date'])\n",
    "print(monthly_stock_data.shape)\n",
    "# Removed nearly 200k rows from overall data. Almost 190k rows have been removed because of index funds\n",
    "# For normal stocks removed only 10k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3867329\n",
       "0      71292\n",
       "Name: Num_unique_RET, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if one unique combination of Cusip_6digit & date has only one unique Ret or not\n",
    "# using nunique\n",
    "\n",
    "num_unique_ret_for_every_cusip6date = pd.DataFrame(monthly_stock_data.groupby('cusip_6digit_date')['RET'].nunique()).reset_index()\n",
    "num_unique_ret_for_every_cusip6date.columns = ['cusip_6digit_date', 'Num_unique_RET']\n",
    "num_unique_ret_for_every_cusip6date['Num_unique_RET'].value_counts()\n",
    "# One cusip_6digit_date has only  1 unique RET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     40008\n",
      "2       188\n",
      "3         5\n",
      "5         4\n",
      "4         2\n",
      "8         2\n",
      "9         1\n",
      "12        1\n",
      "Name: cusip_6digit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The data is finally unique at CUSIP_6digit & date. \n",
    "# Assuming that if 2 rows have different CUSIP_6 digits, they are totally different companies\n",
    "# But saw a lot of companies where 2 different CUSIP's are actually the same company\n",
    "# But wondering if it's possible for company to have multiple CUSIP_6digits?\n",
    "\n",
    "print(monthly_stock_data.groupby('COMNAM')['cusip_6digit'].nunique().value_counts())\n",
    "# Looks like for every company name, there is a unique CUSIP_6digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    22482\n",
      "2     4624\n",
      "3     1436\n",
      "4      408\n",
      "5      106\n",
      "6       33\n",
      "7        9\n",
      "9        1\n",
      "Name: cusip_6digit, dtype: int64\n",
      "40212\n",
      "29100\n"
     ]
    }
   ],
   "source": [
    "# But what about ticker?\n",
    "print(monthly_stock_data.groupby('TICKER')['cusip_6digit'].nunique().value_counts())\n",
    "# Wow. One particular ticker had 9 different CUSIP_6digits associated with it\n",
    "\n",
    "# I understand that different companies can use the same ticker across time. But 9 different companies using the same ticker?\n",
    "# That must be one hell of a ticker\n",
    "\n",
    "print(len(monthly_stock_data['COMNAM'].unique()))\n",
    "print(len(monthly_stock_data['TICKER'].unique()))\n",
    "# Noticed that there are only 29,100 unique ticker's in the entire data\n",
    "# The number of unique values in ticker is significantly lesser than number of unique values in CUSIP, PERMNO, etc.\n",
    "# May be I can just assume any 2 cusip_6digits with same ticker as the same company?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMNAM             \n",
       "GREYHOUND CORP         280\n",
       "GILLETTE CO            170\n",
       "GENPACT LTD            161\n",
       "GREYHOUND DIAL CORP     12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But turns out sometimes, 2 Cusip_6digits associated with same ticker can be very different companies\n",
    "\n",
    "monthly_stock_data[monthly_stock_data['TICKER'] == 'G'][['COMNAM']].value_counts()\n",
    "# Clearly, I can't just rely on ticker to identify each company uniquely\n",
    "# The trick to identify the cases tickers like 'GM' where 2 Cusip_6 digits are legit the same General Motors\n",
    "# While avoiding false positives like 'G' where multiple Cusip_6 digits that are associated with it are different companies\n",
    "\n",
    "# This is really important because I intend to calculate prior 1Y return, 5Y return etc. columns for each company later\n",
    "# If code doesn't recognize the fact that 2 CUSIP_6digits belong to the same company, then I can't calculate those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think compustat data could be very relevant here\n",
    "\n",
    "# Already know from documentation that the CUSIP column in both CRSP & Compustat databases is the most recent CUSIP for every stock\n",
    "# But discovered that the phrase 'most recent' has very different meaning in both the data sources\n",
    "# In Compustat, the CUSIP is truly ther final CUSIP that appears for every stock\n",
    "# For example, if GM went bankrupt in 2009 & came back in 2010, compustat has put the 2021 GM CUSIP for all GM rows since 1967\n",
    "# In sharp contrast, CRSP takes the fact that a major corporate event (bankruptcy/restructuring/merger) took place in 2009\n",
    "# In CRSP's world, the 'old' GM stopped trading in 2009. The GM that came back in 2010 is another new company\n",
    "# There's countless cases like this: General motors, Kmart, Abercrombie etc.\n",
    "# Summary: The CUSIP in CRSP not truly the real most recent CUSIP. It is the most recent CUSIP of the companies current shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the difference (explained above) block between the 2 databases could be really helpful in figuring out interlinked cusips\n",
    "\n",
    "# For example, let's say GM had cusip x from 1967 - 2009, cusip y from 2010-2020\n",
    "# Just by using monthly stock data, there is no way to know that x & y belong to GM\n",
    "# Once again: Not every cusip associated with same ticker are the same company\n",
    "# But in compustat data, all GM rows from 1967 onwards have cusip y\n",
    "# So, by joining both data sets on something like ticker, date, month, I can probably map x to y (GMJan1980 mapped to GMJan1980)\n",
    "# Once I have mapped all such interlinked cusips, I will be able to identify that x and y are indeed interlinked cusips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading prepared fundamental data\n",
    "comp_data = pd.read_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\fundemental_data_prepared.csv')\n",
    "comp_data['cusip_6digit'] = comp_data['cusip'].astype(str).str[0:6]\n",
    "comp_data['cusip_8digit'] = comp_data['cusip'].astype(str).str[0:8]\n",
    "\n",
    "comp_data.rename(columns = {'naics':'NAICS'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3938621, 30)\n",
      "(438619, 30)\n",
      "(35142, 30)\n",
      "(149654, 30)\n"
     ]
    }
   ],
   "source": [
    "# Discovered it won't be easy to join CRSP & Compustat data\n",
    "\n",
    "print(monthly_stock_data.shape)\n",
    "print(monthly_stock_data[~monthly_stock_data['cusip_6digit'].isin(comp_data['cusip_6digit'])].shape)\n",
    "# Lot of cusip_6digits on CRSP side which just don't appear anywhere in Compustat side\n",
    "\n",
    "# Noticed that there are few cases where cusip_6digit match is not found but there is a company name match\n",
    "print(monthly_stock_data[((monthly_stock_data['COMNAM'].isin(comp_data['company_name'])) &\n",
    "                   (~monthly_stock_data['cusip_6digit'].isin(comp_data['cusip_6digit'])))].shape)\n",
    "\n",
    "# There's a ton of cases where cusip_6digit match is not found but there is a ticker match\n",
    "print(monthly_stock_data[((monthly_stock_data['TICKER'].isin(comp_data['ticker'])) &\n",
    "                   (~monthly_stock_data['cusip_6digit'].isin(comp_data['cusip_6digit'])))].shape)\n",
    "# Ofcourse, some of these ticker matches are going to completely different companies\n",
    "# But, there's going to be 2 types of legit cases\n",
    "# 1) Cases like GM where CUSIP x will not be found in Compustat but is actually linked to y (that's there in CRSP too)\n",
    "# 2) realized  in some cases CUSIP a is actually CUSIP b on the compustat side (continued below)\n",
    "# But if all rows in CRSP belonging are already coded a, then I guess I am not gonna find any interlinks btw. a & some other CUSIP\n",
    "# However, the fact that a & b are linked is gonna help me when joining CRSP & Compustat (eventually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cusip_6digit</th>\n",
       "      <th>cusip_8digit</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>COMNAM</th>\n",
       "      <th>SHRCLS</th>\n",
       "      <th>RET</th>\n",
       "      <th>Mcap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181888</th>\n",
       "      <td>1973-01-31</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.047766</td>\n",
       "      <td>2.208755e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187727</th>\n",
       "      <td>1973-02-28</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.037540</td>\n",
       "      <td>2.101534e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193567</th>\n",
       "      <td>1973-03-30</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.017007</td>\n",
       "      <td>2.065794e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199393</th>\n",
       "      <td>1973-04-30</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.012111</td>\n",
       "      <td>2.040775e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216683</th>\n",
       "      <td>1973-07-31</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>1.933554e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date cusip_6digit cusip_8digit TICKER               COMNAM  \\\n",
       "181888 1973-01-31       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "187727 1973-02-28       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "193567 1973-03-30       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "199393 1973-04-30       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "216683 1973-07-31       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "\n",
       "       SHRCLS        RET          Mcap  \n",
       "181888    NaN  -0.047766  2.208755e+10  \n",
       "187727    NaN  -0.037540  2.101534e+10  \n",
       "193567    NaN  -0.017007  2.065794e+10  \n",
       "199393    NaN  -0.012111  2.040775e+10  \n",
       "216683    NaN   0.016917  1.933554e+10  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's investigate few big companies that don't have a match on the compustat side\n",
    "\n",
    "monthly_stock_data[(monthly_stock_data['Year'] == 1973) & \n",
    "                   (~monthly_stock_data['cusip_6digit'].isin(comp_data\n",
    "                  ['cusip_6digit']))][imp_cols_for_investigation].sort_values(by = ['Mcap'],ascending = False)[0:5]\n",
    "\n",
    "# GM is not being matched? wow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cusip_6digit</th>\n",
       "      <th>cusip_8digit</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>COMNAM</th>\n",
       "      <th>SHRCLS</th>\n",
       "      <th>RET</th>\n",
       "      <th>Mcap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>1967-01-31</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.140417</td>\n",
       "      <td>2.146163e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>1967-02-28</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.025291</td>\n",
       "      <td>2.067602e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5286</th>\n",
       "      <td>1967-03-31</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.053541</td>\n",
       "      <td>2.178302e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7510</th>\n",
       "      <td>1967-04-28</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.134426</td>\n",
       "      <td>2.471123e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>1967-05-31</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.079769</td>\n",
       "      <td>2.249722e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176438</th>\n",
       "      <td>2009-02-27</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.252492</td>\n",
       "      <td>1.373542e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3183305</th>\n",
       "      <td>2009-03-31</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.137778</td>\n",
       "      <td>1.184374e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3190134</th>\n",
       "      <td>2009-04-30</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.172164e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196925</th>\n",
       "      <td>2009-05-29</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.609375</td>\n",
       "      <td>4.579215e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203710</th>\n",
       "      <td>2009-06-30</td>\n",
       "      <td>370442</td>\n",
       "      <td>37044210</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CORP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date cusip_6digit cusip_8digit TICKER               COMNAM  \\\n",
       "850     1967-01-31       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "3067    1967-02-28       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "5286    1967-03-31       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "7510    1967-04-28       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "9736    1967-05-31       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "...            ...          ...          ...    ...                  ...   \n",
       "3176438 2009-02-27       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "3183305 2009-03-31       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "3190134 2009-04-30       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "3196925 2009-05-29       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "3203710 2009-06-30       370442     37044210     GM  GENERAL MOTORS CORP   \n",
       "\n",
       "        SHRCLS        RET          Mcap  \n",
       "850        NaN   0.140417  2.146163e+10  \n",
       "3067       NaN  -0.025291  2.067602e+10  \n",
       "5286       NaN   0.053541  2.178302e+10  \n",
       "7510       NaN   0.134426  2.471123e+10  \n",
       "9736       NaN  -0.079769  2.249722e+10  \n",
       "...        ...        ...           ...  \n",
       "3176438    NaN  -0.252492  1.373542e+09  \n",
       "3183305    NaN  -0.137778  1.184374e+09  \n",
       "3190134    NaN  -0.010309  1.172164e+09  \n",
       "3196925    NaN  -0.609375  4.579215e+08  \n",
       "3203710    NaN        NaN           NaN  \n",
       "\n",
       "[510 rows x 8 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_stock_data[monthly_stock_data['cusip_6digit'] == '370442'][imp_cols_for_investigation]\n",
    "# Last month of GM trading is June 2009? That doesn't make sense. GM is currently listed on stock market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>quarter_end_date</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236107</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>1967-03-31</td>\n",
       "      <td>4854.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236108</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>1967-06-30</td>\n",
       "      <td>5561.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236109</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>1967-09-30</td>\n",
       "      <td>3776.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236110</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>1967-12-31</td>\n",
       "      <td>5834.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236111</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>1968-03-31</td>\n",
       "      <td>5370.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236318</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>30826.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236319</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>32709.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236320</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>16778.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236321</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>35480.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236322</th>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>37518.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             company_name quarter_end_date    revenue\n",
       "236107  GENERAL MOTORS CO       1967-03-31   4854.297\n",
       "236108  GENERAL MOTORS CO       1967-06-30   5561.500\n",
       "236109  GENERAL MOTORS CO       1967-09-30   3776.400\n",
       "236110  GENERAL MOTORS CO       1967-12-31   5834.098\n",
       "236111  GENERAL MOTORS CO       1968-03-31   5370.500\n",
       "...                   ...              ...        ...\n",
       "236318  GENERAL MOTORS CO       2019-12-31  30826.000\n",
       "236319  GENERAL MOTORS CO       2020-03-31  32709.000\n",
       "236320  GENERAL MOTORS CO       2020-06-30  16778.000\n",
       "236321  GENERAL MOTORS CO       2020-09-30  35480.000\n",
       "236322  GENERAL MOTORS CO       2020-12-31  37518.000\n",
       "\n",
       "[216 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interestingly, GM does appear in compustat data. But GM has a very different CUSIP number\n",
    "# Found that GM's real current CUSIP number is actually 37045V10 by googling\n",
    "\n",
    "comp_data[comp_data['cusip_8digit'] == '37045V10'][['company_name', 'quarter_end_date', 'revenue']]\n",
    "# Verified this cusip is actually GM by tallying reveune numbers\n",
    "# In Compustat data, GM appeasr as one continuous stock that has been listed on stock market since 1967\n",
    "# So, what is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cusip_6digit</th>\n",
       "      <th>cusip_8digit</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>COMNAM</th>\n",
       "      <th>SHRCLS</th>\n",
       "      <th>RET</th>\n",
       "      <th>Mcap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3311647</th>\n",
       "      <td>2010-10-29</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3318415</th>\n",
       "      <td>2010-11-30</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>5.130000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325192</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>5.793286e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331970</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.010038</td>\n",
       "      <td>5.735133e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338747</th>\n",
       "      <td>2011-02-28</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.081118</td>\n",
       "      <td>5.269910e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163355</th>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.190438</td>\n",
       "      <td>4.240340e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171197</th>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001350</td>\n",
       "      <td>4.234616e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179068</th>\n",
       "      <td>2020-10-30</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166948</td>\n",
       "      <td>4.941578e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187025</th>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.269621</td>\n",
       "      <td>6.274854e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195035</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>37045V</td>\n",
       "      <td>37045V10</td>\n",
       "      <td>GM</td>\n",
       "      <td>GENERAL MOTORS CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.050182</td>\n",
       "      <td>5.959967e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date cusip_6digit cusip_8digit TICKER             COMNAM SHRCLS  \\\n",
       "3311647 2010-10-29       37045V     37045V10    NaN                NaN    NaN   \n",
       "3318415 2010-11-30       37045V     37045V10     GM  GENERAL MOTORS CO    NaN   \n",
       "3325192 2010-12-31       37045V     37045V10     GM  GENERAL MOTORS CO    NaN   \n",
       "3331970 2011-01-31       37045V     37045V10     GM  GENERAL MOTORS CO    NaN   \n",
       "3338747 2011-02-28       37045V     37045V10     GM  GENERAL MOTORS CO    NaN   \n",
       "...            ...          ...          ...    ...                ...    ...   \n",
       "4163355 2020-08-31       37045V     37045V10     GM  GENERAL MOTORS CO    NaN   \n",
       "4171197 2020-09-30       37045V     37045V10     GM  GENERAL MOTORS CO    NaN   \n",
       "4179068 2020-10-30       37045V     37045V10     GM  GENERAL MOTORS CO    NaN   \n",
       "4187025 2020-11-30       37045V     37045V10     GM  GENERAL MOTORS CO    NaN   \n",
       "4195035 2020-12-31       37045V     37045V10     GM  GENERAL MOTORS CO    NaN   \n",
       "\n",
       "               RET          Mcap  \n",
       "3311647        NaN           NaN  \n",
       "3318415          C  5.130000e+10  \n",
       "3325192   0.077778  5.793286e+10  \n",
       "3331970  -0.010038  5.735133e+10  \n",
       "3338747  -0.081118  5.269910e+10  \n",
       "...            ...           ...  \n",
       "4163355   0.190438  4.240340e+10  \n",
       "4171197  -0.001350  4.234616e+10  \n",
       "4179068   0.166948  4.941578e+10  \n",
       "4187025   0.269621  6.274854e+10  \n",
       "4195035  -0.050182  5.959967e+10  \n",
       "\n",
       "[123 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As a matter of fact, I can see few GM rows even in CRSP data for the CUSIP: 37045V10\n",
    "monthly_stock_data[monthly_stock_data['CUSIP'] == '37045V10'][imp_cols_for_investigation]\n",
    "# The earliest month this CUSIP appears is Oct 2010\n",
    "# Why are GM rows under 2 different tickers in CRSP data? One CUSIP appears from 1967 to 2009 & there's another CUSIP from 2010\n",
    "\n",
    "# This GM case perfectly illustrates the difference between CRSP & Compustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1828155, 64)\n",
      "1755431\n",
      "1828155\n"
     ]
    }
   ],
   "source": [
    "# Checking level of fundamental data\n",
    "# Fundamental data is nearly unique at cusip_6digit, qtr & yr \n",
    "\n",
    "print(comp_data.shape)\n",
    "comp_data['cusip_6digit_date'] = comp_data['cusip_6digit'].astype(str) + comp_data['fiscal_quarter'].astype(str) + comp_data['fiscal_year'].astype(str)\n",
    "print(len(comp_data['cusip_6digit_date'].unique()))\n",
    "# Fundamental data is perfectly unique at cusip_8digit, qtr & yr \n",
    "comp_data['cusip_8digit_date'] = comp_data['cusip_8digit'].astype(str) + comp_data['fiscal_quarter'].astype(str) + comp_data['fiscal_year'].astype(str)\n",
    "print(len(comp_data['cusip_8digit_date'].unique()))\n",
    "\n",
    "comp_data['Month'] = pd.to_datetime(comp_data['quarter_end_date']).dt.month\n",
    "comp_data['Year'] = pd.to_datetime(comp_data['quarter_end_date']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLIANCEBERNSTEIN HOLDING LP    136\n",
      "Name: company_name, dtype: int64\n",
      "AMBAC INDUSTRIES INC    45\n",
      "Name: company_name, dtype: int64\n",
      ".1    164704\n",
      ".2     57659\n",
      ".3     17066\n",
      ".Z      8354\n",
      ".A      5055\n",
      "Name: ticker, dtype: int64\n",
      "ALLIANCEBERNSTEIN HOLDING LP    136\n",
      "ASBESTOS CORP LTD               120\n",
      "ALEX BROWN INC                   50\n",
      "AMBAC INDUSTRIES INC             45\n",
      "ABI AMERICAN BUSINESSPHONE       20\n",
      "Name: company_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Before exploring the use of ticker, there's something unique about TICKER column in compustat\n",
    "# If the same ticker (Ex: 'AB') has been used by 4 different companies, Compustat has set tickers to AB, AB.1, AB.2, AB.3\n",
    "\n",
    "# There's only one company with ticker 'AB' in compustat data\n",
    "print(comp_data[comp_data['ticker']== 'AB']['company_name'].value_counts())\n",
    "# Ticker of AMBAC Industries which also used to use the ticker AB, was coded as AB.1\n",
    "print(comp_data[comp_data['ticker']== 'AB.1']['company_name'].value_counts())\n",
    "\n",
    "# Need to correct cases like this before exploring ticker join. There's lot of such cases\n",
    "print(comp_data[comp_data['ticker'].astype(str).str[-2:-1] == '.']['ticker'].astype(str).str[-2:].value_counts()[0:5])\n",
    "\n",
    "# Creating a new ticker_mod column that will be using to explore join\n",
    "#comp_data['ticker_mod'] = np.where(comp_data['ticker'].astype(str).str[-2:-1] == '.', \n",
    "#                                   comp_data['ticker'].astype(str).str[: -2], comp_data['ticker'])\n",
    "\n",
    "# Rather than just removing '.' if it occurs in last but one digit, let's remove any characters that occur after a '.'\n",
    "comp_data['ticker_mod'] = comp_data['ticker'].str.split('.').str[0]\n",
    "comp_data['ticker_mod'] = comp_data['ticker_mod'].str.upper()\n",
    "\n",
    "# Now all the companies that have used 'AB' across histroy are coded as 'AB'\n",
    "print(comp_data[comp_data['ticker_mod'] == 'AB']['company_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new ticker_mod column in CRSP that will use to explore join\n",
    "monthly_stock_data['ticker_mod'] = monthly_stock_data['TICKER'].str.split('.').str[0]\n",
    "monthly_stock_data['ticker_mod'] = monthly_stock_data['ticker_mod'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1755079\n",
      "2      34098\n",
      "3       1124\n",
      "4         34\n",
      "Name: # rows, dtype: int64\n",
      "1       3832744\n",
      "2          2497\n",
      "173           8\n",
      "223           8\n",
      "43            8\n",
      "         ...   \n",
      "34            1\n",
      "197           1\n",
      "35            1\n",
      "38            1\n",
      "3041          1\n",
      "Name: # rows, Length: 230, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Wondering, if it's possible for 1 ticker to appear twice in each quarter\n",
    "comp_data['ticker_qtr'] = comp_data['ticker_mod'].astype(str) + comp_data['Year'].astype(str) + comp_data['Month'].astype(str)\n",
    "comp_ticker_qtr_repeat = pd.DataFrame(comp_data.groupby('ticker_qtr')['cusip_6digit'].nunique()).reset_index()\n",
    "comp_ticker_qtr_repeat.columns = ['ticker_qtr', '# rows']\n",
    "print(comp_ticker_qtr_repeat['# rows'].value_counts())\n",
    "comp_ticker_qtr_problematic = comp_ticker_qtr_repeat[comp_ticker_qtr_repeat['# rows'] > 1]['ticker_qtr']\n",
    "\n",
    "# Creating similar ticker_qtr column in CRSP data too\n",
    "monthly_stock_data['ticker_qtr'] = monthly_stock_data['ticker_mod'].astype(str) + monthly_stock_data['Year'].astype(str) + monthly_stock_data['Month'].astype(str)\n",
    "crsp_ticker_qtr_repeat = pd.DataFrame(monthly_stock_data.groupby('ticker_qtr')['cusip_6digit'].nunique()).reset_index()\n",
    "crsp_ticker_qtr_repeat.columns = ['ticker_qtr', '# rows']\n",
    "print(crsp_ticker_qtr_repeat['# rows'].value_counts())\n",
    "crsp_ticker_qtr_problematic = crsp_ticker_qtr_repeat[crsp_ticker_qtr_repeat['# rows'] > 1]['ticker_qtr']\n",
    "\n",
    "# Compiling a full list of problematic ticker qtr combinations\n",
    "problematic_ticker_qtr = list(comp_ticker_qtr_problematic) + list(crsp_ticker_qtr_problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Planning to use ticker on both sides to figure out additional matches\n",
    "# The idea is that if 2 tickers are in the same period of time, the matches are same company\n",
    "finding_interlinked_cusips = pd.merge(monthly_stock_data[~monthly_stock_data['NAICS'].isnull()][\n",
    "                                      ['Month','Year','NAICS','cusip_6digit','ticker_mod','COMNAM','Mcap']],\n",
    "                                      comp_data[~comp_data['ticker_qtr'].isin(problematic_ticker_qtr)][\n",
    "                                          ['Month','Year','NAICS','gvkey','cusip_6digit','ticker_mod','company_name','Mcap']],\n",
    "                                      left_on = ['Month','Year','ticker_mod','NAICS'],\n",
    "                                      right_on = ['Month','Year','ticker_mod','NAICS'],\n",
    "                                      how = 'inner')\n",
    "# Why I am using NAICS code too in this join? (along with ticker's)\n",
    "# For example, found that ABI american business phones (telecom stock) used to have a ticker AB. This company got acquired 1987\n",
    "# AllianceBernstien (finance company) adopted the ticker 'AB' in 2000 after acquiring Bernstien\n",
    "# Alliance used to be listed under the name Alliance capital with 'AC' originally in 1980's\n",
    "# I think using NAICS code in this join ensures that a lot of mismatches are weeded out\n",
    "# It's extremely unlikely 2 different companies with same NAICS code used same ticker in the same period of time\n",
    "# Ofcourse, NAICS codes in monthly data appear from 1997 only (when the government introduced them)\n",
    "# So, this join won't work for rows before 1997\n",
    "# I actually intend to do a SIC join later\n",
    "\n",
    "# Getting all the CUSIP's that are potentially interlinked with date, ticker, siccd matching\n",
    "tickers_potentially_linked_with_naics_date = finding_interlinked_cusips[((finding_interlinked_cusips['cusip_6digit_x']  != finding_interlinked_cusips['cusip_6digit_y']) &\n",
    "                                                                       (~finding_interlinked_cusips['ticker_mod'].isnull()) &\n",
    "                                                                       (~finding_interlinked_cusips['NAICS'].isnull()) & \n",
    "                                                                       (~finding_interlinked_cusips['Month'].isnull()) &\n",
    "                                                                       (~finding_interlinked_cusips['Year'].isnull()))]\n",
    "\n",
    "len(tickers_potentially_linked_with_naics_date['cusip_6digit_x'].unique())\n",
    "# Wow, I found 100's of CUSIP's that are potentially linked with each other. That's something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This time I will use ticker & sic code rather than ticker & naics\n",
    "monthly_stock_data['SICCD'] = pd.to_numeric(monthly_stock_data['SICCD'] , errors = 'coerce').astype('float64')\n",
    "\n",
    "finding_interlinked_cusips = pd.merge(monthly_stock_data[['Month','Year','SICCD','cusip_6digit','ticker_mod','COMNAM','Mcap']],\n",
    "                                      comp_data[~comp_data['ticker_qtr'].isin(problematic_ticker_qtr)][\n",
    "                                          ['Month','Year','sic_code','gvkey','cusip_6digit','ticker_mod','company_name','Mcap']],\n",
    "                                      left_on = ['Month','Year','ticker_mod','SICCD'],\n",
    "                                      right_on = ['Month','Year','ticker_mod','sic_code'],\n",
    "                                      how = 'inner')\n",
    "\n",
    "# Getting all the CUSIP's that are potentially interlinked with date, ticker, siccd matching\n",
    "tickers_potentially_linked_with_sic_date = finding_interlinked_cusips[((finding_interlinked_cusips['cusip_6digit_x']  != finding_interlinked_cusips['cusip_6digit_y']) &\n",
    "                                                                       (~finding_interlinked_cusips['ticker_mod'].isnull()) &\n",
    "                                                                       (~finding_interlinked_cusips['SICCD'].isnull()) & \n",
    "                                                                       (~finding_interlinked_cusips['Month'].isnull()) &\n",
    "                                                                       (~finding_interlinked_cusips['Year'].isnull()))]\n",
    "\n",
    "len(tickers_potentially_linked_with_sic_date['cusip_6digit_x'].unique())\n",
    "# Wow, I found 100's of CUSIP's that are potentially linked with each other. That's something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if I remove date from join? Exploring join on ticker & sector\n",
    "# Trying to find matches on just ticker & sic code (I can try NAICS code next)\n",
    "\n",
    "finding_interlinked_cusips = pd.merge(monthly_stock_data[['Month','Year','SICCD','cusip_6digit','ticker_mod','COMNAM','Mcap']],\n",
    "                                      comp_data[~comp_data['ticker_qtr'].isin(problematic_ticker_qtr)][\n",
    "                                          ['Month','Year','sic_code','gvkey','cusip_6digit','ticker_mod','company_name','Mcap']],\n",
    "                                      left_on = ['ticker_mod','SICCD'],\n",
    "                                      right_on = ['ticker_mod','sic_code'],\n",
    "                                      how = 'inner')\n",
    "\n",
    "# Getting all the CUSIP's that are potentially interlinked with ticker, siccd matching\n",
    "tickers_potentially_linked_with_sic = finding_interlinked_cusips[((finding_interlinked_cusips['cusip_6digit_x']  != finding_interlinked_cusips['cusip_6digit_y']) &\n",
    "                                                                  (~finding_interlinked_cusips['ticker_mod'].isnull()) &\n",
    "                                                                  (~finding_interlinked_cusips['SICCD'].isnull()))]\n",
    "len(tickers_potentially_linked_with_sic['cusip_6digit_x'].unique())\n",
    "# Interestingly, found lot of additional matches when I don't match on date & year particuarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month_x</th>\n",
       "      <th>Year_x</th>\n",
       "      <th>SICCD</th>\n",
       "      <th>cusip_6digit_x</th>\n",
       "      <th>ticker_mod</th>\n",
       "      <th>COMNAM</th>\n",
       "      <th>Mcap_x</th>\n",
       "      <th>Month_y</th>\n",
       "      <th>Year_y</th>\n",
       "      <th>sic_code</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>cusip_6digit_y</th>\n",
       "      <th>company_name</th>\n",
       "      <th>Mcap_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11059</th>\n",
       "      <td>8</td>\n",
       "      <td>2005</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>00081T</td>\n",
       "      <td>ABD</td>\n",
       "      <td>A C C O BRANDS CORP</td>\n",
       "      <td>1.368606e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>1972</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>3939</td>\n",
       "      <td>253034</td>\n",
       "      <td>DICK (A.B.) CO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11060</th>\n",
       "      <td>8</td>\n",
       "      <td>2005</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>00081T</td>\n",
       "      <td>ABD</td>\n",
       "      <td>A C C O BRANDS CORP</td>\n",
       "      <td>1.368606e+09</td>\n",
       "      <td>6</td>\n",
       "      <td>1972</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>3939</td>\n",
       "      <td>253034</td>\n",
       "      <td>DICK (A.B.) CO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11061</th>\n",
       "      <td>8</td>\n",
       "      <td>2005</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>00081T</td>\n",
       "      <td>ABD</td>\n",
       "      <td>A C C O BRANDS CORP</td>\n",
       "      <td>1.368606e+09</td>\n",
       "      <td>9</td>\n",
       "      <td>1972</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>3939</td>\n",
       "      <td>253034</td>\n",
       "      <td>DICK (A.B.) CO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11062</th>\n",
       "      <td>8</td>\n",
       "      <td>2005</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>00081T</td>\n",
       "      <td>ABD</td>\n",
       "      <td>A C C O BRANDS CORP</td>\n",
       "      <td>1.368606e+09</td>\n",
       "      <td>12</td>\n",
       "      <td>1972</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>3939</td>\n",
       "      <td>253034</td>\n",
       "      <td>DICK (A.B.) CO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11063</th>\n",
       "      <td>8</td>\n",
       "      <td>2005</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>00081T</td>\n",
       "      <td>ABD</td>\n",
       "      <td>A C C O BRANDS CORP</td>\n",
       "      <td>1.368606e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>1973</td>\n",
       "      <td>3579.0</td>\n",
       "      <td>3939</td>\n",
       "      <td>253034</td>\n",
       "      <td>DICK (A.B.) CO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78445040</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>N22717</td>\n",
       "      <td>CLB</td>\n",
       "      <td>CORE LABORATORIES NV</td>\n",
       "      <td>1.179562e+09</td>\n",
       "      <td>9</td>\n",
       "      <td>1982</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>3525</td>\n",
       "      <td>218677</td>\n",
       "      <td>CORE LABORATORIES</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78445041</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>N22717</td>\n",
       "      <td>CLB</td>\n",
       "      <td>CORE LABORATORIES NV</td>\n",
       "      <td>1.179562e+09</td>\n",
       "      <td>12</td>\n",
       "      <td>1982</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>3525</td>\n",
       "      <td>218677</td>\n",
       "      <td>CORE LABORATORIES</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78445042</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>N22717</td>\n",
       "      <td>CLB</td>\n",
       "      <td>CORE LABORATORIES NV</td>\n",
       "      <td>1.179562e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>1983</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>3525</td>\n",
       "      <td>218677</td>\n",
       "      <td>CORE LABORATORIES</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78445043</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>N22717</td>\n",
       "      <td>CLB</td>\n",
       "      <td>CORE LABORATORIES NV</td>\n",
       "      <td>1.179562e+09</td>\n",
       "      <td>6</td>\n",
       "      <td>1983</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>3525</td>\n",
       "      <td>218677</td>\n",
       "      <td>CORE LABORATORIES</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78445044</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>N22717</td>\n",
       "      <td>CLB</td>\n",
       "      <td>CORE LABORATORIES NV</td>\n",
       "      <td>1.179562e+09</td>\n",
       "      <td>9</td>\n",
       "      <td>1983</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>3525</td>\n",
       "      <td>218677</td>\n",
       "      <td>CORE LABORATORIES</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1458825 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Month_x  Year_x   SICCD cusip_6digit_x ticker_mod  \\\n",
       "11059           8    2005  3579.0         00081T        ABD   \n",
       "11060           8    2005  3579.0         00081T        ABD   \n",
       "11061           8    2005  3579.0         00081T        ABD   \n",
       "11062           8    2005  3579.0         00081T        ABD   \n",
       "11063           8    2005  3579.0         00081T        ABD   \n",
       "...           ...     ...     ...            ...        ...   \n",
       "78445040       12    2020  1389.0         N22717        CLB   \n",
       "78445041       12    2020  1389.0         N22717        CLB   \n",
       "78445042       12    2020  1389.0         N22717        CLB   \n",
       "78445043       12    2020  1389.0         N22717        CLB   \n",
       "78445044       12    2020  1389.0         N22717        CLB   \n",
       "\n",
       "                        COMNAM        Mcap_x  Month_y  Year_y  sic_code  \\\n",
       "11059      A C C O BRANDS CORP  1.368606e+09        3    1972    3579.0   \n",
       "11060      A C C O BRANDS CORP  1.368606e+09        6    1972    3579.0   \n",
       "11061      A C C O BRANDS CORP  1.368606e+09        9    1972    3579.0   \n",
       "11062      A C C O BRANDS CORP  1.368606e+09       12    1972    3579.0   \n",
       "11063      A C C O BRANDS CORP  1.368606e+09        3    1973    3579.0   \n",
       "...                        ...           ...      ...     ...       ...   \n",
       "78445040  CORE LABORATORIES NV  1.179562e+09        9    1982    1389.0   \n",
       "78445041  CORE LABORATORIES NV  1.179562e+09       12    1982    1389.0   \n",
       "78445042  CORE LABORATORIES NV  1.179562e+09        3    1983    1389.0   \n",
       "78445043  CORE LABORATORIES NV  1.179562e+09        6    1983    1389.0   \n",
       "78445044  CORE LABORATORIES NV  1.179562e+09        9    1983    1389.0   \n",
       "\n",
       "          gvkey cusip_6digit_y       company_name  Mcap_y  \n",
       "11059      3939         253034     DICK (A.B.) CO     NaN  \n",
       "11060      3939         253034     DICK (A.B.) CO     NaN  \n",
       "11061      3939         253034     DICK (A.B.) CO     NaN  \n",
       "11062      3939         253034     DICK (A.B.) CO     NaN  \n",
       "11063      3939         253034     DICK (A.B.) CO     NaN  \n",
       "...         ...            ...                ...     ...  \n",
       "78445040   3525         218677  CORE LABORATORIES     NaN  \n",
       "78445041   3525         218677  CORE LABORATORIES     NaN  \n",
       "78445042   3525         218677  CORE LABORATORIES     NaN  \n",
       "78445043   3525         218677  CORE LABORATORIES     NaN  \n",
       "78445044   3525         218677  CORE LABORATORIES     NaN  \n",
       "\n",
       "[1458825 rows x 14 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a closer look at all the additional matches that I got without date in join columns\n",
    "tickers_potentially_linked_with_sic[~tickers_potentially_linked_with_sic['cusip_6digit_x'].isin(\n",
    "    tickers_potentially_linked_with_sic_date['cusip_6digit_x'].unique())]\n",
    "\n",
    "# Lot's of mismatches. Lot's of cases where 2 different companies (same SIC code) used the same ticker across different times\n",
    "# I obviously expected some companies in same sector to grab an old ticker. But there are too many false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learnt date columns are too imp to remove\n",
    "# But what if don't use any sector of company in matching?\n",
    "# Trying to find matches on just ticker & date\n",
    "\n",
    "finding_interlinked_cusips = pd.merge(monthly_stock_data[['Month','Year','SICCD','cusip_6digit','ticker_mod', \n",
    "                                                          'COMNAM','Mcap', 'ticker_qtr']],\n",
    "                                      comp_data[~comp_data['ticker_qtr'].isin(problematic_ticker_qtr)][\n",
    "                                          ['Month','Year','sic_code','gvkey','cusip_6digit','ticker_mod', 'company_name', \n",
    "                                           'Mcap']],\n",
    "                                      left_on = ['Month','Year','ticker_mod'],\n",
    "                                      right_on = ['Month','Year','ticker_mod'],\n",
    "                                      how = 'inner')\n",
    "\n",
    "# But before attempting look at matches, need to make certain adjustments keeping the following scenaris in mind\n",
    "# Realized after few trails that these adjustments are imp when I am loosening join criteria by not using sector at all\n",
    "\n",
    "# Scenario 1\n",
    "# Let's say AGCO corporation used ticker 'AG' from 1980-2009 & ticker 'AGCO' from 2010-2020\n",
    "# Also, let's say First majestic silver corporation also used ticker 'FMS' from 1980-2009 & changed to ticker 'AG' from 2010-2020\n",
    "# The scenario I just described is problematic because AGCO corp 2005Q2 row will map to 2005 First majestic silver row\n",
    "# (Remember: Compustat has only the most recent row. So all First Majestic Silver rows will have ticker 'AG', AGCO corporation will have 'AGCO')\n",
    "# Ofcourse, if I had use SIC/NAICS, this problem wouldn't have happened because one is agricultural & another is mining sector\n",
    "\n",
    "# Getting the full list of tickers that mapped to multiple company names (across time)\n",
    "comp_ticker_mapping = pd.DataFrame(finding_interlinked_cusips.groupby('COMNAM')['ticker_mod'].nunique()).reset_index()\n",
    "comp_ticker_mapping.columns = ['COMNAM', '#unique_ticker_mod']\n",
    "problematic_comp = comp_ticker_mapping[comp_ticker_mapping['#unique_ticker_mod'] > 1]['COMNAM'].unique()\n",
    "\n",
    "# Getting the full list of tickers that mapped to multiple CUSIP_6digits (across time)\n",
    "cusip_ticker_mapping = pd.DataFrame(finding_interlinked_cusips.groupby('cusip_6digit_x')['ticker_mod'].nunique()).reset_index()\n",
    "cusip_ticker_mapping.columns = ['cusip_6digit_x', '#unique_ticker_mod']\n",
    "problematic_cusips = cusip_ticker_mapping[cusip_ticker_mapping['#unique_ticker_mod'] > 1]['cusip_6digit_x'].unique()\n",
    "\n",
    "# Scenario 2: another scenario\n",
    "# Let's say ABC construction corp was listed btw.1967-1980 and used the ticker 'ABC'\n",
    "# Also, let's say abc broadcasting was listed from 1967 - 2020 but used the ticker 'AB' from 1967-1980 before moving to 'ABC'\n",
    "# This scenario is turning out to be problematic because not all companies in CRSP are in compustat\n",
    "# So now, ABC construction 1967 rows are getting attached to abc broadcasting rows since its the only company in compustat 1967\n",
    "# Ofcourse, if I had use SIC/NAICS, this problem wouldn't have happened because one is real esatte & another is media sector\n",
    "\n",
    "ticker_comp_mapping = pd.DataFrame(finding_interlinked_cusips.groupby('ticker_mod')['COMNAM'].nunique()).reset_index()\n",
    "ticker_comp_mapping.columns = ['ticker_mod', '#unique_comp']\n",
    "problematic_tickers_comp = ticker_comp_mapping[ticker_comp_mapping['#unique_comp'] > 1]['ticker_mod'].unique()\n",
    "\n",
    "ticker_cusip_mapping = pd.DataFrame(finding_interlinked_cusips.groupby('ticker_mod')['cusip_6digit_x'].nunique()).reset_index()\n",
    "ticker_cusip_mapping.columns = ['ticker_mod', '#unique_cusip']\n",
    "problematic_ticker_cusip = ticker_cusip_mapping[ticker_cusip_mapping['#unique_cusip'] > 1]['ticker_mod'].unique()\n",
    "\n",
    "# Compiling a full list of problematic Tickers\n",
    "problematic_tickers = list(problematic_tickers_comp) + list(problematic_ticker_cusip)\n",
    "problematic_tickers = list(set(problematic_tickers)) # Dropping duplicates from the list\n",
    "\n",
    "# Getting all the CUSIP's that are potentially interlinked with ticker, siccd matching\n",
    "tickers_potentially_linked_with_date = finding_interlinked_cusips[((finding_interlinked_cusips['cusip_6digit_x']  != finding_interlinked_cusips['cusip_6digit_y']) &\n",
    "                                                                   (~finding_interlinked_cusips['ticker_mod'].isnull()) & \n",
    "                                                                   (~finding_interlinked_cusips['ticker_mod'].isin(problematic_tickers)) &\n",
    "                                                                   (~finding_interlinked_cusips['Month'].isnull()) &\n",
    "                                                                   (~finding_interlinked_cusips['Year'].isnull()) & \n",
    "                                                                   (~finding_interlinked_cusips['COMNAM'].isin(problematic_comp)) &\n",
    "                                                                   (~finding_interlinked_cusips['cusip_6digit_x'].isin(problematic_cusips)))]\n",
    "\n",
    "len(tickers_potentially_linked_with_date['cusip_6digit_x'].unique())\n",
    "# lot's of matches OMG!\n",
    "# Let's investigate the extra matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>ticker_mod</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>SICCD</th>\n",
       "      <th>sic_code</th>\n",
       "      <th>cusip_6digit_x</th>\n",
       "      <th>cusip_6digit_y</th>\n",
       "      <th>COMNAM</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>12</td>\n",
       "      <td>2006</td>\n",
       "      <td>ATS</td>\n",
       "      <td>166254</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>00203R</td>\n",
       "      <td>00211C</td>\n",
       "      <td>A P T SATELLITE HOLDINGS LTD</td>\n",
       "      <td>ATS ANDLAUER INCOME FUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4556</th>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>ATS</td>\n",
       "      <td>166254</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>00203R</td>\n",
       "      <td>00211C</td>\n",
       "      <td>A P T SATELLITE HOLDINGS LTD</td>\n",
       "      <td>ATS ANDLAUER INCOME FUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4557</th>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>ATS</td>\n",
       "      <td>166254</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>00203R</td>\n",
       "      <td>00211C</td>\n",
       "      <td>A P T SATELLITE HOLDINGS LTD</td>\n",
       "      <td>ATS ANDLAUER INCOME FUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>9</td>\n",
       "      <td>2007</td>\n",
       "      <td>ATS</td>\n",
       "      <td>166254</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>00203R</td>\n",
       "      <td>00211C</td>\n",
       "      <td>A P T SATELLITE HOLDINGS LTD</td>\n",
       "      <td>ATS ANDLAUER INCOME FUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>ATS</td>\n",
       "      <td>166254</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>00203R</td>\n",
       "      <td>00211C</td>\n",
       "      <td>A P T SATELLITE HOLDINGS LTD</td>\n",
       "      <td>ATS ANDLAUER INCOME FUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>ATS</td>\n",
       "      <td>166254</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>00203R</td>\n",
       "      <td>00211C</td>\n",
       "      <td>A P T SATELLITE HOLDINGS LTD</td>\n",
       "      <td>ATS ANDLAUER INCOME FUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>ATS</td>\n",
       "      <td>166254</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>4210.0</td>\n",
       "      <td>00203R</td>\n",
       "      <td>00211C</td>\n",
       "      <td>A P T SATELLITE HOLDINGS LTD</td>\n",
       "      <td>ATS ANDLAUER INCOME FUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>11</td>\n",
       "      <td>1996</td>\n",
       "      <td>ABDR</td>\n",
       "      <td>1895</td>\n",
       "      <td>7330.0</td>\n",
       "      <td>5812.0</td>\n",
       "      <td>002553</td>\n",
       "      <td>022909</td>\n",
       "      <td>ABACUS DIRECT CORP</td>\n",
       "      <td>AMBASSADOR FOOD SVCS CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5862</th>\n",
       "      <td>2</td>\n",
       "      <td>1997</td>\n",
       "      <td>ABDR</td>\n",
       "      <td>1895</td>\n",
       "      <td>7330.0</td>\n",
       "      <td>5812.0</td>\n",
       "      <td>002553</td>\n",
       "      <td>022909</td>\n",
       "      <td>ABACUS DIRECT CORP</td>\n",
       "      <td>AMBASSADOR FOOD SVCS CORP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month  Year ticker_mod   gvkey   SICCD  sic_code cusip_6digit_x  \\\n",
       "4555     12  2006        ATS  166254  4899.0    4210.0         00203R   \n",
       "4556      3  2007        ATS  166254  4899.0    4210.0         00203R   \n",
       "4557      6  2007        ATS  166254  4899.0    4210.0         00203R   \n",
       "4558      9  2007        ATS  166254  4899.0    4210.0         00203R   \n",
       "4559     12  2007        ATS  166254  4899.0    4210.0         00203R   \n",
       "4560      3  2008        ATS  166254  4899.0    4210.0         00203R   \n",
       "4561      6  2008        ATS  166254  4899.0    4210.0         00203R   \n",
       "5860     11  1996       ABDR    1895  7330.0    5812.0         002553   \n",
       "5862      2  1997       ABDR    1895  7330.0    5812.0         002553   \n",
       "\n",
       "     cusip_6digit_y                        COMNAM               company_name  \n",
       "4555         00211C  A P T SATELLITE HOLDINGS LTD   ATS ANDLAUER INCOME FUND  \n",
       "4556         00211C  A P T SATELLITE HOLDINGS LTD   ATS ANDLAUER INCOME FUND  \n",
       "4557         00211C  A P T SATELLITE HOLDINGS LTD   ATS ANDLAUER INCOME FUND  \n",
       "4558         00211C  A P T SATELLITE HOLDINGS LTD   ATS ANDLAUER INCOME FUND  \n",
       "4559         00211C  A P T SATELLITE HOLDINGS LTD   ATS ANDLAUER INCOME FUND  \n",
       "4560         00211C  A P T SATELLITE HOLDINGS LTD   ATS ANDLAUER INCOME FUND  \n",
       "4561         00211C  A P T SATELLITE HOLDINGS LTD   ATS ANDLAUER INCOME FUND  \n",
       "5860         022909            ABACUS DIRECT CORP  AMBASSADOR FOOD SVCS CORP  \n",
       "5862         022909            ABACUS DIRECT CORP  AMBASSADOR FOOD SVCS CORP  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a closer look at all the extra matches over the initial set of matches that are settled on earlier(the 1st join)\n",
    "cols_to_display = ['Month', 'Year', 'ticker_mod', 'gvkey', 'SICCD', 'sic_code', 'cusip_6digit_x', 'cusip_6digit_y', 'COMNAM', \n",
    "                  'company_name']\n",
    "tickers_potentially_linked_with_date[~tickers_potentially_linked_with_date['cusip_6digit_x'].isin(\n",
    "    tickers_potentially_linked_with_sic_date['cusip_6digit_x'].unique())][cols_to_display][21:30]\n",
    "\n",
    "# Realized, getting so many additional matches because noticed that sic code isn't necessarily the same on both sides\n",
    "# I didn't expect this\n",
    "# Realied that govt. stopped issuing SIC numbers in 1997 (ever since NAICS was introduced)\n",
    "# But private data companies like compustat, Dun & Bradstreet are still issuing their own SIC numbers (based on old system)\n",
    "# So AMC entertainment SIC code is 7830 & 7832 in CRSP & Compustat respectively (because their SIC datasources are different)\n",
    "\n",
    "# But despite multiple attempts, ticker mapping without using SIC as a joining column is proving too hard\n",
    "# Although ticker makes more intuitive sense as an ID column than company name, there are too many false positives\n",
    "# For example, I am joining APT Satellite holdings & ATS Andlauer income fund (2 companies with ticker in the same time)\n",
    "# The real big problem is that there not all companies in CRSP are in Compustat\n",
    "# So, if there's an 'abc' ticker on CRSP, and there happens to be an 'abc' ticker on compustat around the same time, it's joining\n",
    "# However, by looking at names in output, I can tell they are 2 very different companies\n",
    "# To summarize, if I want to use ticker, it has to be along with SIC matching\n",
    "\n",
    "# May be I can escape using sic code if I try join on company names\n",
    "# I am more hopeful about name matching.\n",
    "# Afterall, how likely is it that 2 companies have the exact same name in the same period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using company name too to find any additional interlinked CUSIP's\n",
    "# But company name need to be prepared properly before using it\n",
    "\n",
    "# Taking only alphabets in companies name\n",
    "monthly_stock_data['company_mod'] = monthly_stock_data['COMNAM'].str.replace('[^a-zA-Z]', '')\n",
    "# Removing all white spaces (even between company name)\n",
    "monthly_stock_data['company_mod'] = monthly_stock_data['company_mod'].str.replace(' ', '')\n",
    "# Converting all characters to upper\n",
    "monthly_stock_data['company_mod'] = monthly_stock_data['company_mod'].str.upper()\n",
    "\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['COMNAM'].astype(str).str[-4:] == '-OLD', monthly_stock_data['company_mod'].astype(str).str[:-4], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['COMNAM'].astype(str).str[-3:] == 'NEW', monthly_stock_data['company_mod'].astype(str).str[:-3], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['COMNAM'].astype(str).str[-4:] == 'CL A', monthly_stock_data['company_mod'].astype(str).str[:-4], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['COMNAM'].astype(str).str[-4:] == 'CL B', monthly_stock_data['company_mod'].astype(str).str[:-4], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['COMNAM'].astype(str).str[-4:] == 'CL C', monthly_stock_data['company_mod'].astype(str).str[:-4], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['COMNAM'].astype(str).str[-3:] == ' FD', monthly_stock_data['company_mod'].astype(str).str[:-3], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['COMNAM'].astype(str).str[-3:] == ' TR', monthly_stock_data['company_mod'].astype(str).str[:-3], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['COMNAM'].astype(str).str[-3:] == ' CO', monthly_stock_data['company_mod'].astype(str).str[:-3], monthly_stock_data['company_mod'])\n",
    "\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-3:] == 'LTD', monthly_stock_data['company_mod'].astype(str).str[:-3], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-3:] == 'INC', monthly_stock_data['company_mod'].astype(str).str[:-3], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-3:] == 'ETF', monthly_stock_data['company_mod'].astype(str).str[:-3], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-3:] == 'PLC', monthly_stock_data['company_mod'].astype(str).str[:-3], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-4:] == 'CORP', monthly_stock_data['company_mod'].astype(str).str[:-4], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-4:] == 'FUND', monthly_stock_data['company_mod'].astype(str).str[:-4], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-5:] == 'COLTD', monthly_stock_data['company_mod'].astype(str).str[:-5], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-5:] == 'TRUST', monthly_stock_data['company_mod'].astype(str).str[:-5], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-6:] == 'BERHAD', monthly_stock_data['company_mod'].astype(str).str[:-6], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-7:] == 'LIMITED', monthly_stock_data['company_mod'].astype(str).str[:-7], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-7:] == 'COMPANY', monthly_stock_data['company_mod'].astype(str).str[:-7], monthly_stock_data['company_mod'])\n",
    "monthly_stock_data['company_mod'] = np.where(monthly_stock_data['company_mod'].astype(str).str[-11:] == 'CORPORATION', monthly_stock_data['company_mod'].astype(str).str[:-11], monthly_stock_data['company_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, I have to prepare company name in comp data\n",
    "\n",
    "# Taking only alphabets in companies name\n",
    "comp_data['company_mod'] = comp_data['company_name'].str.replace('[^a-zA-Z]', '')\n",
    "# Removing all white spaces (even between company name)\n",
    "comp_data['company_mod'] = comp_data['company_mod'].str.replace(' ', '')\n",
    "# Converting all characters to upper\n",
    "comp_data['company_mod'] = comp_data['company_mod'].str.upper()\n",
    "\n",
    "comp_data['company_mod'] = np.where(comp_data['company_name'].astype(str).str[-4:] == '-OLD', comp_data['company_mod'].astype(str).str[:-4], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_name'].astype(str).str[-3:] == 'NEW', comp_data['company_mod'].astype(str).str[:-3], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_name'].astype(str).str[-4:] == 'CL A', comp_data['company_mod'].astype(str).str[:-4], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_name'].astype(str).str[-4:] == 'CL B', comp_data['company_mod'].astype(str).str[:-4], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_name'].astype(str).str[-4:] == 'CL C', comp_data['company_mod'].astype(str).str[:-4], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_name'].astype(str).str[-3:] == ' FD', comp_data['company_mod'].astype(str).str[:-3], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_name'].astype(str).str[-3:] == ' TR', comp_data['company_mod'].astype(str).str[:-3], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_name'].astype(str).str[-3:] == ' CO', comp_data['company_mod'].astype(str).str[:-3], comp_data['company_mod'])\n",
    "\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-3:] == 'LTD', comp_data['company_mod'].astype(str).str[:-3], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-3:] == 'INC', comp_data['company_mod'].astype(str).str[:-3], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-3:] == 'ETF', comp_data['company_mod'].astype(str).str[:-3], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-3:] == 'PLC', comp_data['company_mod'].astype(str).str[:-3], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-4:] == 'CORP', comp_data['company_mod'].astype(str).str[:-4], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-4:] == 'FUND', comp_data['company_mod'].astype(str).str[:-4], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-5:] == 'COLTD', comp_data['company_mod'].astype(str).str[:-5], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-5:] == 'TRUST', comp_data['company_mod'].astype(str).str[:-5], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-6:] == 'BERHAD', comp_data['company_mod'].astype(str).str[:-6], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-7:] == 'LIMITED', comp_data['company_mod'].astype(str).str[:-7], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-7:] == 'COMPANY', comp_data['company_mod'].astype(str).str[:-7], comp_data['company_mod'])\n",
    "comp_data['company_mod'] = np.where(comp_data['company_mod'].astype(str).str[-11:] == 'CORPORATION', comp_data['company_mod'].astype(str).str[:-11], comp_data['company_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     38729\n",
      "2       199\n",
      "3        12\n",
      "4         2\n",
      "10        1\n",
      "Name: # rows, dtype: int64\n",
      "1       3883429\n",
      "2          8699\n",
      "3           900\n",
      "5           428\n",
      "8           283\n",
      "         ...   \n",
      "83            1\n",
      "104           1\n",
      "82            1\n",
      "80            1\n",
      "2987          1\n",
      "Name: # rows, Length: 108, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Wondering, if it's possible for 1 company_mod to appear twice in each quarter\n",
    "comp_data['company_qtr'] = comp_data['company_mod'].astype(str) + comp_data['Year'].astype(str) + comp_data['Month'].astype(str)\n",
    "compustat_comp_qtr_repeat = pd.DataFrame(comp_data.groupby('company_mod')['cusip_6digit'].nunique()).reset_index()\n",
    "compustat_comp_qtr_repeat.columns = ['company_qtr', '# rows']\n",
    "print(compustat_comp_qtr_repeat['# rows'].value_counts())\n",
    "compustat_comp_qtr_problematic = compustat_comp_qtr_repeat[compustat_comp_qtr_repeat['# rows'] > 1]['company_qtr']\n",
    "\n",
    "# Creating similar company_mod_qtr column in CRSP data too\n",
    "monthly_stock_data['company_qtr'] = monthly_stock_data['company_mod'].astype(str) + monthly_stock_data['Year'].astype(str) + monthly_stock_data['Month'].astype(str)\n",
    "crsp_comp_qtr_repeat = pd.DataFrame(monthly_stock_data.groupby('company_qtr')['cusip_6digit'].nunique()).reset_index()\n",
    "crsp_comp_qtr_repeat.columns = ['company_qtr', '# rows']\n",
    "print(crsp_comp_qtr_repeat['# rows'].value_counts())\n",
    "crsp_comp_qtr_problematic = crsp_comp_qtr_repeat[crsp_comp_qtr_repeat['# rows'] > 1]['company_qtr']\n",
    "\n",
    "# Compiling a full list of problematic company qtr combinations\n",
    "problematic_comp_mod_qtr = list(compustat_comp_qtr_problematic) + list(crsp_comp_qtr_problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_mod</th>\n",
       "      <th>Num_unique_gvkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29496</th>\n",
       "      <td>RBCTARGETCORPBD</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19400</th>\n",
       "      <td>ISHARESIBONDSDECTRM</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18690</th>\n",
       "      <td>INVESCOBULMUNBND</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19640</th>\n",
       "      <td>ISHARESTARGETDATE</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19395</th>\n",
       "      <td>ISHARESIBNDDECTR</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               company_mod  Num_unique_gvkey\n",
       "29496      RBCTARGETCORPBD                10\n",
       "19400  ISHARESIBONDSDECTRM                10\n",
       "18690     INVESCOBULMUNBND                 9\n",
       "19640    ISHARESTARGETDATE                 9\n",
       "19395     ISHARESIBNDDECTR                 8"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I also checked if 1 company_mod can map to multiple gvkeys\n",
    "comnam_gvkey_mapping = pd.DataFrame(comp_data.groupby('company_mod')['gvkey'].nunique()).reset_index()\n",
    "comnam_gvkey_mapping.columns = ['company_mod','Num_unique_gvkey']\n",
    "comnam_gvkey_mapping = comnam_gvkey_mapping.sort_values(by=['Num_unique_gvkey'],ascending = False)\n",
    "comnam_gvkey_mapping[0:5]\n",
    "\n",
    "# A lot of indices have this problem of 1 company_mod mapping to multiple gvkeys\n",
    "# But there's few normal stocks that are using the same name\n",
    "# There are 3 diff companies with the name \"WEST\")\n",
    "# I don't like this situation. But I recognize that as long as these 'WEST's don't appear multiple times in same qtr it's not  a problem \n",
    "# Hence, I am not adding these names to problematic list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "853"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not even going to use SIC code in join because it's ridiculously unlikely that 2 companies have literally the same name in same time\n",
    "# Want to try to 2 joins\n",
    "# 1) Joining on company_mod, Month, Year\n",
    "# 2) Joining on company_mod only\n",
    "\n",
    "# Exploring 1st join\n",
    "finding_interlinked_cusips = pd.merge(monthly_stock_data[['cusip_6digit','Month', 'Year', 'company_mod', 'COMNAM']].drop_duplicates(),\n",
    "                                      comp_data[~comp_data['company_mod'].isin(problematic_comp_mod_qtr)][\n",
    "                                          ['cusip_6digit','gvkey','Month', 'Year', 'company_mod', 'company_name']].drop_duplicates(),\n",
    "                                      left_on = ['Month', 'Year', 'company_mod'],\n",
    "                                      right_on = ['Month', 'Year', 'company_mod'],\n",
    "                                      how = 'inner')\n",
    "\n",
    "# Getting all the CUSIP's that are potentially interlinked with date, company_name, siccd matching\n",
    "company_names_potentially_linked_with_date = finding_interlinked_cusips[((finding_interlinked_cusips['cusip_6digit_x']  != finding_interlinked_cusips['cusip_6digit_y']) &\n",
    "                                                                         (~finding_interlinked_cusips['company_mod'].isnull()) &\n",
    "                                                                         (~finding_interlinked_cusips['Month'].isnull()) & \n",
    "                                                                         (~finding_interlinked_cusips['Year'].isnull()))]\n",
    "len(company_names_potentially_linked_with_date['cusip_6digit_x'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1140"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finding_interlinked_cusips = pd.merge(monthly_stock_data[['cusip_6digit','company_mod','SICCD', 'Year',\n",
    "                                                          'COMNAM']].drop_duplicates(),\n",
    "                                      comp_data[~comp_data['company_mod'].isin(problematic_comp_mod_qtr)]\n",
    "                                      [['cusip_6digit','gvkey','company_mod','company_name']].drop_duplicates(),\n",
    "                                      left_on = ['company_mod'],\n",
    "                                      right_on = ['company_mod'],\n",
    "                                      how = 'inner')\n",
    "\n",
    "# Getting all the CUSIP's that are potentially interlinked with company_name, siccd matching\n",
    "company_names_potentially_linked = finding_interlinked_cusips[((finding_interlinked_cusips['cusip_6digit_x']  != finding_interlinked_cusips['cusip_6digit_y']) &\n",
    "                                                                         (~finding_interlinked_cusips['company_mod'].isnull()))]\n",
    "len(company_names_potentially_linked['cusip_6digit_x'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip_6digit_x</th>\n",
       "      <th>company_mod</th>\n",
       "      <th>SICCD</th>\n",
       "      <th>Year</th>\n",
       "      <th>COMNAM</th>\n",
       "      <th>cusip_6digit_y</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66906</th>\n",
       "      <td>898402</td>\n",
       "      <td>FIRSTCAPITAL</td>\n",
       "      <td>6711.0</td>\n",
       "      <td>1989</td>\n",
       "      <td>FIRST CAPITAL CORP</td>\n",
       "      <td>31942S</td>\n",
       "      <td>117034</td>\n",
       "      <td>FIRST CAPITAL INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66907</th>\n",
       "      <td>898402</td>\n",
       "      <td>FIRSTCAPITAL</td>\n",
       "      <td>6711.0</td>\n",
       "      <td>1990</td>\n",
       "      <td>FIRST CAPITAL CORP</td>\n",
       "      <td>31942S</td>\n",
       "      <td>117034</td>\n",
       "      <td>FIRST CAPITAL INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67058</th>\n",
       "      <td>319846</td>\n",
       "      <td>FIRSTCONNECTICUTBAN</td>\n",
       "      <td>6711.0</td>\n",
       "      <td>1972</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP INC</td>\n",
       "      <td>319850</td>\n",
       "      <td>186344</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67059</th>\n",
       "      <td>319846</td>\n",
       "      <td>FIRSTCONNECTICUTBAN</td>\n",
       "      <td>6711.0</td>\n",
       "      <td>1973</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP INC</td>\n",
       "      <td>319850</td>\n",
       "      <td>186344</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67060</th>\n",
       "      <td>319846</td>\n",
       "      <td>FIRSTCONNECTICUTBAN</td>\n",
       "      <td>6711.0</td>\n",
       "      <td>1974</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP INC</td>\n",
       "      <td>319850</td>\n",
       "      <td>186344</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67061</th>\n",
       "      <td>319846</td>\n",
       "      <td>FIRSTCONNECTICUTBAN</td>\n",
       "      <td>6711.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP INC</td>\n",
       "      <td>319850</td>\n",
       "      <td>186344</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67062</th>\n",
       "      <td>319846</td>\n",
       "      <td>FIRSTCONNECTICUTBAN</td>\n",
       "      <td>6711.0</td>\n",
       "      <td>1976</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP INC</td>\n",
       "      <td>319850</td>\n",
       "      <td>186344</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67063</th>\n",
       "      <td>319846</td>\n",
       "      <td>FIRSTCONNECTICUTBAN</td>\n",
       "      <td>6711.0</td>\n",
       "      <td>1977</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP INC</td>\n",
       "      <td>319850</td>\n",
       "      <td>186344</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67064</th>\n",
       "      <td>319846</td>\n",
       "      <td>FIRSTCONNECTICUTBAN</td>\n",
       "      <td>6711.0</td>\n",
       "      <td>1978</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP INC</td>\n",
       "      <td>319850</td>\n",
       "      <td>186344</td>\n",
       "      <td>FIRST CONNECTICUT BANCORP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cusip_6digit_x          company_mod   SICCD  Year  \\\n",
       "66906         898402         FIRSTCAPITAL  6711.0  1989   \n",
       "66907         898402         FIRSTCAPITAL  6711.0  1990   \n",
       "67058         319846  FIRSTCONNECTICUTBAN  6711.0  1972   \n",
       "67059         319846  FIRSTCONNECTICUTBAN  6711.0  1973   \n",
       "67060         319846  FIRSTCONNECTICUTBAN  6711.0  1974   \n",
       "67061         319846  FIRSTCONNECTICUTBAN  6711.0  1975   \n",
       "67062         319846  FIRSTCONNECTICUTBAN  6711.0  1976   \n",
       "67063         319846  FIRSTCONNECTICUTBAN  6711.0  1977   \n",
       "67064         319846  FIRSTCONNECTICUTBAN  6711.0  1978   \n",
       "\n",
       "                              COMNAM cusip_6digit_y   gvkey  \\\n",
       "66906             FIRST CAPITAL CORP         31942S  117034   \n",
       "66907             FIRST CAPITAL CORP         31942S  117034   \n",
       "67058  FIRST CONNECTICUT BANCORP INC         319850  186344   \n",
       "67059  FIRST CONNECTICUT BANCORP INC         319850  186344   \n",
       "67060  FIRST CONNECTICUT BANCORP INC         319850  186344   \n",
       "67061  FIRST CONNECTICUT BANCORP INC         319850  186344   \n",
       "67062  FIRST CONNECTICUT BANCORP INC         319850  186344   \n",
       "67063  FIRST CONNECTICUT BANCORP INC         319850  186344   \n",
       "67064  FIRST CONNECTICUT BANCORP INC         319850  186344   \n",
       "\n",
       "                    company_name  \n",
       "66906          FIRST CAPITAL INC  \n",
       "66907          FIRST CAPITAL INC  \n",
       "67058  FIRST CONNECTICUT BANCORP  \n",
       "67059  FIRST CONNECTICUT BANCORP  \n",
       "67060  FIRST CONNECTICUT BANCORP  \n",
       "67061  FIRST CONNECTICUT BANCORP  \n",
       "67062  FIRST CONNECTICUT BANCORP  \n",
       "67063  FIRST CONNECTICUT BANCORP  \n",
       "67064  FIRST CONNECTICUT BANCORP  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a closer look at the additional matches that I got without date in join columns\n",
    "company_names_potentially_linked[~company_names_potentially_linked['cusip_6digit_x'].isin(\n",
    "    company_names_potentially_linked_with_date['cusip_6digit_x'].unique())][1001:1010]\n",
    "\n",
    "# There's some mismatches as expected\n",
    "# For example, the company_mod AIRLEASE has been used by 2 different companies (unrelated) across time\n",
    "# It's easy to think that mismatches are going only to be few (which company is going to use an old name? lol!)\n",
    "# But found too many false positives\n",
    "# There used to be a big company called Abington bank, based in Abington, MA in 1990. That company went bankrupt\n",
    "# Another Abington bank from Abington, PA listed in 2000's\n",
    "# The risk is too high for few hundred additional matches\n",
    "\n",
    "# Code from investigation of a particular false positive\n",
    "#monthly_stock_data[monthly_stock_data['company_mod'] == 'UNIONELECTRICCO'][['date', 'cusip_6digit', 'COMNAM', 'Mcap']]\n",
    "#comp_data[comp_data['company_mod'] == 'UNIONELECTRICCO'][['quarter_end_date', 'cusip_6digit', 'company_name', 'Mcap','cusip']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>cusip_6digit_x</th>\n",
       "      <th>ticker_mod</th>\n",
       "      <th>COMNAM</th>\n",
       "      <th>Mcap_x</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>cusip_6digit_y</th>\n",
       "      <th>company_name</th>\n",
       "      <th>Mcap_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>325412.0</td>\n",
       "      <td>00972G</td>\n",
       "      <td>CLTX</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC</td>\n",
       "      <td>2.669986e+07</td>\n",
       "      <td>16687</td>\n",
       "      <td>15119A</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC -ADR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>6</td>\n",
       "      <td>2014</td>\n",
       "      <td>325412.0</td>\n",
       "      <td>00972G</td>\n",
       "      <td>CLTX</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC</td>\n",
       "      <td>2.550680e+07</td>\n",
       "      <td>16687</td>\n",
       "      <td>15119A</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC -ADR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>325412.0</td>\n",
       "      <td>00972G</td>\n",
       "      <td>CLTX</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC</td>\n",
       "      <td>2.509540e+07</td>\n",
       "      <td>16687</td>\n",
       "      <td>15119A</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC -ADR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>325412.0</td>\n",
       "      <td>00972G</td>\n",
       "      <td>CLTX</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC</td>\n",
       "      <td>1.974720e+07</td>\n",
       "      <td>16687</td>\n",
       "      <td>15119A</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC -ADR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3850</th>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>325412.0</td>\n",
       "      <td>00972G</td>\n",
       "      <td>CLTX</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC</td>\n",
       "      <td>3.126640e+06</td>\n",
       "      <td>16687</td>\n",
       "      <td>15119A</td>\n",
       "      <td>CELSUS THERAPEUTICS PLC -ADR</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140023</th>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>334112.0</td>\n",
       "      <td>G7945M</td>\n",
       "      <td>STX</td>\n",
       "      <td>SEAGATE TECHNOLOGY PLC</td>\n",
       "      <td>1.563130e+10</td>\n",
       "      <td>150937</td>\n",
       "      <td>G7997R</td>\n",
       "      <td>SEAGATE TECHNOLOGY HOLDINGS</td>\n",
       "      <td>15524.9780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140024</th>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>334112.0</td>\n",
       "      <td>G7945M</td>\n",
       "      <td>STX</td>\n",
       "      <td>SEAGATE TECHNOLOGY PLC</td>\n",
       "      <td>1.273465e+10</td>\n",
       "      <td>150937</td>\n",
       "      <td>G7997R</td>\n",
       "      <td>SEAGATE TECHNOLOGY HOLDINGS</td>\n",
       "      <td>12558.7776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140025</th>\n",
       "      <td>6</td>\n",
       "      <td>2020</td>\n",
       "      <td>334112.0</td>\n",
       "      <td>G7945M</td>\n",
       "      <td>STX</td>\n",
       "      <td>SEAGATE TECHNOLOGY PLC</td>\n",
       "      <td>1.242322e+10</td>\n",
       "      <td>150937</td>\n",
       "      <td>G7997R</td>\n",
       "      <td>SEAGATE TECHNOLOGY HOLDINGS</td>\n",
       "      <td>12427.7668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140026</th>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>334112.0</td>\n",
       "      <td>G7945M</td>\n",
       "      <td>STX</td>\n",
       "      <td>SEAGATE TECHNOLOGY PLC</td>\n",
       "      <td>1.263958e+10</td>\n",
       "      <td>150937</td>\n",
       "      <td>G7997R</td>\n",
       "      <td>SEAGATE TECHNOLOGY HOLDINGS</td>\n",
       "      <td>12692.1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140027</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>334112.0</td>\n",
       "      <td>G7945M</td>\n",
       "      <td>STX</td>\n",
       "      <td>SEAGATE TECHNOLOGY PLC</td>\n",
       "      <td>1.597089e+10</td>\n",
       "      <td>150937</td>\n",
       "      <td>G7997R</td>\n",
       "      <td>SEAGATE TECHNOLOGY HOLDINGS</td>\n",
       "      <td>14902.8600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1725 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Month  Year     NAICS cusip_6digit_x ticker_mod  \\\n",
       "3846        3  2014  325412.0         00972G       CLTX   \n",
       "3847        6  2014  325412.0         00972G       CLTX   \n",
       "3848        9  2014  325412.0         00972G       CLTX   \n",
       "3849       12  2014  325412.0         00972G       CLTX   \n",
       "3850        3  2015  325412.0         00972G       CLTX   \n",
       "...       ...   ...       ...            ...        ...   \n",
       "140023     12  2019  334112.0         G7945M        STX   \n",
       "140024      3  2020  334112.0         G7945M        STX   \n",
       "140025      6  2020  334112.0         G7945M        STX   \n",
       "140026      9  2020  334112.0         G7945M        STX   \n",
       "140027     12  2020  334112.0         G7945M        STX   \n",
       "\n",
       "                         COMNAM        Mcap_x   gvkey cusip_6digit_y  \\\n",
       "3846    CELSUS THERAPEUTICS PLC  2.669986e+07   16687         15119A   \n",
       "3847    CELSUS THERAPEUTICS PLC  2.550680e+07   16687         15119A   \n",
       "3848    CELSUS THERAPEUTICS PLC  2.509540e+07   16687         15119A   \n",
       "3849    CELSUS THERAPEUTICS PLC  1.974720e+07   16687         15119A   \n",
       "3850    CELSUS THERAPEUTICS PLC  3.126640e+06   16687         15119A   \n",
       "...                         ...           ...     ...            ...   \n",
       "140023   SEAGATE TECHNOLOGY PLC  1.563130e+10  150937         G7997R   \n",
       "140024   SEAGATE TECHNOLOGY PLC  1.273465e+10  150937         G7997R   \n",
       "140025   SEAGATE TECHNOLOGY PLC  1.242322e+10  150937         G7997R   \n",
       "140026   SEAGATE TECHNOLOGY PLC  1.263958e+10  150937         G7997R   \n",
       "140027   SEAGATE TECHNOLOGY PLC  1.597089e+10  150937         G7997R   \n",
       "\n",
       "                        company_name      Mcap_y  \n",
       "3846    CELSUS THERAPEUTICS PLC -ADR         NaN  \n",
       "3847    CELSUS THERAPEUTICS PLC -ADR         NaN  \n",
       "3848    CELSUS THERAPEUTICS PLC -ADR         NaN  \n",
       "3849    CELSUS THERAPEUTICS PLC -ADR         NaN  \n",
       "3850    CELSUS THERAPEUTICS PLC -ADR         NaN  \n",
       "...                              ...         ...  \n",
       "140023   SEAGATE TECHNOLOGY HOLDINGS  15524.9780  \n",
       "140024   SEAGATE TECHNOLOGY HOLDINGS  12558.7776  \n",
       "140025   SEAGATE TECHNOLOGY HOLDINGS  12427.7668  \n",
       "140026   SEAGATE TECHNOLOGY HOLDINGS  12692.1984  \n",
       "140027   SEAGATE TECHNOLOGY HOLDINGS  14902.8600  \n",
       "\n",
       "[1725 rows x 11 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am wondering if I just should work with company,date matches or if there's any sense in using ticker, sic, date matches\n",
    "\n",
    "tickers_potentially_linked_with_sic_date[~tickers_potentially_linked_with_sic_date['cusip_6digit_x'].isin(\n",
    "    company_names_potentially_linked_with_date['cusip_6digit_x'].unique())][2500:2510]\n",
    "# Explored the extra matches. Wow. There's some really good matches here\n",
    "# For example, a company called Alfacell renamed to Tamir biotechnology. Ticker, SIC, date joined picked up this combination\n",
    "# Picking up this sort of interlink is not even remotely possible if one relies only on company names\n",
    "# Even if some sort of string matching, an interlink like Alfacell & Tamir biotechnology will never get picked up\n",
    "# Another example is DNA MED INC renamed to DNA Medical\n",
    "\n",
    "# Similarly, there's value in earlier discovered ticker,naics, date matches\n",
    "tickers_potentially_linked_with_naics_date[~tickers_potentially_linked_with_naics_date['cusip_6digit_x'].isin(\n",
    "    company_names_potentially_linked_with_date['cusip_6digit_x'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1112\n",
      "2      23\n",
      "Name: # Unique_gvkey, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# To summarize, I will make effective use of 3 different matches methods to identify all potential matches\n",
    "cusip_gvkey_mapping_discovered_matches = pd.concat([tickers_potentially_linked_with_naics_date[['cusip_6digit_x','gvkey','company_name']],\n",
    "                                                   tickers_potentially_linked_with_sic_date[['cusip_6digit_x','gvkey','company_name']],\n",
    "                                                   company_names_potentially_linked_with_date[['cusip_6digit_x','gvkey','company_name']]],\n",
    "                                                   axis = 0)\n",
    "\n",
    "# Removing duplicates\n",
    "cusip_gvkey_mapping_discovered_matches = cusip_gvkey_mapping_discovered_matches.drop_duplicates(subset = ['cusip_6digit_x', \n",
    "                                                                                                         'gvkey'])\n",
    "\n",
    "# Wondering if it's possible for 1 cusip_6digit_x to map to multiple gvkeys\n",
    "cusip_6digit_x_gvkey_uniques = pd.DataFrame(cusip_gvkey_mapping_discovered_matches.groupby('cusip_6digit_x')['gvkey'].nunique()).reset_index()\n",
    "cusip_6digit_x_gvkey_uniques.columns = ['cusip_6digit', '# Unique_gvkey']\n",
    "print(cusip_6digit_x_gvkey_uniques['# Unique_gvkey'].value_counts())\n",
    "\n",
    "# Discovered it's possible because even gvkey is not truly perfect identifier (as discussed in fundamental data prep code)\n",
    "cusip_6digit_x_gvkey_uniques[cusip_6digit_x_gvkey_uniques['# Unique_gvkey'] == 2]\n",
    "cusip_gvkey_mapping_discovered_matches[cusip_gvkey_mapping_discovered_matches['cusip_6digit_x'] == '12561E'][['gvkey','company_name']]\n",
    "# For example, cusip_6digit_x '12561E' mapped to gvkey '147809' based on ticker, sic, month, year\n",
    "# But the same cusip_6digit_x '12561E' also mapped to gvkey '6346' based on company_name, month, year\n",
    "\n",
    "# If a cusip_6digit has 2 different gvkeys, let's just take the gvkey that appears in most number of rows (on Comp side)\n",
    "num_of_gvkeys = pd.DataFrame(comp_data['gvkey'].value_counts()).reset_index()\n",
    "num_of_gvkeys.columns = ['gvkey','Num_of_values']\n",
    "\n",
    "cusip_gvkey_mapping_discovered_matches = pd.merge(cusip_gvkey_mapping_discovered_matches,\n",
    "                                                  num_of_gvkeys,\n",
    "                                                  left_on = 'gvkey',\n",
    "                                                  right_on = 'gvkey',\n",
    "                                                  how = 'left')\n",
    "cusip_gvkey_mapping_discovered_matches = cusip_gvkey_mapping_discovered_matches.sort_values(by = ['cusip_6digit_x', 'gvkey', \n",
    "                                                                                                  'Num_of_values'],ascending = False)\n",
    "cusip_gvkey_mapping_discovered_matches = cusip_gvkey_mapping_discovered_matches.drop_duplicates(subset = ['cusip_6digit_x'])\n",
    "cusip_gvkey_mapping_discovered_matches = cusip_gvkey_mapping_discovered_matches[['cusip_6digit_x', 'gvkey']]\n",
    "# Now all cusip_6digits map gvkey one to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27016, 2)\n",
      "1    23747\n",
      "2       82\n",
      "3       23\n",
      "4       18\n",
      "5       16\n",
      "Name: # Unique gvkey, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# creating the cusip_6digit & gvkey mapping when cusip_6digits do match on CRSP & compustat side\n",
    "\n",
    "cusip_gvkey_mapping_same_cusip = pd.merge(monthly_stock_data['cusip_6digit'].drop_duplicates(),\n",
    "                                          comp_data[['cusip_6digit','gvkey']].drop_duplicates(),\n",
    "                                          left_on = 'cusip_6digit',\n",
    "                                          right_on = 'cusip_6digit',\n",
    "                                          how = 'inner')\n",
    "print(cusip_gvkey_mapping_same_cusip.shape)\n",
    "\n",
    "# If a cusip_6digit maps to multiple gvkeys, let's just exclude it from matches (I can try cusip_8digit mapping for such cases)\n",
    "cusip6digit_gvkey_uniques = pd.DataFrame(cusip_gvkey_mapping_same_cusip.groupby('cusip_6digit')['gvkey'].nunique()).reset_index()\n",
    "cusip6digit_gvkey_uniques.columns = ['cusip_6digit', '# Unique gvkey']\n",
    "print(cusip6digit_gvkey_uniques['# Unique gvkey'].value_counts()[0:5])\n",
    "# known there like <1% cusip_6digits that are problematic\n",
    "\n",
    "# Getting all such problematic cusip_6digits\n",
    "problematic_id = cusip6digit_gvkey_uniques[cusip6digit_gvkey_uniques['# Unique gvkey'] > 1]['cusip_6digit']\n",
    "cusip_gvkey_mapping_same_cusip = cusip_gvkey_mapping_same_cusip[~cusip_gvkey_mapping_same_cusip['cusip_6digit'].isin(\n",
    "                                                                problematic_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80729, 5)\n",
      "1    3944\n",
      "2    2023\n",
      "4     546\n",
      "3     529\n",
      "5     409\n",
      "Name: # Unique gvkey date, dtype: int64\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "# If unable to match a cusip_6digit from above join (because 1 cusip_6digit mapping to multiple gvkeys, using cusip_6digit & date)\n",
    "# Let's see how many additional matches I am able to get\n",
    "\n",
    "cusip_gvkey_mapping_same_cusip_with_date = pd.merge(monthly_stock_data[~monthly_stock_data['cusip_6digit'].isin(\n",
    "                                                    cusip_gvkey_mapping_same_cusip['cusip_6digit'])][\n",
    "                                                    ['cusip_6digit','Month','Year','cusip_6digit_date']].drop_duplicates(),\n",
    "                                                    comp_data[['cusip_6digit','gvkey','Month','Year']].drop_duplicates(),\n",
    "                                                    left_on = ['cusip_6digit','Month','Year'],\n",
    "                                                    right_on = ['cusip_6digit','Month','Year'],\n",
    "                                                    how = 'inner')\n",
    "print(cusip_gvkey_mapping_same_cusip_with_date.shape)\n",
    "\n",
    "# If a cusip_6digit_date maps to multiple gvkeys, let's just exclude it from matches (I can try cusip_8digit mapping for such cases)\n",
    "cusip6digit_gvkey_uniques = pd.DataFrame(cusip_gvkey_mapping_same_cusip_with_date.groupby('cusip_6digit_date')['gvkey'].nunique()).reset_index()\n",
    "cusip6digit_gvkey_uniques.columns = ['cusip_6digit_date', '# Unique gvkey date']\n",
    "print(cusip6digit_gvkey_uniques['# Unique gvkey date'].value_counts()[0:5])\n",
    "# know there like <1% cusip_6digits that are problematic\n",
    "\n",
    "# Getting all such problematic cusip_6digit_dates\n",
    "problematic_id = cusip6digit_gvkey_uniques[cusip6digit_gvkey_uniques['# Unique gvkey date'] > 1]['cusip_6digit_date']\n",
    "cusip_gvkey_mapping_same_cusip_with_date = cusip_gvkey_mapping_same_cusip_with_date[~cusip_gvkey_mapping_same_cusip_with_date\n",
    "                                                                                    ['cusip_6digit_date'].isin(problematic_id)]\n",
    "cusip_gvkey_mapping_same_cusip_with_date = cusip_gvkey_mapping_same_cusip_with_date.drop_duplicates(subset = ['cusip_6digit',\n",
    "                                                                                                'gvkey','cusip_6digit_date'])\n",
    "print(len(cusip_gvkey_mapping_same_cusip_with_date['cusip_6digit'].unique()))\n",
    "\n",
    "# Didn't get many mappings this way. Just 3944 mappings\n",
    "# Will not be using this data because very few additional mappings \n",
    "\n",
    "# Will only use mappings from cusip_gvkey_mapping_discovered_matches and cusip_gvkey_mapping_same_cusip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565\n"
     ]
    }
   ],
   "source": [
    "# Compiling a full list of cusip_6digit & gvkey matchings\n",
    "cusip_gvkey_mapping_discovered_matches.rename(columns = {'cusip_6digit_x': 'cusip_6digit'}, inplace = True)\n",
    "print(len(cusip_gvkey_mapping_discovered_matches[cusip_gvkey_mapping_discovered_matches['cusip_6digit'].isin(\n",
    "                                           cusip_gvkey_mapping_same_cusip['cusip_6digit'])]))\n",
    "\n",
    "# Realised that almost half of cusip_6digits in discovered matches are in cusip_gvkey_mapping_same_cusip\n",
    "# This is partly because of 2 reasons:\n",
    "# 1) Matching algorithm is not perfect. There are going to be few boundary cases that are wrong matches\n",
    "# 2) Even compustat data is not perfect. For ex: amc has 2 gvkeys (1038 & 164271)\n",
    "#    Ofcourse, GM doesn't have this problem as entire GM has just 1 gvkey\n",
    "#    Matching algo is able to catch such mistakes in compustat data\n",
    "\n",
    "# Lot of cases like amc have been found (companies which delisted and appeared again)\n",
    "# But I am going to attempt to come up with some adjustments or create some new ID\n",
    "# Because creating a new real unique ID is not worth it considering just few hundred cases (not much additional lift)\n",
    "# Honestly, this whole additional company matching exercise is a massive time sink\n",
    "# I worked so hard with ticker, company_name columns to find additional matches when CUSIP_6digits are not same\n",
    "# Found around 1100, of which I could have mapped half anyway I just did cusip_6digit to cusip_6digit join (print line above)\n",
    "# This whole matching excercise has been challenging! & fun(?) but time to move on\n",
    "\n",
    "# Let's just rely on gvkey to be a unique company identifier\n",
    "# Hence, I will prioritize matches that have been found with cusip_6 digit matching over matches when cusips are not matching \n",
    "\n",
    "cusip_gvkey_mapping_discovered_matches = cusip_gvkey_mapping_discovered_matches[~cusip_gvkey_mapping_discovered_matches['cusip_6digit'].isin(\n",
    "                                                                                cusip_gvkey_mapping_same_cusip['cusip_6digit'])]\n",
    "\n",
    "\n",
    "full_cusip6digit_gvkey = pd.concat([cusip_gvkey_mapping_same_cusip, cusip_gvkey_mapping_discovered_matches], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24317, 2)\n",
      "24317\n",
      "1    24317\n",
      "Name: gvkey, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking final version of cusip_6digit mapping file\n",
    "print(full_cusip6digit_gvkey.shape)\n",
    "print(len(full_cusip6digit_gvkey['cusip_6digit'].unique()))\n",
    "print(pd.DataFrame(full_cusip6digit_gvkey.groupby('cusip_6digit')['gvkey'].nunique()).reset_index()['gvkey'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3938621, 34)\n",
      "(24317, 2)\n",
      "(3938621, 35)\n"
     ]
    }
   ],
   "source": [
    "# Joining gvkey into monthly_stock_data with cusip_6digit mapping file that was just created.\n",
    "print(monthly_stock_data.shape)\n",
    "print(full_cusip6digit_gvkey.shape)\n",
    "monthly_stock_data = pd.merge(monthly_stock_data,\n",
    "                              full_cusip6digit_gvkey,\n",
    "                              left_on = 'cusip_6digit',\n",
    "                              right_on = 'cusip_6digit',\n",
    "                              how = 'left')\n",
    "print(monthly_stock_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3938621, 35)\n",
      "384282\n",
      "29760\n",
      "30265\n",
      "5443\n"
     ]
    }
   ],
   "source": [
    "print(monthly_stock_data.shape)\n",
    "print(monthly_stock_data['gvkey'].isnull().sum())\n",
    "print(len(monthly_stock_data['cusip_6digit'].unique()))\n",
    "print(len(monthly_stock_data['cusip_8digit'].unique()))\n",
    "print(len(monthly_stock_data[monthly_stock_data['gvkey'].isnull()]['cusip_6digit'].unique()))\n",
    "# According to the observations there are few cusip_6digits of small companies which were on market for short period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3938621, 35)\n",
      "(1828155, 72)\n",
      "(23353, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    23353\n",
       "Name: # of gvkey, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempting to map on cusip_8digit\n",
    "\n",
    "print(monthly_stock_data.shape)\n",
    "print(comp_data.shape)\n",
    "cusip8digit_gvkey_mapping_same_cusip = pd.merge(monthly_stock_data['cusip_8digit'].drop_duplicates(),\n",
    "                                                comp_data[['cusip_8digit','gvkey']].drop_duplicates(),\n",
    "                                                left_on = 'cusip_8digit',\n",
    "                                                right_on = 'cusip_8digit',\n",
    "                                                how = 'inner')\n",
    "print(cusip8digit_gvkey_mapping_same_cusip.shape)\n",
    "\n",
    "# As expected, no cusip_8digit maps to multiple gvkeys\n",
    "# That's because on compustat side a cusip_8digit maps to a single gvkey\n",
    "cusip_gvkey_comp_data_uniques = pd.DataFrame(cusip8digit_gvkey_mapping_same_cusip.groupby('cusip_8digit')['gvkey'].nunique()).reset_index()\n",
    "cusip_gvkey_comp_data_uniques.columns = ['cusip_8digit', '# of gvkey']\n",
    "cusip_gvkey_comp_data_uniques['# of gvkey'].value_counts()\n",
    "# No need to exclude any matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3938621, 35)\n",
      "(3938621, 36)\n"
     ]
    }
   ],
   "source": [
    "# Creating cusip_8digit mapping file and merging into monthly_stock_data\n",
    "\n",
    "cusip_8digit_mapping_file = cusip8digit_gvkey_mapping_same_cusip[['cusip_8digit','gvkey']].drop_duplicates()\n",
    "\n",
    "print(monthly_stock_data.shape)\n",
    "monthly_stock_data = pd.merge(monthly_stock_data,\n",
    "                              cusip_8digit_mapping_file,\n",
    "                              left_on = 'cusip_8digit',\n",
    "                              right_on = 'cusip_8digit',\n",
    "                              how = 'left')\n",
    "print(monthly_stock_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3938621, 36)\n",
      "589896\n",
      "29760\n",
      "30265\n",
      "6912\n"
     ]
    }
   ],
   "source": [
    "print(monthly_stock_data.shape)\n",
    "print(monthly_stock_data['gvkey_y'].isnull().sum())\n",
    "print(len(monthly_stock_data['cusip_6digit'].unique()))\n",
    "print(len(monthly_stock_data['cusip_8digit'].unique()))\n",
    "print(len(monthly_stock_data[monthly_stock_data['gvkey_y'].isnull()]['cusip_8digit'].unique()))\n",
    "# As expected cusip_8digits results in lesser matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28402"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many rows have gvkey_y but no gvkey_x (additional matches)\n",
    "\n",
    "len(monthly_stock_data[(monthly_stock_data['gvkey_x'].isnull()) & (~monthly_stock_data['gvkey_y'].isnull())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ETF                991\n",
       "Close end funds    197\n",
       "Name: SHRCD_Description, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just checking if there's any cases where gvkey_x doesn't match with gvkey_y\n",
    "\n",
    "monthly_stock_data[((monthly_stock_data['gvkey_x'] != monthly_stock_data['gvkey_y']) &\n",
    "                    (~monthly_stock_data['gvkey_x'].isnull()) &\n",
    "                    (~monthly_stock_data['gvkey_y'].isnull()))]['SHRCD_Description'].value_counts()\n",
    "# All cases where gvkey  from cusip_6digit mapping and cusip_8digit mapping are actually some index funds\n",
    "# When both mappings don't match, let's consider the 8 digit mapping (very few mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gvkey      355880\n",
      "gvkey_y    589896\n",
      "gvkey_x    384282\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "monthly_stock_data['gvkey'] =  monthly_stock_data['gvkey_y'].fillna(monthly_stock_data['gvkey_x'])\n",
    "print(monthly_stock_data[['gvkey','gvkey_y','gvkey_x']].isnull().sum())\n",
    "# Removing gvkey_x and gvkey_y. Don't need them anymore\n",
    "monthly_stock_data = monthly_stock_data.drop(['gvkey_x','gvkey_y'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24524\n",
      "5219\n",
      "29760\n",
      "ETF                  8229\n",
      "Normal Stock         1778\n",
      "Close end funds       301\n",
      "Foreign Companies     254\n",
      "REIT                   87\n",
      "Name: SHRCD_Description, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Need to do few final checks\n",
    "print(len(monthly_stock_data['gvkey'].unique()))\n",
    "print(len(monthly_stock_data[monthly_stock_data['gvkey'].isnull()]['cusip_6digit'].unique()))\n",
    "print(len(monthly_stock_data['cusip_6digit'].unique()))\n",
    "# Why are counts not adding up perfectly?\n",
    "\n",
    "# Is it possible for 1 cusip_6digit to map to multiple gvkey? I have encountered cases like this earlier\n",
    "cusip_6digit_gvkey_mapping = pd.DataFrame(monthly_stock_data.groupby('cusip_6digit')['gvkey'].nunique()).reset_index()\n",
    "cusip_6digit_gvkey_mapping.columns = ['cusip_6digit','Num_of_unique_gvkey']\n",
    "cusip_mapto_multiple_gvkeys = cusip_6digit_gvkey_mapping[cusip_6digit_gvkey_mapping['Num_of_unique_gvkey'] > 1]['cusip_6digit']\n",
    "print(monthly_stock_data[monthly_stock_data['cusip_6digit'].isin(cusip_mapto_multiple_gvkeys)]['SHRCD_Description'].value_counts())\n",
    "# Once again, it's possible for some cusip_6digit's to map to multiple gvkeys. Some of these rows are normal stocks too!!!!\n",
    "# At the stage of cusip_6digit join, excluded any cusip_6digit that maps to gvkeys\n",
    "# However, it's possible that 1 cusip_6digit, can map to 2 different cusip_8digits (& 2 different gvkeys)\n",
    "# Remember, there's a cusip_8digit join too along with cusip_6digit join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan1973-01-31        1787\n",
      "nan1973-02-28        1785\n",
      "nan1973-03-30        1776\n",
      "nan1972-12-29        1765\n",
      "nan1972-11-30        1747\n",
      "                     ... \n",
      "27980.01999-04-30       1\n",
      "29123.01997-06-30       1\n",
      "24723.01991-10-31       1\n",
      "5530.02002-10-31        1\n",
      "26930.02008-02-29       1\n",
      "Name: gvkey_date, Length: 3580122, dtype: int64\n",
      "(0, 36)\n"
     ]
    }
   ],
   "source": [
    "# But, does one gvkey appear only once for a specific date (Month, year)?\n",
    "# Otherwise, there's a problem with level of data\n",
    "\n",
    "# Earlier, I made sure that level of data is cusip_6digit_date\n",
    "monthly_stock_data['cusip_6digit_date'].value_counts().max()\n",
    "\n",
    "# Testing gvkey, date as level of data\n",
    "monthly_stock_data['gvkey_date'] = monthly_stock_data['gvkey'].astype(str) + monthly_stock_data['date'].astype(str)\n",
    "print(monthly_stock_data['gvkey_date'].value_counts())\n",
    "# Oh no! data is not unique at gvkey & date\n",
    "\n",
    "# Some companies has listed their stock twice in market (at the same time)\n",
    "# For example, Carnival has a US listed stock (Ticker CCL) & they have listed their UK ADR too (ticker CUK)\n",
    "# This is not like classA & classB. Remember, I already took only 1 class for each company\n",
    "# Literally same company. Listed effectively the same company twice. Crazy!!!\n",
    "monthly_stock_data[monthly_stock_data['gvkey_date'] == '14162.01987-07-31'][['gvkey','date','cusip_6digit','TICKER','COMNAM',\n",
    "                                                                            'SHRCLS', 'company_mod']]\n",
    "\n",
    "# Very few such cases. Without investgating too much into it (because very few cases), let's take 1st cusip that appears\n",
    "monthly_stock_data = monthly_stock_data.sort_values(by = ['gvkey', 'date', 'cusip_6digit'], ascending = True)\n",
    "\n",
    "# Checking if gvkey's and cusip_6digit's match\n",
    "print(monthly_stock_data[monthly_stock_data['gvkey'].isin(monthly_stock_data['cusip_6digit'])].shape)\n",
    "# gvkeys and cusip 6 digit don't match\n",
    "# So, let's create a new unique identifier called ultimate id\n",
    "# when gvkey exists, ultimate id is gvkey. When gvkey is null, cusip 6digit becomes ultimate id\n",
    "monthly_stock_data['ultimate_id'] = monthly_stock_data['gvkey'].fillna(monthly_stock_data['cusip_6digit'])\n",
    "\n",
    "monthly_stock_data = monthly_stock_data.drop_duplicates(subset = ['ultimate_id', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating stock exchange by changing numerical codes\n",
    "monthly_stock_data['stock_exchange'] = np.where(monthly_stock_data['PRIMEXCH'] == 'N', 'NYSE', None)\n",
    "monthly_stock_data['stock_exchange'] = np.where(monthly_stock_data['PRIMEXCH'] == 'A', 'NYSE_MKT', monthly_stock_data['stock_exchange'])\n",
    "monthly_stock_data['stock_exchange'] = np.where(monthly_stock_data['PRIMEXCH'] == 'Q', 'NASDAQ', monthly_stock_data['stock_exchange'])\n",
    "monthly_stock_data['stock_exchange'] = np.where(monthly_stock_data['PRIMEXCH'] == 'R', 'ARCHA', monthly_stock_data['stock_exchange'])\n",
    "monthly_stock_data['stock_exchange'] = np.where(monthly_stock_data['PRIMEXCH'] == 'B', 'BATS', monthly_stock_data['stock_exchange'])\n",
    "monthly_stock_data['stock_exchange'] = np.where(monthly_stock_data['PRIMEXCH'] == 'X', 'OTHER', monthly_stock_data['stock_exchange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3935354, 39)\n",
      "3935354\n"
     ]
    }
   ],
   "source": [
    "monthly_stock_data['ultimate_id_date'] = monthly_stock_data['ultimate_id'].astype(str) + monthly_stock_data['date'].astype(str)\n",
    "\n",
    "print(monthly_stock_data.shape)\n",
    "print(len(monthly_stock_data['ultimate_id_date'].unique()))\n",
    "# The data is finally at ultimate id and date level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting important columns only (initial cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting important columns only (initial cut)\n",
    "\n",
    "# Keys and ids\n",
    "vimp_ids = ['ultimate_id', 'date', 'ultimate_id_date','vwretd','ewretd','gvkey','cusip_6digit','cusip_8digit']\n",
    "good_to_have_ids = ['COMNAM', 'CUSIP', 'NCUSIP']\n",
    "\n",
    "# Predictors\n",
    "vimp_preds = ['NAICS', 'PRC', 'VOL']\n",
    "good_to_have_columns = ['SHROUT','stock_exchange','SHRCD_Description', 'Mcap']\n",
    "\n",
    "# Target variable\n",
    "target = ['RET']\n",
    "\n",
    "monthly_stock_data = monthly_stock_data[vimp_ids + good_to_have_ids + vimp_preds + good_to_have_columns + target]\n",
    "\n",
    "# ALso, let's not forget to dedup the data\n",
    "monthly_stock_data = monthly_stock_data.drop_duplicates(subset = ['ultimate_id_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_stock_data = monthly_stock_data.loc[:,~monthly_stock_data.columns.duplicated()]\n",
    "# Can use this line if accidentally selected same column twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Industry names based on NAICS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Change Indicator    2181\n",
       "Code                  19\n",
       "Industry_name         19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAICS_2017_data = pd.read_excel(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\NAICS codes\\2017_NAICS_Structure.xlsx', header = None)\n",
    "# Reading in excel using pd.read_excel\n",
    "# header = None because the original file has no column names\n",
    "# The excel columns has actually only 3 columns. \n",
    "# But pd.read_excel is creating 6 columns. So, just selecting the first 3 columns\n",
    "NAICS_2017_data = NAICS_2017_data.iloc[:, 0:3]\n",
    "NAICS_2017_data.columns = ['Change Indicator', 'Code', 'Industry_name']\n",
    "NAICS_2017_data = NAICS_2017_data.iloc[3:]\n",
    "NAICS_2017_data['Code'].value_counts().max() # Using value_counts().max(), observing that each NAICS Code appears only once\n",
    "NAICS_2017_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The change column is pretty useless. But there are few empty rows with both Code & Title missing\n",
    "NAICS_2017_data = NAICS_2017_data.drop(['Change Indicator'],axis = 1)\n",
    "# Removing the rows with null values in 'Code' or 'INDEX ITEM DESCRIPTION' - Dropna subset\n",
    "NAICS_2017_data = NAICS_2017_data.dropna(subset = ['Code', 'Industry_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to merge NAICS codes from previous years too\n",
    "NAICS_2012_data = pd.read_excel(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\NAICS codes\\2012_NAICS_Index_File.xlsx')\n",
    "NAICS_2012_data = NAICS_2012_data.iloc[:, 0:2]\n",
    "NAICS_2012_data.rename(columns = {'NAICS12':'Code', 'INDEX ITEM DESCRIPTION': 'Industry_name'}, inplace = True)\n",
    "NAICS_2012_data = NAICS_2012_data.dropna(subset = ['Code', 'Industry_name'])\n",
    "\n",
    "NAICS_2007_data = pd.read_excel(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\NAICS codes\\2007_NAICS_Index_File.xlsx')\n",
    "NAICS_2007_data = NAICS_2007_data.iloc[:, 0:2]\n",
    "NAICS_2007_data.rename(columns = {'NAICS07':'Code', 'INDEX ITEM DESCRIPTION': 'Industry_name'}, inplace = True)\n",
    "NAICS_2007_data = NAICS_2007_data.dropna(subset = ['Code', 'Industry_name'])\n",
    "\n",
    "NAICS_2002_data = pd.read_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\NAICS codes\\2002_NAICS.txt', sep = '  ', delimiter = None)\n",
    "NAICS_2002_data = NAICS_2002_data.iloc[:, 0:2]\n",
    "NAICS_2002_data.rename(columns = {'Unnamed: 1': 'Industry_name'}, inplace = True)\n",
    "# NAICS_2002_data = NAICS_2002_data.drop(['2002 NAICS Title'],axis = 1) \n",
    "# No need of this line because selected only first 3 columns when reading in csv\n",
    "NAICS_2002_data = NAICS_2002_data.dropna(subset = ['Code', 'Industry_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Code             0\n",
       "Industry_name    0\n",
       "Year             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAICS_2017_data['Year'] = 2017\n",
    "NAICS_2012_data['Year'] = 2012\n",
    "NAICS_2007_data['Year'] = 2007\n",
    "NAICS_2002_data['Year'] = 2002\n",
    "\n",
    "All_years_NAICS_combined = pd.concat([NAICS_2017_data,NAICS_2012_data,NAICS_2007_data,NAICS_2002_data], axis = 0)\n",
    "All_years_NAICS_combined = All_years_NAICS_combined.sort_values(by=['Code','Year'],ascending = False)\n",
    "All_years_NAICS_combined = All_years_NAICS_combined.drop_duplicates(subset = ['Code'],keep = \"first\")\n",
    "All_years_NAICS_combined.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2794855"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to make sure that the format of join keys are the same on both sides\n",
    "monthly_stock_data['NAICS_str'] = monthly_stock_data['NAICS'].astype(str)\n",
    "All_years_NAICS_combined['Code'] = All_years_NAICS_combined['Code'].astype(str) + '.0'\n",
    "\n",
    "monthly_stock_data = pd.merge(monthly_stock_data,\n",
    "                              All_years_NAICS_combined[['Code','Industry_name']],\n",
    "                              left_on = 'NAICS_str',\n",
    "                              right_on = 'Code',\n",
    "                              how = 'left')\n",
    "\n",
    "# Checking if there are any NAICS codes which don't have a match on the right side data\n",
    "monthly_stock_data['Industry_name'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514191.0    1414\n",
       "513310.0    1115\n",
       "452110.0     906\n",
       "421690.0     862\n",
       "233210.0     861\n",
       "            ... \n",
       "331942.0       7\n",
       "332110.0       6\n",
       "514519.0       5\n",
       "235930.0       3\n",
       "481300.0       1\n",
       "Name: NAICS, Length: 99, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking which NAICS codes don't have a match in our NAICS industry file\n",
    "monthly_stock_data[((monthly_stock_data['Industry_name'].isnull()) & ~(monthly_stock_data['NAICS'].isnull()))]['NAICS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create prior 3 month % change for every row\n",
    "# But before I do that, I need to sort the dataframe on Permno & date columns\n",
    "\n",
    "# Realized that date is not in datetime format\n",
    "monthly_stock_data['date'].dtype # It's not datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stock_data['date'] = pd.to_datetime(monthly_stock_data['date']) #Converting date to datetime format\n",
    "# monthly_stock_data['date'].dt.month.value_counts() # Just to make sure datetime code has worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create prior 1M, 2M, 3M & 6M % change for every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stock_data = monthly_stock_data.sort_values(by=['ultimate_id','date'],ascending = True)\n",
    "\n",
    "# Using shift to shift the return column\n",
    "monthly_stock_data['RET'] = pd.to_numeric(monthly_stock_data['RET'], errors = 'coerce')\n",
    "monthly_stock_data['RET_shift1'] = monthly_stock_data['RET'].shift(1).astype('float64')\n",
    "monthly_stock_data['RET_shift2'] = monthly_stock_data['RET'].shift(2).astype('float64')\n",
    "monthly_stock_data['RET_shift3'] = monthly_stock_data['RET'].shift(3).astype('float64')\n",
    "monthly_stock_data['RET_shift4'] = monthly_stock_data['RET'].shift(4).astype('float64')\n",
    "monthly_stock_data['RET_shift5'] = monthly_stock_data['RET'].shift(5).astype('float64')\n",
    "monthly_stock_data['RET_shift6'] = monthly_stock_data['RET'].shift(6).astype('float64')\n",
    "\n",
    "monthly_stock_data['1M_RET'] = (1+monthly_stock_data['RET_shift1'])-1\n",
    "monthly_stock_data['2M_RET'] = (1+monthly_stock_data['RET_shift1'])*(1+monthly_stock_data['RET_shift2'])-1\n",
    "monthly_stock_data['3M_RET'] = (1+monthly_stock_data['RET_shift1'])*(1+monthly_stock_data['RET_shift2'])*(1+monthly_stock_data['RET_shift3'])-1\n",
    "monthly_stock_data['6M_RET'] = (1+monthly_stock_data['RET_shift1'])*(1+monthly_stock_data['RET_shift2'])*(1+monthly_stock_data['RET_shift3'])*(1+monthly_stock_data['RET_shift4'])*(1+monthly_stock_data['RET_shift5'])*(1+monthly_stock_data['RET_shift6'])-1\n",
    "\n",
    "# But I also need to make sure that I am not using some different stock returns to calculate prior returns for every row\n",
    "monthly_stock_data['ultimate_id_shift1'] = monthly_stock_data['ultimate_id'].shift(1)\n",
    "monthly_stock_data['ultimate_id_shift2'] = monthly_stock_data['ultimate_id'].shift(2)\n",
    "monthly_stock_data['ultimate_id_shift3'] = monthly_stock_data['ultimate_id'].shift(3)\n",
    "monthly_stock_data['ultimate_id_shift6'] = monthly_stock_data['ultimate_id'].shift(6)\n",
    "\n",
    "monthly_stock_data['gap_date_shift1'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(1)).dt.days\n",
    "monthly_stock_data['gap_date_shift2'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(2)).dt.days\n",
    "monthly_stock_data['gap_date_shift3'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(3)).dt.days\n",
    "monthly_stock_data['gap_date_shift6'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(6)).dt.days\n",
    "\n",
    "\n",
    "monthly_stock_data['1M_RET'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift1']) &\n",
    "                                        (monthly_stock_data['gap_date_shift1'].between(25, 35, inclusive=False))) , \n",
    "                                         monthly_stock_data['1M_RET'], None)\n",
    "monthly_stock_data['2M_RET'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift2']) &\n",
    "                                        (monthly_stock_data['gap_date_shift2'].between(53, 67, inclusive=False))) , \n",
    "                                         monthly_stock_data['2M_RET'], None)\n",
    "monthly_stock_data['3M_RET'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift3']) &\n",
    "                                        (monthly_stock_data['gap_date_shift3'].between(83, 97, inclusive=False))) , \n",
    "                                         monthly_stock_data['3M_RET'], None)\n",
    "# don't need the code below because the dataframe is sorted by Permno & date\n",
    "#monthly_stock_data['3M_RET'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift1']) & \n",
    "#                                          (monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift2']) & \n",
    "#                                          (monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift3'])), \n",
    "#                                         monthly_stock_data['3M_RET'], None)\n",
    "monthly_stock_data['6M_RET'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift6']) &\n",
    "                                        (monthly_stock_data['gap_date_shift6'].between(170 ,190, inclusive=False))) , \n",
    "                                         monthly_stock_data['6M_RET'], None)\n",
    "\n",
    "#monthly_stock_data[['ultimate_id','date','RET','RET_shift1','RET_shift2','RET_shift3','RET_shift4','RET_shift5','RET_shift6',\n",
    "#                    '1M_RET', '2M_RET', '3M_RET', '6M_RET']].to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\checking_all_months_return_code.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create future 1M, 2M, 3M & 6M % change for every row (Potential target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important points to remmember while cleaning final code\n",
    "#All variable names have to be standardized\n",
    "#concatinate code into relevant blocks(don't have too many blocks)\n",
    "#make sure lines which need to be executed first or above\n",
    "#please put heading for relevant sections(headings can be size 3 or size 6)\n",
    "\n",
    "monthly_stock_data = monthly_stock_data.sort_values(by=['ultimate_id','date'],ascending = True)\n",
    "\n",
    "monthly_stock_data['RET'] = pd.to_numeric(monthly_stock_data['RET'], errors = 'coerce')\n",
    "monthly_stock_data['RET']= monthly_stock_data['RET'].astype('float64')\n",
    "monthly_stock_data['RET_shiftnot1'] = monthly_stock_data['RET'].shift(-1)\n",
    "monthly_stock_data['RET_shiftnot2'] = monthly_stock_data['RET'].shift(-2)\n",
    "monthly_stock_data['RET_shiftnot3'] = monthly_stock_data['RET'].shift(-3)\n",
    "monthly_stock_data['RET_shiftnot4'] = monthly_stock_data['RET'].shift(-4)\n",
    "monthly_stock_data['RET_shiftnot5'] = monthly_stock_data['RET'].shift(-5)\n",
    "monthly_stock_data['RET_shiftnot6'] = monthly_stock_data['RET'].shift(-6)\n",
    "\n",
    "monthly_stock_data['ultimate_id_shiftnot1'] = monthly_stock_data['ultimate_id'].shift(-1)\n",
    "monthly_stock_data['ultimate_id_shiftnot2'] = monthly_stock_data['ultimate_id'].shift(-2)\n",
    "monthly_stock_data['ultimate_id_shiftnot5'] = monthly_stock_data['ultimate_id'].shift(-5)\n",
    "\n",
    "monthly_stock_data['gap_date_shiftnot1'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(-1)).dt.days\n",
    "monthly_stock_data['gap_date_shiftnot2'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(-2)).dt.days\n",
    "monthly_stock_data['gap_date_shiftnot5'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(-5)).dt.days\n",
    "\n",
    "\n",
    "monthly_stock_data['1M_neg_RET'] = (1+monthly_stock_data['RET'])-1\n",
    "monthly_stock_data['2M_neg_RET'] = (1+monthly_stock_data['1M_neg_RET'])*(1+monthly_stock_data['RET_shiftnot1'])-1\n",
    "monthly_stock_data['3M_neg_RET'] = (1+monthly_stock_data['2M_neg_RET'])*(1+monthly_stock_data['RET_shiftnot2'])-1\n",
    "monthly_stock_data['6M_neg_RET'] = (1+monthly_stock_data['3M_neg_RET'])*(1+monthly_stock_data['RET_shiftnot3'])*(1+monthly_stock_data['RET_shiftnot4'])*(1+monthly_stock_data['RET_shiftnot5'])-1\n",
    "\n",
    "monthly_stock_data['1M_neg_RET'] = np.where((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id']),\n",
    "                                              monthly_stock_data['1M_neg_RET'], None)\n",
    "monthly_stock_data['2M_neg_RET'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shiftnot1']) &\n",
    "                                             (monthly_stock_data['gap_date_shiftnot1'].between(-35,-25, inclusive=False))), \n",
    "                                              monthly_stock_data['2M_neg_RET'], None)\n",
    "monthly_stock_data['3M_neg_RET'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shiftnot2']) &\n",
    "                                             (monthly_stock_data['gap_date_shiftnot2'].between(-65,-55, inclusive=False))), \n",
    "                                              monthly_stock_data['3M_neg_RET'], None)\n",
    "monthly_stock_data['6M_neg_RET'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shiftnot5']) &\n",
    "                                             (monthly_stock_data['gap_date_shiftnot5'].between(-160,-140, inclusive=False))), \n",
    "                                              monthly_stock_data['6M_neg_RET'], None)\n",
    "\n",
    "#monthly_stock_data[['ultimate_id','date','RET','RET_shiftnot1','RET_shiftnot2','RET_shiftnot3','RET_shiftnot4','RET_shiftnot5','RET_shiftnot6',\n",
    "#                     '1M_neg_RET','2M_neg_RET', '3M_neg_RET', '6M_neg_RET']].to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\monthly_fututre_data.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating 1M,2M,3M and 6M vwretd and ewretd(both prev. and future) using RET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3935354, 56)\n",
      "(3935354, 72)\n"
     ]
    }
   ],
   "source": [
    "index_data = monthly_stock_data[['date','vwretd','ewretd']].drop_duplicates(subset = 'date', keep = \"last\")        \n",
    "index_data['vwretd_shift1'] = index_data['vwretd'].shift(1)\n",
    "index_data['vwretd_shift2'] = index_data['vwretd'].shift(2)\n",
    "index_data['vwretd_shift3'] = index_data['vwretd'].shift(3)\n",
    "index_data['vwretd_shift4'] = index_data['vwretd'].shift(4)\n",
    "index_data['vwretd_shift5'] = index_data['vwretd'].shift(5)\n",
    "index_data['vwretd_shift6'] = index_data['vwretd'].shift(6)\n",
    "\n",
    "index_data['vwretd_shiftnot1'] = index_data['vwretd'].shift(-1)\n",
    "index_data['vwretd_shiftnot2'] = index_data['vwretd'].shift(-2)\n",
    "index_data['vwretd_shiftnot3'] = index_data['vwretd'].shift(-3)\n",
    "index_data['vwretd_shiftnot4'] = index_data['vwretd'].shift(-4)\n",
    "index_data['vwretd_shiftnot5'] = index_data['vwretd'].shift(-5)\n",
    "index_data['vwretd_shiftnot6'] = index_data['vwretd'].shift(-6)\n",
    "\n",
    "index_data['ewretd_shift1'] = index_data['ewretd'].shift(1)\n",
    "index_data['ewretd_shift2'] = index_data['ewretd'].shift(2)\n",
    "index_data['ewretd_shift3'] = index_data['ewretd'].shift(3)\n",
    "index_data['ewretd_shift4'] = index_data['ewretd'].shift(4)\n",
    "index_data['ewretd_shift5'] = index_data['ewretd'].shift(5)\n",
    "index_data['ewretd_shift6'] = index_data['ewretd'].shift(6)\n",
    "\n",
    "index_data['ewretd_shiftnot1'] = index_data['ewretd'].shift(-1)\n",
    "index_data['ewretd_shiftnot2'] = index_data['ewretd'].shift(-2)\n",
    "index_data['ewretd_shiftnot3'] = index_data['ewretd'].shift(-3)\n",
    "index_data['ewretd_shiftnot4'] = index_data['ewretd'].shift(-4)\n",
    "index_data['ewretd_shiftnot5'] = index_data['ewretd'].shift(-5)\n",
    "index_data['ewretd_shiftnot6'] = index_data['ewretd'].shift(-6)\n",
    "\n",
    "\n",
    "index_data['vwretd_1M_prev'] = (1+index_data['vwretd_shift1'])-1\n",
    "index_data['vwretd_2M_prev'] = (1+index_data['vwretd_1M_prev'])*(1+index_data['vwretd_shift2'])-1\n",
    "index_data['vwretd_3M_prev'] = (1+index_data['vwretd_2M_prev'])*(1+index_data['vwretd_shift3'])-1\n",
    "index_data['vwretd_6M_prev'] = (1+index_data['vwretd_3M_prev'])*(1+index_data['vwretd_shift4'])*(1+index_data['vwretd_shift5'])*(1+index_data['vwretd_shift6'])-1\n",
    "\n",
    "\n",
    "index_data['vwretd_1M_futr'] = (1+index_data['vwretd'])-1\n",
    "index_data['vwretd_2M_futr'] = (1+index_data['vwretd_1M_futr'])*(1+index_data['vwretd_shiftnot1'])-1\n",
    "index_data['vwretd_3M_futr'] = (1+index_data['vwretd_2M_futr'])*(1+index_data['vwretd_shiftnot2'])-1\n",
    "index_data['vwretd_6M_futr'] = (1+index_data['vwretd_3M_futr'])*(1+index_data['vwretd_shiftnot3'])*(1+index_data['vwretd_shiftnot4'])*(1+index_data['vwretd_shiftnot5'])-1\n",
    "\n",
    "index_data['ewretd_1M_prev'] = (1+index_data['ewretd_shift1'])-1\n",
    "index_data['ewretd_2M_prev'] = (1+index_data['ewretd_1M_prev'])*(1+index_data['ewretd_shift2'])-1\n",
    "index_data['ewretd_3M_prev'] = (1+index_data['ewretd_2M_prev'])*(1+index_data['ewretd_shift3'])-1\n",
    "index_data['ewretd_6M_prev'] = (1+index_data['ewretd_3M_prev'])*(1+index_data['ewretd_shift4'])*(1+index_data['ewretd_shift5'])*(1+index_data['ewretd_shift6'])-1\n",
    "\n",
    "\n",
    "index_data['ewretd_1M_futr'] = (1+index_data['ewretd'])-1\n",
    "index_data['ewretd_2M_futr'] = (1+index_data['ewretd_1M_futr'])*(1+index_data['ewretd_shiftnot1'])-1\n",
    "index_data['ewretd_3M_futr'] = (1+index_data['ewretd_2M_futr'])*(1+index_data['ewretd_shiftnot2'])-1\n",
    "index_data['ewretd_6M_futr'] = (1+index_data['ewretd_3M_futr'])*(1+index_data['ewretd_shiftnot3'])*(1+index_data['ewretd_shiftnot4'])*(1+index_data['ewretd_shiftnot5'])-1\n",
    "\n",
    "print(monthly_stock_data.shape)\n",
    "monthly_stock_data = pd.merge(monthly_stock_data,\n",
    "                             index_data[['date','vwretd_1M_prev',\n",
    "                                        'vwretd_2M_prev','vwretd_3M_prev','vwretd_6M_prev','vwretd_1M_futr','vwretd_2M_futr',\n",
    "                                        'vwretd_3M_futr','vwretd_6M_futr','ewretd_1M_prev','ewretd_2M_prev','ewretd_3M_prev',\n",
    "                                        'ewretd_6M_prev','ewretd_1M_futr','ewretd_2M_futr','ewretd_3M_futr','ewretd_6M_futr']],\n",
    "                             left_on = 'date',\n",
    "                             right_on = 'date',\n",
    "                             how = 'left')\n",
    "print(monthly_stock_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "monthly_stock_data['RET_vwretd_1M_prev'] = monthly_stock_data['1M_RET'] - monthly_stock_data['vwretd_1M_prev']\n",
    "monthly_stock_data['RET_vwretd_2M_prev'] = monthly_stock_data['2M_RET'] - monthly_stock_data['vwretd_2M_prev']\n",
    "monthly_stock_data['RET_vwretd_3M_prev'] = monthly_stock_data['3M_RET'] - monthly_stock_data['vwretd_3M_prev']\n",
    "monthly_stock_data['RET_vwretd_6M_prev'] = monthly_stock_data['6M_RET'] - monthly_stock_data['vwretd_6M_prev']\n",
    "\n",
    "\n",
    "monthly_stock_data['RET_vwretd_1M_futr'] = monthly_stock_data['1M_neg_RET'] - monthly_stock_data['vwretd_1M_futr']\n",
    "monthly_stock_data['RET_vwretd_2M_futr'] = monthly_stock_data['2M_neg_RET'] - monthly_stock_data['vwretd_2M_futr']\n",
    "monthly_stock_data['RET_vwretd_3M_futr'] = monthly_stock_data['3M_neg_RET'] - monthly_stock_data['vwretd_3M_futr']\n",
    "monthly_stock_data['RET_vwretd_6M_futr'] = monthly_stock_data['6M_neg_RET'] - monthly_stock_data['vwretd_6M_futr']\n",
    "\n",
    "\n",
    "monthly_stock_data['RET_ewretd_1M_prev'] = monthly_stock_data['1M_RET'] - monthly_stock_data['ewretd_1M_prev']\n",
    "monthly_stock_data['RET_ewretd_2M_prev'] = monthly_stock_data['2M_RET'] - monthly_stock_data['ewretd_2M_prev']\n",
    "monthly_stock_data['RET_ewretd_3M_prev'] = monthly_stock_data['3M_RET'] - monthly_stock_data['ewretd_3M_prev']\n",
    "monthly_stock_data['RET_ewretd_6M_prev'] = monthly_stock_data['6M_RET'] - monthly_stock_data['ewretd_6M_prev']\n",
    "\n",
    "\n",
    "monthly_stock_data['RET_ewretd_1M_futr'] = monthly_stock_data['1M_neg_RET'] - monthly_stock_data['ewretd_1M_futr']\n",
    "monthly_stock_data['RET_ewretd_2M_futr'] = monthly_stock_data['2M_neg_RET'] - monthly_stock_data['ewretd_2M_futr']\n",
    "monthly_stock_data['RET_ewretd_3M_futr'] = monthly_stock_data['3M_neg_RET'] - monthly_stock_data['ewretd_3M_futr']\n",
    "monthly_stock_data['RET_ewretd_6M_futr'] = monthly_stock_data['6M_neg_RET'] - monthly_stock_data['ewretd_6M_futr']\n",
    "\n",
    "# monthly_stock_data[0:1000].to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\new file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating volume relativity index 1M and 2M (measuring how big or small volume is relative to long term average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stock_data = monthly_stock_data.sort_values(by=['ultimate_id','date'],ascending = True)\n",
    "monthly_stock_data['Vol_shrout_shift1'] = (monthly_stock_data['VOL']/monthly_stock_data['SHROUT']).shift(1)\n",
    "monthly_stock_data['Vol_shrout_shift2'] = (monthly_stock_data['VOL']/monthly_stock_data['SHROUT']).shift(2)\n",
    "monthly_stock_data['Vol_shrout_shift3'] = (monthly_stock_data['VOL']/monthly_stock_data['SHROUT']).shift(3)\n",
    "monthly_stock_data['Vol_shrout_shift4'] = (monthly_stock_data['VOL']/monthly_stock_data['SHROUT']).shift(4)\n",
    "monthly_stock_data['Vol_shrout_shift5'] = (monthly_stock_data['VOL']/monthly_stock_data['SHROUT']).shift(5)\n",
    "monthly_stock_data['Vol_shrout_shift6'] = (monthly_stock_data['VOL']/monthly_stock_data['SHROUT']).shift(6)\n",
    "monthly_stock_data['Vol_shrout_shift7'] = (monthly_stock_data['VOL']/monthly_stock_data['SHROUT']).shift(7)\n",
    "monthly_stock_data['Vol_shrout_shift8'] = (monthly_stock_data['VOL']/monthly_stock_data['SHROUT']).shift(8)\n",
    "\n",
    "monthly_stock_data['Vol_6M_avg'] = ((monthly_stock_data['Vol_shrout_shift2']+monthly_stock_data['Vol_shrout_shift3']+monthly_stock_data['Vol_shrout_shift4']+monthly_stock_data['Vol_shrout_shift5']+monthly_stock_data['Vol_shrout_shift6']+monthly_stock_data['Vol_shrout_shift7']))/6\n",
    "monthly_stock_data['Vol_1M_6M_index'] = (monthly_stock_data['Vol_shrout_shift1']/monthly_stock_data['Vol_6M_avg'])\n",
    "\n",
    "monthly_stock_data['ultimate_id_shift1'] = monthly_stock_data['ultimate_id'].shift(1)\n",
    "monthly_stock_data['ultimate_id_shift7'] = monthly_stock_data['ultimate_id'].shift(7)\n",
    "\n",
    "monthly_stock_data['gap_date_shift1'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(1)).dt.days\n",
    "monthly_stock_data['gap_date_shift7'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(7)).dt.days\n",
    "\n",
    "\n",
    "monthly_stock_data['Vol_1M_6M_index'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift7']) &\n",
    "                                                  (monthly_stock_data['gap_date_shift7'].between(200, 220, inclusive=False))), \n",
    "                                                   monthly_stock_data['Vol_1M_6M_index'], None)\n",
    "\n",
    "#monthly_stock_data[['ultimate_id','ultimate_id_Vol_shift1','ultimate_id_Vol_shift7','date','VOL','Vol_shrout_shift1','Vol_shrout_shift2','Vol_shrout_shift3','Vol_shrout_shift4','Vol_shrout_shift5','Vol_shrout_shift6','Vol_shrout_shift7',\n",
    "#                    'Vol_6M_avg', 'Vol_1M_index']].to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\Vol_1M_check.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stock_data['Vol_of_2M_6M_avg'] = (monthly_stock_data['Vol_shrout_shift3']+monthly_stock_data['Vol_shrout_shift4']+monthly_stock_data['Vol_shrout_shift5']+monthly_stock_data['Vol_shrout_shift6']+monthly_stock_data['Vol_shrout_shift7']+monthly_stock_data['Vol_shrout_shift8'])/6\n",
    "monthly_stock_data['Vol_2M_avg'] = (monthly_stock_data['Vol_shrout_shift1']+monthly_stock_data['Vol_shrout_shift2'])/2\n",
    "monthly_stock_data['Vol_2M_6M_index'] = monthly_stock_data['Vol_2M_avg']/monthly_stock_data['Vol_of_2M_6M_avg']\n",
    "\n",
    "monthly_stock_data['ultimate_id_shift8'] = monthly_stock_data['ultimate_id'].shift(8)\n",
    "\n",
    "\n",
    "monthly_stock_data['gap_date_shift8'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(8)).dt.days\n",
    "\n",
    "\n",
    "monthly_stock_data['Vol_2M_6M_index'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift8']) &\n",
    "                                                  (monthly_stock_data['gap_date_shift8'].between(230 ,250, inclusive=False))), \n",
    "                                                   monthly_stock_data['Vol_2M_6M_index'], None)\n",
    "\n",
    "#monthly_stock_data[['ultimate_id','ultimate_id_Vol_2M_shift1','ultimate_id_Vol_2M_shift8','date','VOL','Vol_shrout_shift1','Vol_shrout_shift2','Vol_shrout_shift3','Vol_shrout_shift4','Vol_shrout_shift5','Vol_shrout_shift6','Vol_shrout_shift7','Vol_shrout_shift8',\n",
    "#                    'Vol_of_2M_6M_avg', 'Vol_2M_avg','Vol_2M_index']].to_csv(r'C:\\Users\\joshn\\Downloads\\MS in Data Science\\Stock market analysis\\Vol_2M_check.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating volume of shares 6M average for a task in a daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's get prior 6M vol avg from monthly data\n",
    "# For all jan rows in daily data, I will merge Jun-Dec 6M vol average\n",
    "\n",
    "monthly_stock_data = monthly_stock_data.sort_values(by=['ultimate_id','date'],ascending = True)\n",
    "monthly_stock_data['Vol_shrout_6M_avg'] = (monthly_stock_data['Vol_shrout_shift1']+monthly_stock_data['Vol_shrout_shift2']+monthly_stock_data['Vol_shrout_shift3']+monthly_stock_data['Vol_shrout_shift4']+monthly_stock_data['Vol_shrout_shift5']+monthly_stock_data['Vol_shrout_shift6'])/6\n",
    "monthly_stock_data['ultimate_id_shift6'] = monthly_stock_data['ultimate_id'].shift(6)\n",
    "monthly_stock_data['gap_date_shift6'] = (monthly_stock_data['date'] - monthly_stock_data['date'].shift(6)).dt.days\n",
    "monthly_stock_data['Vol_shrout_6M_avg'] = np.where(((monthly_stock_data['ultimate_id'] == monthly_stock_data['ultimate_id_shift6']) &\n",
    "                                                    (monthly_stock_data['gap_date_shift6'].between(170 ,190, inclusive=False))), \n",
    "                                                   monthly_stock_data['Vol_shrout_6M_avg'], None)\n",
    "\n",
    "# extracting month and year from date column because I need to join monthly and daily data using month and year as keys.\n",
    "monthly_stock_data['date'] = pd.to_datetime(monthly_stock_data['date']) \n",
    "monthly_stock_data['Month'] = monthly_stock_data['date'].dt.month\n",
    "monthly_stock_data['Year'] = monthly_stock_data['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stock_data[['ultimate_id','gvkey','cusip_6digit','cusip_8digit','Month','Year',\n",
    "                    'Vol_shrout_6M_avg']].to_csv(r'Vol_shrout_6M_avg_from monthly_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Monthly stock data & Compustat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 2)\n",
      "(54, 3)\n",
      "(54, 3)\n",
      "(54, 4)\n",
      "(54, 4)\n",
      "(54, 5)\n",
      "(54, 5)\n",
      "(54, 6)\n"
     ]
    }
   ],
   "source": [
    "# Looking at match rates between CRSP and Compustat data\n",
    "monthly_stock_data['date'] = pd.to_datetime(monthly_stock_data['date'])\n",
    "monthly_stock_data['Year'] = monthly_stock_data['date'].dt.year\n",
    "\n",
    "# Getting number of rows by year in each dataframe\n",
    "evaluating_cusip_merge = pd.DataFrame(monthly_stock_data[monthly_stock_data['SHRCD_Description'].isin(shrcd_to_consider)])['Year'].value_counts().reset_index()\n",
    "evaluating_cusip_merge.columns = ['Year', 'Num_of_rows']\n",
    "\n",
    "# Getting #rows where CUSIP 6digit's don't match (across all years in compustat data)\n",
    "cusip_quarter = comp_data['cusip_6digit'].unique()\n",
    "monthly_stock_data['comp_cusip_both_quarter_monthly'] = np.where(monthly_stock_data['cusip_6digit'].isin(cusip_quarter), 0, 1)\n",
    "num_cusip_qrt_mnt = pd.DataFrame(monthly_stock_data[monthly_stock_data['SHRCD_Description'].isin(shrcd_to_consider)].groupby('Year')['comp_cusip_both_quarter_monthly'].sum()).reset_index()\n",
    "num_cusip_qrt_mnt.columns = ['Year', 'Num_cusip6_no_match']\n",
    "\n",
    "print(evaluating_cusip_merge.shape)\n",
    "evaluating_cusip_merge = pd.merge(evaluating_cusip_merge,\n",
    "                                  num_cusip_qrt_mnt,\n",
    "                                  left_on = 'Year',\n",
    "                                  right_on = 'Year',\n",
    "                                  how = 'left')\n",
    "print(evaluating_cusip_merge.shape)\n",
    "\n",
    "# Getting #rows where CUSIP 8digit's don't match (across all years in compustat data)\n",
    "cusip_quarter = comp_data['cusip_8digit'].unique()\n",
    "monthly_stock_data['comp_cusip_both_quarter_monthly'] = np.where(monthly_stock_data['cusip_8digit'].isin(cusip_quarter), 0, 1)\n",
    "num_cusip_qrt_mnt = pd.DataFrame(monthly_stock_data[monthly_stock_data['SHRCD_Description'].isin(shrcd_to_consider)].groupby('Year')['comp_cusip_both_quarter_monthly'].sum()).reset_index()\n",
    "num_cusip_qrt_mnt.columns = ['Year', 'Num_cusip8_no_match']\n",
    "\n",
    "print(evaluating_cusip_merge.shape)\n",
    "evaluating_cusip_merge = pd.merge(evaluating_cusip_merge,\n",
    "                                  num_cusip_qrt_mnt,\n",
    "                                  left_on = 'Year',\n",
    "                                  right_on = 'Year',\n",
    "                                  how = 'left')\n",
    "print(evaluating_cusip_merge.shape)\n",
    "\n",
    "# Getting #rows where CUSIP 6 digit's don't match (across all years in compustat data)\n",
    "comp_data['quarter_end_date'] = pd.to_datetime(comp_data['quarter_end_date'])\n",
    "comp_data['calendar_year'] = comp_data['quarter_end_date'].dt.year\n",
    "\n",
    "yearly_cusip_mismatch = pd.DataFrame(None) # better to have a null df, It will be easy to use during iterations.\n",
    "for i in monthly_stock_data['Year'].unique():\n",
    "    cusip_quarter_i = comp_data[comp_data['calendar_year'] == i]['cusip_6digit'].unique()\n",
    "    yearly_mismatch_num = len(monthly_stock_data[(monthly_stock_data['Year'] == i) & \n",
    "                                                 (monthly_stock_data['SHRCD_Description'].isin(shrcd_to_consider)) &\n",
    "                                                 (~monthly_stock_data['cusip_6digit'].isin(cusip_quarter_i))])\n",
    "    num_df = pd.DataFrame([[i,yearly_mismatch_num]], columns=['Year', 'Num_cusip6_yearly_no_match'])\n",
    "#    print(num_df)\n",
    "    yearly_cusip_mismatch = pd.concat([yearly_cusip_mismatch , num_df], axis = 0)\n",
    "    \n",
    "print(evaluating_cusip_merge.shape)\n",
    "evaluating_cusip_merge = pd.merge(evaluating_cusip_merge,\n",
    "                                  yearly_cusip_mismatch,\n",
    "                                  left_on = 'Year',\n",
    "                                  right_on = 'Year',\n",
    "                                  how = 'left')\n",
    "print(evaluating_cusip_merge.shape)\n",
    "\n",
    "# Getting #rows where CUSIP 8 digit's don't match (across all years in compustat data)\n",
    "yearly_cusip_mismatch = pd.DataFrame(None) # better to have a null df, It will be easy to use during iterations.\n",
    "for i in monthly_stock_data['Year'].unique():\n",
    "    cusip_quarter_i = comp_data[comp_data['calendar_year'] == i]['cusip_8digit'].unique()\n",
    "    yearly_mismatch_num = len(monthly_stock_data[(monthly_stock_data['Year'] == i) & \n",
    "                                                 (monthly_stock_data['SHRCD_Description'].isin(shrcd_to_consider)) &\n",
    "                                                 (~monthly_stock_data['cusip_8digit'].isin(cusip_quarter_i))])\n",
    "    num_df = pd.DataFrame([[i,yearly_mismatch_num]], columns=['Year', 'Num_cusip8_yearly_no_match'])\n",
    "#    print(num_df)\n",
    "    yearly_cusip_mismatch = pd.concat([yearly_cusip_mismatch , num_df], axis = 0)\n",
    "    \n",
    "print(evaluating_cusip_merge.shape)\n",
    "evaluating_cusip_merge = pd.merge(evaluating_cusip_merge,\n",
    "                                  yearly_cusip_mismatch,\n",
    "                                  left_on = 'Year',\n",
    "                                  right_on = 'Year',\n",
    "                                  how = 'left')\n",
    "print(evaluating_cusip_merge.shape)\n",
    "\n",
    "evaluating_cusip_merge = evaluating_cusip_merge.sort_values(by=['Year'],ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Num_of_rows</th>\n",
       "      <th>Num_cusip6_no_match</th>\n",
       "      <th>Num_cusip8_no_match</th>\n",
       "      <th>Num_cusip6_yearly_no_match</th>\n",
       "      <th>Num_cusip8_yearly_no_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1967</td>\n",
       "      <td>26313</td>\n",
       "      <td>7307</td>\n",
       "      <td>9163</td>\n",
       "      <td>9865</td>\n",
       "      <td>11038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1968</td>\n",
       "      <td>26392</td>\n",
       "      <td>6063</td>\n",
       "      <td>7848</td>\n",
       "      <td>8046</td>\n",
       "      <td>9312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1969</td>\n",
       "      <td>27318</td>\n",
       "      <td>5263</td>\n",
       "      <td>7008</td>\n",
       "      <td>7275</td>\n",
       "      <td>8596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1970</td>\n",
       "      <td>28442</td>\n",
       "      <td>4907</td>\n",
       "      <td>6685</td>\n",
       "      <td>7032</td>\n",
       "      <td>8395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1971</td>\n",
       "      <td>29407</td>\n",
       "      <td>4880</td>\n",
       "      <td>6672</td>\n",
       "      <td>6973</td>\n",
       "      <td>8360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1972</td>\n",
       "      <td>36911</td>\n",
       "      <td>7821</td>\n",
       "      <td>9837</td>\n",
       "      <td>12136</td>\n",
       "      <td>13620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1973</td>\n",
       "      <td>66109</td>\n",
       "      <td>20830</td>\n",
       "      <td>23752</td>\n",
       "      <td>36573</td>\n",
       "      <td>38294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1974</td>\n",
       "      <td>62383</td>\n",
       "      <td>17363</td>\n",
       "      <td>20235</td>\n",
       "      <td>32692</td>\n",
       "      <td>34470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1975</td>\n",
       "      <td>60709</td>\n",
       "      <td>15565</td>\n",
       "      <td>18444</td>\n",
       "      <td>31056</td>\n",
       "      <td>32853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1976</td>\n",
       "      <td>60674</td>\n",
       "      <td>14849</td>\n",
       "      <td>17776</td>\n",
       "      <td>31189</td>\n",
       "      <td>32994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1977</td>\n",
       "      <td>60371</td>\n",
       "      <td>14095</td>\n",
       "      <td>17044</td>\n",
       "      <td>31220</td>\n",
       "      <td>33056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1978</td>\n",
       "      <td>59103</td>\n",
       "      <td>12867</td>\n",
       "      <td>15832</td>\n",
       "      <td>30515</td>\n",
       "      <td>32376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1979</td>\n",
       "      <td>58234</td>\n",
       "      <td>11729</td>\n",
       "      <td>14758</td>\n",
       "      <td>30321</td>\n",
       "      <td>32236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1980</td>\n",
       "      <td>59001</td>\n",
       "      <td>11071</td>\n",
       "      <td>14262</td>\n",
       "      <td>31474</td>\n",
       "      <td>33440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1981</td>\n",
       "      <td>63184</td>\n",
       "      <td>11004</td>\n",
       "      <td>14424</td>\n",
       "      <td>17005</td>\n",
       "      <td>19918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1982</td>\n",
       "      <td>65050</td>\n",
       "      <td>10570</td>\n",
       "      <td>14071</td>\n",
       "      <td>13837</td>\n",
       "      <td>17025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1983</td>\n",
       "      <td>69869</td>\n",
       "      <td>11279</td>\n",
       "      <td>15154</td>\n",
       "      <td>15282</td>\n",
       "      <td>18855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1984</td>\n",
       "      <td>75400</td>\n",
       "      <td>12215</td>\n",
       "      <td>16371</td>\n",
       "      <td>16137</td>\n",
       "      <td>19959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1985</td>\n",
       "      <td>75011</td>\n",
       "      <td>11839</td>\n",
       "      <td>16068</td>\n",
       "      <td>15003</td>\n",
       "      <td>18963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1986</td>\n",
       "      <td>76840</td>\n",
       "      <td>11691</td>\n",
       "      <td>15943</td>\n",
       "      <td>14411</td>\n",
       "      <td>18502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1987</td>\n",
       "      <td>81068</td>\n",
       "      <td>12393</td>\n",
       "      <td>16745</td>\n",
       "      <td>14698</td>\n",
       "      <td>18887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1988</td>\n",
       "      <td>80452</td>\n",
       "      <td>12017</td>\n",
       "      <td>16099</td>\n",
       "      <td>14287</td>\n",
       "      <td>18231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1989</td>\n",
       "      <td>77483</td>\n",
       "      <td>10997</td>\n",
       "      <td>14834</td>\n",
       "      <td>12907</td>\n",
       "      <td>16636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1990</td>\n",
       "      <td>75733</td>\n",
       "      <td>10157</td>\n",
       "      <td>13856</td>\n",
       "      <td>11849</td>\n",
       "      <td>15453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1991</td>\n",
       "      <td>74725</td>\n",
       "      <td>9161</td>\n",
       "      <td>12803</td>\n",
       "      <td>10382</td>\n",
       "      <td>13984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1992</td>\n",
       "      <td>76757</td>\n",
       "      <td>8560</td>\n",
       "      <td>12113</td>\n",
       "      <td>9714</td>\n",
       "      <td>13230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1993</td>\n",
       "      <td>80913</td>\n",
       "      <td>7927</td>\n",
       "      <td>11545</td>\n",
       "      <td>8920</td>\n",
       "      <td>12490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1994</td>\n",
       "      <td>87976</td>\n",
       "      <td>8277</td>\n",
       "      <td>12211</td>\n",
       "      <td>9343</td>\n",
       "      <td>13227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1995</td>\n",
       "      <td>90882</td>\n",
       "      <td>8527</td>\n",
       "      <td>12479</td>\n",
       "      <td>9986</td>\n",
       "      <td>13895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996</td>\n",
       "      <td>96811</td>\n",
       "      <td>9472</td>\n",
       "      <td>13506</td>\n",
       "      <td>11424</td>\n",
       "      <td>15394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997</td>\n",
       "      <td>100689</td>\n",
       "      <td>9782</td>\n",
       "      <td>13920</td>\n",
       "      <td>12019</td>\n",
       "      <td>16057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>99325</td>\n",
       "      <td>9188</td>\n",
       "      <td>13295</td>\n",
       "      <td>11341</td>\n",
       "      <td>15352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999</td>\n",
       "      <td>93943</td>\n",
       "      <td>7821</td>\n",
       "      <td>11722</td>\n",
       "      <td>9483</td>\n",
       "      <td>13333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>91853</td>\n",
       "      <td>7360</td>\n",
       "      <td>11234</td>\n",
       "      <td>9137</td>\n",
       "      <td>12971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2001</td>\n",
       "      <td>84623</td>\n",
       "      <td>6193</td>\n",
       "      <td>9776</td>\n",
       "      <td>7991</td>\n",
       "      <td>11521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2002</td>\n",
       "      <td>77499</td>\n",
       "      <td>5342</td>\n",
       "      <td>8432</td>\n",
       "      <td>6921</td>\n",
       "      <td>9948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2003</td>\n",
       "      <td>72200</td>\n",
       "      <td>4509</td>\n",
       "      <td>7126</td>\n",
       "      <td>5974</td>\n",
       "      <td>8533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2004</td>\n",
       "      <td>70161</td>\n",
       "      <td>4238</td>\n",
       "      <td>6619</td>\n",
       "      <td>5703</td>\n",
       "      <td>8058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2005</td>\n",
       "      <td>69247</td>\n",
       "      <td>4051</td>\n",
       "      <td>6281</td>\n",
       "      <td>5446</td>\n",
       "      <td>7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2006</td>\n",
       "      <td>68133</td>\n",
       "      <td>3822</td>\n",
       "      <td>5904</td>\n",
       "      <td>5223</td>\n",
       "      <td>7240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2007</td>\n",
       "      <td>66988</td>\n",
       "      <td>3497</td>\n",
       "      <td>5497</td>\n",
       "      <td>4986</td>\n",
       "      <td>6920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2008</td>\n",
       "      <td>64056</td>\n",
       "      <td>3017</td>\n",
       "      <td>4883</td>\n",
       "      <td>4434</td>\n",
       "      <td>6252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2009</td>\n",
       "      <td>59985</td>\n",
       "      <td>2443</td>\n",
       "      <td>4042</td>\n",
       "      <td>3703</td>\n",
       "      <td>5248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2010</td>\n",
       "      <td>57993</td>\n",
       "      <td>2139</td>\n",
       "      <td>3495</td>\n",
       "      <td>3225</td>\n",
       "      <td>4544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2011</td>\n",
       "      <td>56459</td>\n",
       "      <td>1875</td>\n",
       "      <td>3105</td>\n",
       "      <td>2967</td>\n",
       "      <td>4157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2012</td>\n",
       "      <td>54959</td>\n",
       "      <td>1732</td>\n",
       "      <td>2892</td>\n",
       "      <td>2740</td>\n",
       "      <td>3862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2013</td>\n",
       "      <td>53922</td>\n",
       "      <td>1502</td>\n",
       "      <td>2604</td>\n",
       "      <td>2418</td>\n",
       "      <td>3495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2014</td>\n",
       "      <td>55047</td>\n",
       "      <td>1501</td>\n",
       "      <td>2544</td>\n",
       "      <td>2428</td>\n",
       "      <td>3460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2015</td>\n",
       "      <td>55626</td>\n",
       "      <td>1502</td>\n",
       "      <td>2522</td>\n",
       "      <td>2505</td>\n",
       "      <td>3502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2016</td>\n",
       "      <td>54266</td>\n",
       "      <td>1301</td>\n",
       "      <td>2203</td>\n",
       "      <td>2360</td>\n",
       "      <td>3237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2017</td>\n",
       "      <td>53472</td>\n",
       "      <td>1210</td>\n",
       "      <td>2011</td>\n",
       "      <td>2052</td>\n",
       "      <td>2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018</td>\n",
       "      <td>54082</td>\n",
       "      <td>1304</td>\n",
       "      <td>2023</td>\n",
       "      <td>1893</td>\n",
       "      <td>2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2019</td>\n",
       "      <td>54661</td>\n",
       "      <td>1441</td>\n",
       "      <td>2120</td>\n",
       "      <td>1974</td>\n",
       "      <td>2641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020</td>\n",
       "      <td>55519</td>\n",
       "      <td>1803</td>\n",
       "      <td>2393</td>\n",
       "      <td>2186</td>\n",
       "      <td>2773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  Num_of_rows  Num_cusip6_no_match  Num_cusip8_no_match  \\\n",
       "53  1967        26313                 7307                 9163   \n",
       "52  1968        26392                 6063                 7848   \n",
       "51  1969        27318                 5263                 7008   \n",
       "50  1970        28442                 4907                 6685   \n",
       "49  1971        29407                 4880                 6672   \n",
       "48  1972        36911                 7821                 9837   \n",
       "25  1973        66109                20830                23752   \n",
       "29  1974        62383                17363                20235   \n",
       "30  1975        60709                15565                18444   \n",
       "31  1976        60674                14849                17776   \n",
       "32  1977        60371                14095                17044   \n",
       "34  1978        59103                12867                15832   \n",
       "36  1979        58234                11729                14758   \n",
       "35  1980        59001                11071                14262   \n",
       "28  1981        63184                11004                14424   \n",
       "26  1982        65050                10570                14071   \n",
       "21  1983        69869                11279                15154   \n",
       "16  1984        75400                12215                16371   \n",
       "17  1985        75011                11839                16068   \n",
       "13  1986        76840                11691                15943   \n",
       "8   1987        81068                12393                16745   \n",
       "10  1988        80452                12017                16099   \n",
       "12  1989        77483                10997                14834   \n",
       "15  1990        75733                10157                13856   \n",
       "18  1991        74725                 9161                12803   \n",
       "14  1992        76757                 8560                12113   \n",
       "9   1993        80913                 7927                11545   \n",
       "6   1994        87976                 8277                12211   \n",
       "5   1995        90882                 8527                12479   \n",
       "2   1996        96811                 9472                13506   \n",
       "0   1997       100689                 9782                13920   \n",
       "1   1998        99325                 9188                13295   \n",
       "3   1999        93943                 7821                11722   \n",
       "4   2000        91853                 7360                11234   \n",
       "7   2001        84623                 6193                 9776   \n",
       "11  2002        77499                 5342                 8432   \n",
       "19  2003        72200                 4509                 7126   \n",
       "20  2004        70161                 4238                 6619   \n",
       "22  2005        69247                 4051                 6281   \n",
       "23  2006        68133                 3822                 5904   \n",
       "24  2007        66988                 3497                 5497   \n",
       "27  2008        64056                 3017                 4883   \n",
       "33  2009        59985                 2443                 4042   \n",
       "37  2010        57993                 2139                 3495   \n",
       "38  2011        56459                 1875                 3105   \n",
       "42  2012        54959                 1732                 2892   \n",
       "46  2013        53922                 1502                 2604   \n",
       "41  2014        55047                 1501                 2544   \n",
       "39  2015        55626                 1502                 2522   \n",
       "44  2016        54266                 1301                 2203   \n",
       "47  2017        53472                 1210                 2011   \n",
       "45  2018        54082                 1304                 2023   \n",
       "43  2019        54661                 1441                 2120   \n",
       "40  2020        55519                 1803                 2393   \n",
       "\n",
       "    Num_cusip6_yearly_no_match  Num_cusip8_yearly_no_match  \n",
       "53                        9865                       11038  \n",
       "52                        8046                        9312  \n",
       "51                        7275                        8596  \n",
       "50                        7032                        8395  \n",
       "49                        6973                        8360  \n",
       "48                       12136                       13620  \n",
       "25                       36573                       38294  \n",
       "29                       32692                       34470  \n",
       "30                       31056                       32853  \n",
       "31                       31189                       32994  \n",
       "32                       31220                       33056  \n",
       "34                       30515                       32376  \n",
       "36                       30321                       32236  \n",
       "35                       31474                       33440  \n",
       "28                       17005                       19918  \n",
       "26                       13837                       17025  \n",
       "21                       15282                       18855  \n",
       "16                       16137                       19959  \n",
       "17                       15003                       18963  \n",
       "13                       14411                       18502  \n",
       "8                        14698                       18887  \n",
       "10                       14287                       18231  \n",
       "12                       12907                       16636  \n",
       "15                       11849                       15453  \n",
       "18                       10382                       13984  \n",
       "14                        9714                       13230  \n",
       "9                         8920                       12490  \n",
       "6                         9343                       13227  \n",
       "5                         9986                       13895  \n",
       "2                        11424                       15394  \n",
       "0                        12019                       16057  \n",
       "1                        11341                       15352  \n",
       "3                         9483                       13333  \n",
       "4                         9137                       12971  \n",
       "7                         7991                       11521  \n",
       "11                        6921                        9948  \n",
       "19                        5974                        8533  \n",
       "20                        5703                        8058  \n",
       "22                        5446                        7635  \n",
       "23                        5223                        7240  \n",
       "24                        4986                        6920  \n",
       "27                        4434                        6252  \n",
       "33                        3703                        5248  \n",
       "37                        3225                        4544  \n",
       "38                        2967                        4157  \n",
       "42                        2740                        3862  \n",
       "46                        2418                        3495  \n",
       "41                        2428                        3460  \n",
       "39                        2505                        3502  \n",
       "44                        2360                        3237  \n",
       "47                        2052                        2838  \n",
       "45                        1893                        2600  \n",
       "43                        1974                        2641  \n",
       "40                        2186                        2773  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluating_cusip_merge\n",
    "\n",
    "# Noticed the following trends\n",
    "# 1) Using Cusip_6digit rather than Cusip_8 digit as a joining key results in additional matches\n",
    "# 2) Interestingly, some CUSIP_6digits are not found in the specific year of CRSP data but in other years\n",
    "# 3) mismatches generally peaked around 1970-1980\n",
    "# 4) mismatches reduce gradually. Less than 5% of rows have no match in 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final grouping of monthly_stock_data columns before join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['ultimate_id','gvkey','CUSIP' ,'cusip_6digit','cusip_8digit','COMNAM','NAICS','Industry_name']\n",
    "\n",
    "stock_info_cols = ['SHROUT','stock_exchange','Mcap']\n",
    "\n",
    "time_cols = ['date','Month','Year']\n",
    "\n",
    "price_cols = ['PRC','RET','RET_vwretd_1M_prev','RET_vwretd_2M_prev','RET_vwretd_3M_prev',\n",
    "              'RET_vwretd_6M_prev','RET_vwretd_1M_futr','RET_vwretd_2M_futr','RET_vwretd_3M_futr','RET_vwretd_6M_futr']\n",
    "\n",
    "vol_cols = ['VOL','Vol_1M_6M_index','Vol_2M_6M_index']\n",
    "\n",
    "monthly_stock_data_imp_cols = monthly_stock_data[id_cols + stock_info_cols + time_cols + price_cols + vol_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly selecting only the imp columns from comp data for join\n",
    "\n",
    "comp_data_imp_cols = comp_data.drop(['cusip','ticker', 'split_adjusting_factor', 'company_name','reporting_frequency',\n",
    "                                     'employees', 'sic_code', 'NAICS', 'Mcap','share_holder_equity', \n",
    "                                     'intangible_asset_total', 'asset_total', 'cusip_6digit', 'cusip_8digit','cusip_6digit_date', \n",
    "                                     'cusip_8digit_date', 'ticker_mod', 'ticker_qtr', 'company_mod', 'company_qtr', \n",
    "                                     'calendar_year', 'fiscal_quarter', 'fiscal_year', 'Month', 'Year'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out the closest result date for every row in monthly data\n",
    "\n",
    "comp_data_imp_cols['result_reported_date_mod_month'] = pd.to_datetime(comp_data_imp_cols['result_reported_date_mod']).dt.month\n",
    "comp_data_imp_cols['result_reported_date_mod_year'] = pd.to_datetime(comp_data_imp_cols['result_reported_date_mod']).dt.year\n",
    "\n",
    "finding_closest_result_date = pd.merge(monthly_stock_data_imp_cols[['gvkey','date', 'Year']], \n",
    "                                       comp_data_imp_cols[['gvkey','result_reported_date_mod', 'result_reported_date_mod_year']],\n",
    "                                       left_on = 'gvkey',\n",
    "                                       right_on = 'gvkey',\n",
    "                                       how = 'left')\n",
    "\n",
    "finding_closest_result_date['Gap_btw_years'] = finding_closest_result_date['Year'] - finding_closest_result_date['result_reported_date_mod_year']\n",
    "\n",
    "# The closest result reported date has to be either this year or the prev. year\n",
    "finding_closest_result_date = finding_closest_result_date[(finding_closest_result_date['Gap_btw_years'] >= 0) & \n",
    "                                                          (finding_closest_result_date['Gap_btw_years'] <= 1)] \n",
    "\n",
    "finding_closest_result_date['date'] = pd.to_datetime(finding_closest_result_date['date'])\n",
    "finding_closest_result_date['result_reported_date_mod'] = pd.to_datetime(finding_closest_result_date['result_reported_date_mod'])\n",
    "\n",
    "finding_closest_result_date['gap_btw_days'] = finding_closest_result_date['date'] - finding_closest_result_date['result_reported_date_mod']\n",
    "finding_closest_result_date = finding_closest_result_date[finding_closest_result_date['gap_btw_days'].dt.days > 30]\n",
    "\n",
    "finding_closest_result_date = finding_closest_result_date.sort_values(by = ['gvkey','date','gap_btw_days'], \n",
    "                                                                      ascending = True).drop_duplicates(subset = ['gvkey','date'])\n",
    "\n",
    "# Don't want to consider results that are more than a year old\n",
    "finding_closest_result_date = finding_closest_result_date[finding_closest_result_date['gap_btw_days'].dt.days < 368]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3935354, 27)\n",
      "(3935354, 28)\n"
     ]
    }
   ],
   "source": [
    "# Merginng the closest result date to every row in monthly data (if there is a match)\n",
    "\n",
    "print(monthly_stock_data_imp_cols.shape)\n",
    "monthly_stock_data_imp_cols = pd.merge(monthly_stock_data_imp_cols,\n",
    "                                       finding_closest_result_date[['gvkey', 'date', 'result_reported_date_mod']],\n",
    "                                       left_on = ['gvkey', 'date'],\n",
    "                                       right_on = ['gvkey', 'date'],\n",
    "                                       how = 'left')\n",
    "print(monthly_stock_data_imp_cols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3935354, 28)\n",
      "(3939155, 76)\n"
     ]
    }
   ],
   "source": [
    "comp_data_imp_cols['result_reported_date_mod'] = pd.to_datetime(comp_data_imp_cols['result_reported_date_mod'])\n",
    "\n",
    "# Joining price data and fundemental data based on gvkey, result_reported_date\n",
    "\n",
    "print(monthly_stock_data_imp_cols.shape)\n",
    "monthly_stock_data_imp_cols = pd.merge(monthly_stock_data_imp_cols,\n",
    "                                       comp_data_imp_cols,\n",
    "                                       left_on = ['gvkey', 'result_reported_date_mod'],\n",
    "                                       right_on = ['gvkey', 'result_reported_date_mod'],\n",
    "                                       how = 'left')\n",
    "print(monthly_stock_data_imp_cols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating commonly used financial ratios\n",
    "monthly_stock_data_imp_cols['P/TB_ratio'] = monthly_stock_data_imp_cols['Mcap'] / monthly_stock_data_imp_cols['tangible_equity']\n",
    "monthly_stock_data_imp_cols['P/TB_ratio'].replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "monthly_stock_data_imp_cols['P/E_ratio'] = monthly_stock_data_imp_cols['Mcap'] / (monthly_stock_data_imp_cols['eps_core_excl_extr_12M'].fillna(monthly_stock_data_imp_cols['eps_12M'])*monthly_stock_data_imp_cols['num_shares_eps_12'])\n",
    "monthly_stock_data_imp_cols['P/E_ratio'].replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "monthly_stock_data_imp_cols['P/E_ratio_abs'] = monthly_stock_data_imp_cols['P/E_ratio'].abs()\n",
    "monthly_stock_data_imp_cols['P/E_ratio_abs'].replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "monthly_stock_data_imp_cols['P/S_ratio'] = monthly_stock_data_imp_cols['Mcap'] / monthly_stock_data_imp_cols['revenue_12M']\n",
    "monthly_stock_data_imp_cols['P/S_ratio'].replace([np.inf, -np.inf], np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stock_data_imp_cols.columns\n",
    "\n",
    "# Taking a look at all the columns, derived variables once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_col = ['RET_vwretd_1M_prev','RET_vwretd_2M_prev', 'RET_vwretd_3M_prev', 'RET_vwretd_6M_prev','Vol_1M_6M_index', \n",
    "                'Vol_2M_6M_index','percentage_current_asset_intan', 'percentage_equity_intan','percentage_cash_st_intan', \n",
    "                'percentage_LT_debt_intan','percentage_current_asset_intan_1Y_change','percentage_equity_intan_1Y_change',\n",
    "                'percentage_cash_st_intan_1Y_change','percentage_LT_debt_intan_1Y_change', 'major_acq_12M',\n",
    "                'revenue_5Y_CAGR_adj_dilution_max1','revenue_3Y_CAGR_adj_dilution_max1','revenue_1Y_CAGR_adj_dilution_max1',\n",
    "                'P/TB_ratio', 'P/E_ratio', 'P/S_ratio', 'P/E_ratio_abs']\n",
    "\n",
    "# Modelling only for stocks with Mcap between 1Bn and 10Bn (small cap)\n",
    "# Large caps move largely with index i think\n",
    "\n",
    "X = monthly_stock_data_imp_cols[((monthly_stock_data_imp_cols['Mcap'] > 100000000) & \n",
    "                                 (monthly_stock_data_imp_cols['Mcap'] < 1000000000))][selected_col]\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "y = monthly_stock_data_imp_cols[((monthly_stock_data_imp_cols['Mcap'] > 100000000) & \n",
    "                                 (monthly_stock_data_imp_cols['Mcap'] < 1000000000))].RET_vwretd_1M_futr\n",
    "\n",
    "y.replace([np.inf, -np.inf], np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()\n",
    "# There's significant % of nulls in few important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But this is a project just for my learning experience, I am going to impute all missing columns with mean\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "y = y.fillna(y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF check\n",
    "pd.Series([variance_inflation_factor(X.values, i)  for i in range(X.shape[1])], index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression code\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "y_train_scaled = preprocessing.scale(y_train) \n",
    "# Scaling target variable as well because there is a lot of variance in target variable too\n",
    "# There are lot of video with 10 likes, 20 likes and there's video with 60,000 likes\n",
    "\n",
    "X_test_scaled = preprocessing.scale(X_test)\n",
    "y_test_scaled = preprocessing.scale(y_test)\n",
    "\n",
    "linear_model.fit(X_train_scaled,y_train_scaled)\n",
    "y_test_scaled_pred=linear_model.predict(X_test_scaled)\n",
    "y_train_scaled_pred=linear_model.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data metrics\n",
    "\n",
    "print(\"R^2: \", round(r2_score(y_test,y_test_pred), 2))\n",
    "print(\"Adjusted R^2: \", round(1 - ( 1-linear_model.score(X_test, y_test) ) * ( len(y) - 1 ) / ( len(y) - X.shape[1] - 1 ), 2))\n",
    "print(\"RMSE: \", round(np.sqrt(mean_squared_error(y_test, y_test_pred)), 2))\n",
    "print(\"MAE: \", round(mean_absolute_error(y_test,y_test_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting p values\n",
    "linear_mod = sm.OLS(y_train,X_train)\n",
    "fii = linear_mod.fit()\n",
    "p_values = fii.summary2().tables[1]['P>|t|']\n",
    "p_values\n",
    "# Lot of variables are significant. Created useful derived variables\n",
    "# Unfortunately, there is a lot of unexplained variance (stock markets are effected by numerous features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting coefficients\n",
    "coeff_df = pd.DataFrame(linear_model.coef_, X.columns, columns=['Coefficients'])\n",
    "print(coeff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive feature elimination\n",
    "\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, 5)\n",
    "fit = rfe.fit(X_train, y_train)\n",
    "\n",
    "#print(fit.n_features_)\n",
    "#print(fit.support_)\n",
    "#print(fit.ranking_)\n",
    "\n",
    "list(X_train.iloc[:, fit.support_].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating categorical target variable for logistic regression\n",
    "monthly_stock_data_imp_cols['RET_vwretd_1M_futr > 0'] = np.where(monthly_stock_data_imp_cols['RET_vwretd_1M_futr'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_col = ['RET_vwretd_1M_prev','RET_vwretd_2M_prev', 'RET_vwretd_3M_prev', 'RET_vwretd_6M_prev','Vol_1M_6M_index', \n",
    "                'Vol_2M_6M_index','percentage_current_asset_intan', 'percentage_equity_intan','percentage_cash_st_intan', \n",
    "                'percentage_LT_debt_intan','percentage_current_asset_intan_1Y_change','percentage_equity_intan_1Y_change',\n",
    "                'percentage_cash_st_intan_1Y_change','percentage_LT_debt_intan_1Y_change', 'major_acq_12M',\n",
    "                'revenue_5Y_CAGR_adj_dilution_max1','revenue_3Y_CAGR_adj_dilution_max1','revenue_1Y_CAGR_adj_dilution_max1',\n",
    "                'P/TB_ratio', 'P/E_ratio', 'P/S_ratio', 'P/E_ratio_abs']\n",
    "\n",
    "X = monthly_stock_data_imp_cols[((monthly_stock_data_imp_cols['Mcap'] > 100000000) & \n",
    "                                 (monthly_stock_data_imp_cols['Mcap'] < 1000000000))][selected_col]\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "y = monthly_stock_data_imp_cols[((monthly_stock_data_imp_cols['Mcap'] > 100000000) & \n",
    "                                 (monthly_stock_data_imp_cols['Mcap'] < 1000000000))]['RET_vwretd_1M_futr > 0']\n",
    "\n",
    "y.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "y = y.fillna(1) # Barely 100 nulls in target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building logistic regression\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = logistic_model.predict(X_test)\n",
    "y_test_pred_probas = logistic_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "y_train_pred = logistic_model.predict(X_train)\n",
    "y_train_pred_probas = logistic_model.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_test_pred_probas)\n",
    "auc = metrics.roc_auc_score(y_test, y_test_pred_probas)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance in logistic regression\n",
    "\n",
    "var_imp = pd.concat([pd.DataFrame(X.columns), pd.DataFrame(logistic_model.coef_).T], axis = 1)\n",
    "var_imp.columns = ['var_name', 'coeff']\n",
    "\n",
    "std_df = pd.DataFrame(np.std(X_train, 0)).reset_index()\n",
    "std_df.columns = ['var_name', 'std']\n",
    "\n",
    "var_imp = pd.merge(var_imp,\n",
    "                   std_df,\n",
    "                   left_on = 'var_name',\n",
    "                   right_on = 'var_name',\n",
    "                   how = 'left')\n",
    "var_imp['coeff*std'] = var_imp['coeff']*var_imp['std']\n",
    "\n",
    "var_imp['coeff*std_pos'] = abs(var_imp['coeff*std'])\n",
    "\n",
    "var_imp = var_imp.sort_values(by = 'coeff*std_pos', ascending = False)\n",
    "var_imp = var_imp.drop(['coeff*std_pos'], axis = 1)\n",
    "\n",
    "var_imp['coeff'] = var_imp['coeff'].astype(str)\n",
    "var_imp[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "nEstimator = [100,200] # Reducing parameter for grid search\n",
    "depth = [15,,30] # Too computation heavy for my laptop\n",
    "\n",
    "RF = RandomForestRegressor()\n",
    "hyperParam = [{'n_estimators':nEstimator,'max_depth': depth}]\n",
    "gsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_col = ['RET_vwretd_1M_prev','RET_vwretd_2M_prev', 'RET_vwretd_3M_prev', 'RET_vwretd_6M_prev','Vol_1M_6M_index', \n",
    "                'Vol_2M_6M_index','percentage_current_asset_intan', 'percentage_equity_intan','percentage_cash_st_intan', \n",
    "                'percentage_LT_debt_intan','percentage_current_asset_intan_1Y_change','percentage_equity_intan_1Y_change',\n",
    "                'percentage_cash_st_intan_1Y_change','percentage_LT_debt_intan_1Y_change', 'major_acq_12M',\n",
    "                'revenue_5Y_CAGR_adj_dilution_max1','revenue_3Y_CAGR_adj_dilution_max1','revenue_1Y_CAGR_adj_dilution_max1',\n",
    "                'P/TB_ratio', 'P/E_ratio', 'P/S_ratio', 'P/E_ratio_abs']\n",
    "\n",
    "# Modelling only for stocks with Mcap > 10 Bn\n",
    "X = monthly_stock_data_imp_cols[((monthly_stock_data_imp_cols['Mcap'] > 100000000) & \n",
    "                                 (monthly_stock_data_imp_cols['Mcap'] < 1000000000))][selected_col]\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "y = monthly_stock_data_imp_cols[((monthly_stock_data_imp_cols['Mcap'] > 100000000) & \n",
    "                                 (monthly_stock_data_imp_cols['Mcap'] < 1000000000))].RET_vwretd_1M_futr\n",
    "\n",
    "y.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "y = y.fillna(y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv.fit(X_train, y_train)\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(gsv.best_score_)\n",
    "scores = gsv.cv_results_['mean_test_score'].reshape(len(nEstimator),len(depth))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('max_depth')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(nEstimator)), nEstimator)\n",
    "plt.yticks(np.arange(len(depth)), depth)\n",
    "plt.title('Grid Search r^2 Score')\n",
    "plt.show()\n",
    "maxDepth=gsv.best_params_['max_depth']\n",
    "nEstimators=gsv.best_params_['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest model\n",
    "rf_model = RandomForestRegressor(n_estimators = 100, max_depth = 15, max_features = 5)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "y_train_pred = rf_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error (MAE):', round(metrics.mean_absolute_error(y_test, y_test_pred), 2))\n",
    "print('Median Absolute Error:', round(metrics.median_absolute_error(y_test, y_test_pred), 2))\n",
    "print('Mean Absolute Percentage Error (MAPE):', round(metrics.mean_absolute_percentage_error(y_test, y_test_pred), 2))\n",
    "print('Max Error:', round(metrics.max_error(y_test, y_test_pred), 2))\n",
    "print('Mean Squared Error (MSE):', round(metrics.mean_squared_error(y_test, y_test_pred), 2))\n",
    "print('Root Mean Squared Error (RMSE):', round(metrics.mean_squared_error(y_test, y_test_pred, squared=False), 2))\n",
    "print('R^2:', round(metrics.r2_score(y_test, y_test_pred), 2))\n",
    "print('Explained Variance Score:', round(metrics.explained_variance_score(y_test, y_test_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_var_imp = pd.concat([pd.DataFrame(X.columns), pd.DataFrame(rf_model.feature_importances_)], axis = 1)\n",
    "rf_var_imp.columns = ['var_name', '% imp']\n",
    "rf_var_imp['% imp'] = rf_var_imp['% imp']*100\n",
    "rf_var_imp = rf_var_imp.sort_values(by = ['% imp'], ascending = False)\n",
    "rf_var_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SVR, before doing grid search, need to scale the data\n",
    "\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "y_train_scaled = preprocessing.scale(y_train) \n",
    "# Scaling target variable as well because there is a lot of variance in target variable too\n",
    "# There are lot of video with 10 likes, 20 likes and there's video with 60,000 likes\n",
    "\n",
    "X_test_scaled = preprocessing.scale(X_test)\n",
    "y_test_scaled = preprocessing.scale(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_array = [0.1, 1, 5]\n",
    "gamma = [0.001, 1]\n",
    "\n",
    "svr_model = SVR()\n",
    "hyperParam = [{'C':C_array,'gamma': gamma}]\n",
    "gsv = GridSearchCV(svr_model,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)\n",
    "\n",
    "gsv.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(gsv.best_score_)\n",
    "scores = gsv.cv_results_['mean_test_score'].reshape(len(C_array),len(gamma))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('gamma')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(C_array)), C_array)\n",
    "plt.yticks(np.arange(len(gamma)), gamma)\n",
    "plt.title('Grid Search r^2 Score')\n",
    "plt.show()\n",
    "gamma_para=gsv.best_params_['gamma']\n",
    "C_para = gsv.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "svm_model = SVR(kernel='linear', C=5,gamma=0.001)\n",
    "\n",
    "svm_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# make predictions\n",
    "y_test_pred_scaled = svm_model.predict(X_test_scaled)\n",
    "y_train_pred_scaled = svm_model.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R^2:', round(metrics.r2_score(y_test_scaled, y_test_pred_scaled), 2))\n",
    "print('Explained Variance Score:', round(metrics.explained_variance_score(y_test_scaled, y_test_pred_scaled), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_col = ['RET_vwretd_1M_prev','RET_vwretd_2M_prev', 'RET_vwretd_3M_prev', 'RET_vwretd_6M_prev','Vol_1M_6M_index', \n",
    "                'Vol_2M_6M_index','percentage_current_asset_intan', 'percentage_equity_intan','percentage_cash_st_intan', \n",
    "                'percentage_LT_debt_intan','percentage_current_asset_intan_1Y_change','percentage_equity_intan_1Y_change',\n",
    "                'percentage_cash_st_intan_1Y_change','percentage_LT_debt_intan_1Y_change', 'major_acq_12M',\n",
    "                'revenue_5Y_CAGR_adj_dilution_max1','revenue_3Y_CAGR_adj_dilution_max1','revenue_1Y_CAGR_adj_dilution_max1',\n",
    "                'P/TB_ratio', 'P/E_ratio', 'P/S_ratio', 'P/E_ratio_abs']\n",
    "\n",
    "X = monthly_stock_data_imp_cols[((monthly_stock_data_imp_cols['Mcap'] > 100000000) & \n",
    "                                 (monthly_stock_data_imp_cols['Mcap'] < 1000000000))][selected_col]\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "y = monthly_stock_data_imp_cols[((monthly_stock_data_imp_cols['Mcap'] > 100000000) & \n",
    "                                 (monthly_stock_data_imp_cols['Mcap'] < 1000000000))]['RET_vwretd_1M_futr > 0']\n",
    "\n",
    "y.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "y = y.fillna(1) # Barely 100 nulls in target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)\n",
    "\n",
    "\n",
    "# Scaling X_train & X_test\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "X_test_scaled = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_array = [0.1, 1, 5]\n",
    "gamma = [0.001, 1]\n",
    "\n",
    "svc_model = SVC()\n",
    "hyperParam = [{'C':C_array,'gamma': gamma}]\n",
    "gsv = GridSearchCV(svc_model,hyperParam,cv=5,verbose=1,scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "gsv.fit(X_train_scaled, y_train)\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(gsv.best_score_)\n",
    "scores = gsv.cv_results_['mean_test_score'].reshape(len(C_array),len(gamma))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('gamma')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(C_array)), C_array)\n",
    "plt.yticks(np.arange(len(gamma)), gamma)\n",
    "plt.title('Grid Search AUC')\n",
    "plt.show()\n",
    "gamma_para=gsv.best_params_['gamma']\n",
    "C_para = gsv.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building SVC model\n",
    "\n",
    "svc_model = SVC(C = 5, gamma = 1, probability = True, kernel = 'linear')\n",
    "svc_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred = svc_model.predict(X_test_scaled)\n",
    "y_test_pred_probas = svc_model.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "y_train_pred = svc_model.predict(X_train_scaled)\n",
    "y_train_pred_probas = svc_model.predict_proba(X_train_scaled)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_test_pred_probas)\n",
    "auc = metrics.roc_auc_score(y_test, y_test_pred_probas)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance in SVM\n",
    "\n",
    "var_imp = pd.concat([pd.DataFrame(X.columns), pd.DataFrame(svc_model.coef_).T], axis = 1)\n",
    "var_imp.columns = ['var_name', 'coeff']\n",
    "var_imp['coeff_pos'] = abs(var_imp['coeff'])\n",
    "var_imp = var_imp.sort_values(by = 'coeff_pos', ascending = False)\n",
    "var_imp = var_imp.drop(['coeff_pos'], axis = 1)\n",
    "\n",
    "var_imp['coeff'] = var_imp['coeff'].astype(str)\n",
    "var_imp[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
